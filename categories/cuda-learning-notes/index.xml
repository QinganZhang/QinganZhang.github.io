<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>cuda-learning-notes on Paul&#39;s Blog</title>
    <link>https://qinganzhang.github.io/categories/cuda-learning-notes/</link>
    <description>Recent content in cuda-learning-notes on Paul&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 01 Feb 2024 22:01:50 +0800</lastBuildDate><atom:link href="https://qinganzhang.github.io/categories/cuda-learning-notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>内存模型</title>
      <link>https://qinganzhang.github.io/posts/cuda-learning-notes/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Thu, 01 Feb 2024 22:01:50 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/cuda-learning-notes/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</guid>
      <description>内存模型 全局内存 对全局内存变量的理解： 从主机端看，全局内存变量只是一个指针，主机端不知道其指向何方。主机端也无法进行操作 从设备端看，即为全局</description>
      <content:encoded><![CDATA[<h2 id="内存模型">内存模型</h2>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:11:11.png" style="zoom: 70%;" />
<h3 id="全局内存">全局内存</h3>
<ul>
<li>
<p>对全局内存变量的理解：</p>
<ul>
<li>从主机端看，全局内存变量只是一个指针，主机端不知道其指向何方。主机端也无法进行操作</li>
<li>从设备端看，即为全局内存变量</li>
<li>一个经常会发生的错误就是混用设备和主机的内存地址：主机代码不能直接访问设备变量，设备也不能直接访问主机变量</li>
</ul>
</li>
<li>
<p>对全局内存的读写</p>
<ul>
<li>如果是读操作，有三种部分的访问方式：
<ul>
<li>L1缓存，L2缓存，DRAM</li>
<li>（禁用L1缓存）L2缓存，DRAM
<ul>
<li>Fermi之后都是默认禁用L1</li>
<li>禁用L1缓存的原因是，L1缓存被用作缓冲从寄存器中溢出的数据</li>
</ul>
</li>
<li>只读缓存，L2缓存，DRAM</li>
</ul>
</li>
<li>如果是写操作，则无法被缓存，只经过device层次的L2缓存，没有命中再访问DRAM
<ul>
<li>==不是很清楚==</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="编程模型">编程模型</h4>
<ul>
<li>动态全局内存：</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">double</span> <span class="o">*</span><span class="n">d_x</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl"><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">d_x</span><span class="p">),</span> <span class="mi">100</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span> <span class="c1">// d_x改变为指向设备全局内存的指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">double</span> <span class="o">*</span><span class="n">h_x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">h_x</span> <span class="o">=</span> <span class="p">(</span><span class="kt">double</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span> <span class="c1">// h_x是指向主机内存的指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_x</span><span class="p">,</span> <span class="n">h_x</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>静态全局内存：
<ul>
<li>如果静态全局变量是一个变量（而非数组类型）：此时主机中不可以直接给静态全局内存变量赋值，可以通过 <code>cudaMemcpyToSymbol()</code> 和 <code>cudaMemcpyFromSymbol()</code> 拷贝。  （一个例外：固定内存）
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">double</span> <span class="n">d</span><span class="p">;</span> <span class="c1">// 从设备端来看，d直接就是设备全局内存上的变量；从主机端来看，d是一个指针，但是不知道其指向哪里
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">double</span> <span class="n">h</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">cudaMemcpyToSymbol</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">h</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span> <span class="c1">// H2D
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cudaMemcpyFromSymbol</span><span class="p">(</span><span class="o">&amp;</span><span class="n">h</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span> <span class="c1">// D2H
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// 因为使用cudaMemcpy需要得到d的地址，而主机端无法直接操作设备端的变量。如果非要使用cudaMemcpy:
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">double</span> <span class="o">*</span><span class="n">dptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">cudaGetSymbolAddress</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">dptr</span><span class="p">),</span> <span class="n">d</span><span class="p">);</span> <span class="c1">// 因为主机无法对全局内存变量d取地址，只能使用函数间接得到其地址dptr
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dptr</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span> <span class="n">cudaMemcpyToDevice</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>如果静态全局变量是一个数组，可以使用<code>cudaMemcpy</code>：
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">double</span> <span class="n">d_x</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span> <span class="c1">// d_x[]直接就是设备全局内存上的数组，d_x是其地址
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">double</span> <span class="n">h_x</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_x</span><span class="p">,</span> <span class="n">h_x</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
</ul>
<h4 id="优化">优化</h4>
<p>全局内存访问速度慢，往往是一个 CUDA 程序的性能瓶颈。
优化目标：</p>
<ul>
<li>对齐合并的内存访问，减少带宽浪费</li>
<li>足够的并发内存操作，隐藏内存延迟</li>
</ul>
<h5 id="全局内存的对齐合并访问">全局内存的对齐合并访问</h5>
<ul>
<li>
<p>访问粒度：</p>
<ul>
<li>L1的缓存粒度为128字节（可以禁用L1缓存，只使用L2缓存）</li>
<li>L2的缓存粒度为32字节</li>
<li>只读缓存也可以缓存全局内存中的数据，缓存粒度为32字节
<ul>
<li>使用<code>__ldg()</code>函数将全局内存缓存到只读缓存中</li>
<li>如果编译器能够判断一个全局内存变量在整个核函数的范围内都可读，则自动使用<code>__ldg()</code>函数进行缓存，但是对于全局的写入，没有相应的函数</li>
<li>可以使用<code>__restrict__</code>修饰指针，表示该指针专门用来访问特定的数组（该指针不是别名），nvcc使用只读缓存进行加载</li>
</ul>
</li>
</ul>
</li>
<li>
<p>内存对齐：</p>
<ul>
<li>一次数据传输中，从全局内存转移到 L2 缓存的一片内存的首地址一定是 32 的整数倍。</li>
<li>使用cuda runtime api（比如cudaMalloc）分配的内存的首地址至少是256字节的整数倍</li>
</ul>
</li>
<li>
<p>内存事务：从核函数发起请求，到硬件相应返回数据这个过程</p>
<ul>
<li>内存事务可以分为1段，2段，4段</li>
<li>比如全局内存写入时，经过L2缓存，缓存粒度为32字节，此时一次内存事务可以写入1段32字节，2段64字节，4段128字节，其他字节数量只能组合得到</li>
</ul>
</li>
<li>
<p>全局内存的访问模式：</p>
<ul>
<li>对齐的：内存事务的首地址是缓存粒度的整数倍</li>
<li>合并的（coalesced）：一个warp对全局内存的访问都在一个缓存粒度中（一个warp对全局内存的访问导致最少数量的数据传输），或者可以理解为缓存利用率
<ul>
<li>合并度=$\frac{warp请求的字节数}{由该请求导致的所有数据传输的字节数}$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>几种常见的内存访问模式：（以一维的grid和一维的block为例）</p>
<ul>
<li>理想的内存访问：顺序的合并访问，合并度=100%</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">add</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">z</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>乱序的合并访问：访问是交叉的，但仍是合并的，合并度=100%</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">add_permuted</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">z</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">tid_permuted</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">^</span> <span class="mh">0x1</span><span class="p">;</span> <span class="c1">// 交换两个相邻的数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 比如：threadIdx.x=0, tid_permuted=1; threadIdx.x=1;tid_permuted=0;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">tid_permuted</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">add_permuted</span><span class="o">&lt;&lt;&lt;</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>不对齐的非合并访问（地址错位）
<ul>
<li>如果使用L1 cache，访问粒度为128字节，速度快，但是带宽利用率更低</li>
<li>如果不使用L1 cache，访问粒度为32字节，速度慢，但是带宽利用率更高，从而可以提高总线的整体利用率</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">add_offset</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">z</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">add_offset</span><span class="o">&lt;&lt;&lt;</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// 对于某个thread block，有32个线程
</span></span></span><span class="line"><span class="cl"><span class="c1">// 假设数组x，y，z首地址都是256字节的倍数，而一次访存至少32字节
</span></span></span><span class="line"><span class="cl"><span class="c1">// 由于地址错位，需要进行五次访存，合并度=128/(5*32)=80%
</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>跨越式非合并访问
<ul>
<li>如果使用L1 cache，访问粒度为128字节，合并度很低（而且出现频繁的缓存失效和替换）</li>
<li>如果不使用L1 cache，访问粒度为32字节，合并度稍微提升</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">add_stride</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">z</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">add_stride</span><span class="o">&lt;&lt;&lt;</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// 对于0号线程块(blockIdx.x=0)，将访问：0， 128， 256， 384 ... 等位置
</span></span></span><span class="line"><span class="cl"><span class="c1">// 即stride=gridDim.x=128
</span></span></span><span class="line"><span class="cl"><span class="c1">// 合并度=128/(32*32)=12.5%，触发32次访存，每次访存32字节
</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>广播式非合并访问</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">add_broadcast</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">z</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">// 合并度=4/32=12.5%
</span></span></span><span class="line"><span class="cl"><span class="c1">// 虽然合并度低，但是整个过程只进行了一次访存
</span></span></span><span class="line"><span class="cl"><span class="c1">// 其实更适合使用常量内存
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h5 id="定量衡量核函数的有效带宽">定量衡量核函数的有效带宽</h5>
<ul>
<li>
<p>带宽：</p>
<ul>
<li>理论带宽：硬件限制</li>
<li>有效带宽：核函数实际达到的带宽，$有效带宽=\frac{(读字节数+写字节数)\times 10^{-9}}{运行时间}$</li>
<li>吞吐量：单位时间内操作的执行速度，比如说FPS或（流水线）每个周期完成都少个指令，不仅取决于有效带宽，而且与带宽的利用率、是否命中缓存有关
<ul>
<li>比如数据经常命中缓存，此时吞吐量就可能超过有效带宽</li>
</ul>
</li>
</ul>
</li>
<li>
<p>例子：使用全局内存进行方阵转置，</p>
<ul>
<li>准备工作：测量有效带宽的上限和下限
<ul>
<li>测量有效带宽的上限：对A按行合并读取，对B按行合并写入
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">B</span><span class="p">[</span><span class="n">nx</span> <span class="o">+</span> <span class="n">ny</span> <span class="o">*</span> <span class="n">N</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">nx</span> <span class="o">+</span> <span class="n">ny</span> <span class="o">*</span> <span class="n">N</span><span class="p">];</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>测量有效带宽的下限：对A按列交叉读取，对B按列交叉写入
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">B</span><span class="p">[</span><span class="n">ny</span> <span class="o">+</span> <span class="n">nx</span> <span class="o">*</span> <span class="n">N</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">ny</span> <span class="o">+</span> <span class="n">nx</span> <span class="o">*</span> <span class="n">N</span><span class="p">];</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
<li>测试：code部分如果</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">TILE_DIM</span> <span class="o">=</span> <span class="mi">32</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="kt">double</span> <span class="n">real</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">copy</span><span class="p">(</span><span class="k">const</span> <span class="n">read</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="n">real</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">nx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">TILE_DIM</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">ny</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">TILE_DIM</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/*
</span></span></span><span class="line"><span class="cl"><span class="cm">    code
</span></span></span><span class="line"><span class="cl"><span class="cm">    */</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">dim3</span> <span class="nf">block_size</span><span class="p">(</span><span class="n">TILE_DIM</span><span class="p">,</span> <span class="n">TILE_DIM</span><span class="p">);</span> <span class="c1">// 每个thread block中TILE_DIM*TILE_DIM个线程，每个元素对应一个线程
</span></span></span><span class="line"><span class="cl"><span class="c1">// 此时一个线程块中32*32个线程，少于1024的限制
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">dim3</span> <span class="nf">grid_size</span><span class="p">((</span><span class="n">N</span> <span class="o">+</span> <span class="n">TILE_DIM</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_DIM</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">TILE_DIM</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_DIM</span><span class="p">);</span> <span class="c1">// grid的维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">copy</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">block_size</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p>将A的一行转成B的一列：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">if</span><span class="p">(</span><span class="n">nx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">B</span><span class="p">[</span><span class="n">ny</span> <span class="o">+</span> <span class="n">nx</span> <span class="o">*</span> <span class="n">N</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">nx</span> <span class="o">+</span> <span class="n">ny</span> <span class="o">*</span> <span class="n">N</span><span class="p">];</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>对于A的读取是顺序的，对于B的写入是非顺序的</li>
</ul>
</li>
<li>
<p>将A的一列转成B的一行：更快</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">if</span><span class="p">(</span><span class="n">nx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">ny</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">B</span><span class="p">[</span><span class="n">nx</span> <span class="o">+</span> <span class="n">ny</span> <span class="o">*</span> <span class="n">N</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">ny</span> <span class="o">+</span> <span class="n">nx</span> <span class="o">*</span> <span class="n">N</span><span class="p">];</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>对于A的读取不是顺序的，对于B的写入是顺序的</li>
</ul>
</li>
<li>
<p>分析：</p>
<ul>
<li>
<p>如果对A按行读取（将A的一行转成B的一列），对A按行读取是合并的，写入过程（交叉写入）不缓存</p>
</li>
<li>
<p>如果对B按行写入（将A的一列转成B的一行），对A按列读取是交叉的，写入过程（合并写入）不缓存，应该更慢</p>
</li>
<li>
<p>但是实际上第二种方式更快，原因在于L1缓存命中率
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-08-16:17:47.png" style="zoom: 20%;" /></p>
<ul>
<li>对A按行读取，每个warp读取$32\times4B=128B$，正好是一次L1缓存的访问粒度，相当于每次访问，L1缓存命中率都为0，数据从全局内存拿到L1缓存后，后续这些数据又不再使用。因此，总体来看L1缓存命中率=0</li>
<li>对A按列访问，第0个warp中每个线程此时都L1缓存没有命中，此时会有32次128B的访存，然后数据拿到L1缓存中，后面第1~31个warp中线程都可以命中L1缓存。因此，总体来看缓存命中率=$\frac{31}{32}$=0.96875</li>
<li>可能是对A按列访问由于L1缓存命中率高，隐藏延迟更好，总体耗时更短，==不是很清楚==</li>
</ul>
</li>
</ul>
</li>
<li>
<p>若不能满足读取和写入都是合并的，一般应该尽量做到合并写入</p>
</li>
<li></li>
</ul>
</li>
</ul>
<h5 id="结构体数组和数据结构体">结构体数组和数据结构体</h5>
<ul>
<li>
<p>结构体数组（Structure of Array，SoA）：一个结构体，其中成员是数组</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">struct</span> <span class="n">SoA</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="n">SoA</span> <span class="n">myStruct</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>数组结构体（Array of Structure, AoS）：一个数组，每个元素都是一个结构体</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="n">element</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">Aos</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">Aos</span> <span class="n">array</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>CUDA中普遍倾向于SoA（结构体数组）因为这种内存访问可以有效地合并
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:23:43.png" style="zoom:60%;" /></p>
</li>
</ul>
<h5 id="其他">其他</h5>
<ul>
<li>
<p>增加每个线程中执行独立内存操作的数量，减少核函数发射的数量</p>
<ul>
<li>对于IO密集型的核函数，每个线程多处理一点数据（而非原来只处理一个数据）</li>
<li>比如reduce中，每个线程可以先累加多个数据，然后再进行两两数据的折叠相加</li>
</ul>
</li>
<li>
<p>对核函数的运行配置进行调整，提升SM占用率</p>
<ul>
<li>提升SM占用率会更好隐藏访存延迟吗？==不是很清楚==
<ul>
<li>参考：<code>Better Performance at Lower Occupancy</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>参考</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/t4T7u4SqajH8db0Essedog">https://mp.weixin.qq.com/s/t4T7u4SqajH8db0Essedog</a></li>
</ul>
</li>
</ul>
<h3 id="常量内存">常量内存</h3>
<ul>
<li>常量内存属于全局内存，只有64KB</li>
<li>核函数的参数通过常量内存传递，且限定4KB</li>
<li>常量内存通过Read-Only Data Cache进行缓存，而且读取到的数据可以广播给warp中的其他线程</li>
<li>因为是只读的，因此常量内存必须在全局空间内、所有核函数之外进行声明，且必须在kernel启动前由host进行初始化（比如使用<code>cudaMemcpyToSymbol</code>来进行初始化）</li>
</ul>
<h3 id="纹理和表面内存">纹理和表面内存</h3>
<ul>
<li>纹理内存专门为那些存在大量空间局部性的内存访问模式设计，可以充分利用空间局部性（比如插值、滤波等操作）</li>
<li>纹理内存驻留在全局内存中，经过只读纹理缓存进行缓存</li>
</ul>
<h3 id="寄存器">寄存器</h3>
<ul>
<li>一个寄存器有32bit（4B）的大小，一些常用内建变量存放在寄存器中</li>
<li>核函数中定义的不加任何限定符的变量一般就存放在寄存器中，不加任何限定符的数组可能存放在寄存器中，或者放在局部内存中（即寄存器溢出，会对性能造成很大影响）</li>
<li>核函数前显式说明来帮助编译优化：<code>__launch_bounds_(maxThreadaPerBlock, minBlocksPerMulitprocessor)</code>
<ul>
<li><code>maxThreadaPerBlock</code>：线程块内包含的最大线程数</li>
<li><code>minBlocksPerMulitprocessor</code>：可选参数，每个SM中预期的最小的常驻线程块数量</li>
</ul>
</li>
<li>寄存器只能被一个线程可见，因此每个线程都有一个变量的副本，而且该变量的副本可以值不同</li>
</ul>
<h3 id="局部内存">局部内存</h3>
<ul>
<li>将寄存器放不下的变量、索引值不能再编译时就确定的数组，都存放在局部内存中（编译器进行判断）</li>
<li>局部内存是全局内存的一部分，因此使用时延迟较高</li>
<li>对于计算能力2.0以上的设备，局部内存可能会存储在L1缓存或L2缓存上</li>
</ul>
<h3 id="共享内存">共享内存</h3>
<ul>
<li>主要作用：
<ul>
<li>减少核函数中对全局内存的访问次数，实现高效的线程块内部的通信</li>
<li>优化对全局内存的访问模式，尤其是针对全局内存的跨越式非合并访问，提高带宽利用率</li>
</ul>
</li>
<li>共享内存一般和L1缓存共享64KB片上内存，可以进行配置
<ul>
<li>按设备进行配置
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">cudaDeviceSetCacheConfig</span><span class="p">(</span><span class="n">cudaFuncCache</span> <span class="n">cacheConfig</span><span class="p">);</span> 
</span></span><span class="line"><span class="cl"><span class="cm">/* 参数
</span></span></span><span class="line"><span class="cl"><span class="cm">cudaFuncCachePreferNone: no preference(default)
</span></span></span><span class="line"><span class="cl"><span class="cm">cudaFuncCachePreferShared: prefer 48KB shared memory and 16 KB L1 cache
</span></span></span><span class="line"><span class="cl"><span class="cm">cudaFuncCachePreferL1: prefer 48KB L1 cache and 16 KB shared memory
</span></span></span><span class="line"><span class="cl"><span class="cm">cudaFuncCachePreferEqual: prefer 32KB L1 cache and 32 KB shared memory
</span></span></span><span class="line"><span class="cl"><span class="cm">*/</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>不同核函数自动配置
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">cudaFuncSetCacheConfig</span><span class="p">(</span><span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">func</span><span class="p">,</span> <span class="k">enum</span> <span class="nc">cudaFuncCache</span> <span class="n">cacheConfig</span><span class="p">);</span> <span class="c1">// 配置核函数func对应的共享内存大小 
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
</ul>
<h4 id="编程模型-1">编程模型</h4>
<ul>
<li>静态分配：<code>__shared__ float mat[5][5];</code></li>
<li>动态分配：
<ul>
<li>函数内声明方式：<code>extern __shared__ double arr[];</code>
<ul>
<li>动态共享内存只支持一维数组</li>
</ul>
</li>
<li>核函数的执行配置中，第三个参数为每个线程块中动态共享内存的字节数：<code>&lt;&lt;&lt;grid_size, block_size, sizeof(float) * block_size&gt;&gt;&gt;</code></li>
</ul>
</li>
<li>同步：<code>__syncthreads</code>进行线程块的同步</li>
</ul>
<h4 id="优化-1">优化</h4>
<h3 id="缓存">缓存</h3>
<ul>
<li>
<p>L1和L2缓存：缓存局部内存和全局内存的数据</p>
<ul>
<li>每个SM都有自己的L1缓存，但是L2缓存是所有SM共用的</li>
<li>可以配置是否使用L1缓存</li>
<li>CPU的L1缓存考虑了时间局部性（LRU算法）和空间局部性，GPU的L1缓存只有空间局部性，没有时间局部性（频繁访问一个一级缓存的内存位置不会增加数据留在缓存中的概率）</li>
<li>
<h2 id="cpu的一级缓存是的替换算法是有使用频率和时间局部性的gpu则没有">CPU的一级缓存是的替换算法是有使用频率和时间局部性的，GPU则没有</h2>
</li>
<li>与CPU读写都缓存不同，GPU只会针对读过程进行缓存，写过程不缓存</li>
</ul>
</li>
<li>
<p>每个SM都有一个只读常量缓存</p>
<ul>
<li>使用<code>__ldg()</code>函数显示将数据通过只读数据缓存进行加载</li>
</ul>
</li>
<li>
<p>GPU不是很强调缓存（not dependent on large caches for performance），因为当指令或数据miss时，由于warp切换速度快，所以旧切换warp；即用计算而非cache来隐藏延迟</p>
</li>
</ul>
<h2 id="内存管理">内存管理</h2>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-07-21:05:54.png" alt="image-20231207210554796" style="zoom:80%;" />
<h3 id="常规数据传输函数">常规数据传输函数</h3>
<ul>
<li>
<p><code>cudaMalloc</code>函数：<code>cudaError_t cudaMalloc(void **address, size_t size);</code></p>
<ul>
<li>示例：
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">double</span> <span class="o">*</span><span class="n">d_x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_x</span><span class="p">,</span> <span class="mi">100</span><span class="p">);</span> <span class="c1">// &amp;d_x的类型为double**
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>参数说明：
<ul>
<li><code>address</code>是在分配设备内存的指针</li>
</ul>
</li>
<li>注意事项：
<ul>
<li>==一个经常会发生的错误就是混用设备和主机的内存地址==：主机代码不能直接访问设备变量，设备也不能直接访问主机变量</li>
<li>因为该函数的功能是改变指针<code>d_x</code>的值（即改变<code>d_x</code>指向的位置，将一个指向内存地址的指针赋值给<code>d_x</code>），而非改变<code>d_x</code>所指内容的值，因此只能传入指针<code>d_x</code>的地址，即指针的指针</li>
<li>原来<code>d_x</code>是主机上的一个指针，<code>cudaMalloc</code>之后改变为指向设备全局内存的指针，本质上是GPU地址在内存中的虚拟映射地址</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>cudaMemset</code>函数：<code>cudaError_t cudaMemset(void * devPtr,int value,size_t count)</code></p>
</li>
<li>
<p><code>cudaFree</code>函数：<code>cudaError_t cudaFree(void* address)</code></p>
<ul>
<li>设备内存的分配和释放非常影响性能，尽量重用</li>
<li>CUDA允许在核函数内部使用malloc/free 分配/释放全局内存，但是一般会导致较差的性能</li>
</ul>
</li>
<li>
<p><code>cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind)</code></p>
<ul>
<li>主机端的内存默认是可分页的，如果进行数据拷贝，此时CUDA分配不可分页的固定内存，将可分页内存中的数据复制其中，然后再从固定内存中拷贝数据到显存</li>
<li>如果主机端的内存是可分页的，使用虚拟内存，当该页面被换出到交换区时，设备此时无法访问或者进行控制</li>
</ul>
</li>
<li>
<p><code>cudaMemcpyToSymbol</code>函数和<code>cudaMemcpyFromSymbol</code>函数</p>
<ul>
<li>symbol是一个驻留在全局或常量内存空间中的变量</li>
</ul>
</li>
<li>
<p><code>cudaMemcpy</code> 的异步版本 <code>cudaMemcpyAsync</code></p>
<ul>
<li><code>cudaError_t cudaMemcpyAsync(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind, cudaStream_t stream)</code></li>
<li>使用异步的数据传输函数时，需要将主机内存定义为不可分页内存（使用<code>cudaMallocHost</code>或<code>cudaHostAlloc</code>），从而防止在程序执行期间物理地址被修改</li>
<li>如果将可分页内存传递给<code>cudaMemcpyAsync</code>，则会导致同步传输</li>
</ul>
</li>
<li>
<p>固定内存：</p>
<ul>
<li><code>cudaError_t cudaMallocHost(void **devPtr, size_t count);</code></li>
<li><code>cudaError_t cudaFreeHost(void *ptr);</code></li>
<li>固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。</li>
<li>固定内存有更高的读写带宽，但是分配过多的固定内存可能会降低主机系统的性能，同时固定内存分配和释放的代价更高。<strong>通常, 当传输数据量&gt;=10M时, 使用固定内存是更好的选择</strong>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:24:06.png" style="zoom:60%;" /></li>
</ul>
</li>
<li>
<p>零拷贝内存</p>
<ul>
<li>在零拷贝内存中，主机和设备可以直接访问对方的变量，原理是将host内存直接映射到设备内存空间上，使得设备可以通过DMA的方式访问host的锁页内存</li>
<li><code>cudaError_t cudaHostAlloc(void **pHost, size_t size, unsigned int flags)</code>
<ul>
<li><code>cudaHostAllocDefault</code>：与cudaMallocHost函数行为一致</li>
<li><code>cudaHostAllocPortable</code>：返回能被所有CUDA上下文使用的固定内存，而不仅是执行内存分配的那一个，分配portable memory，适用于主机多线程，让控制不同GPU的主机端线程操作同一块portable memory，实现GPU线程间通信</li>
<li><code>cudaHostAllocMapped</code>：分配mapped memory，可以在kernel中直接访问mapped memory中的数据，不必再内存和显存之间进行数据拷贝，即zero-copy功能</li>
<li><code>cudaHostAllocWriteCombined</code>：分配write-combined memory，提高从CPU向GPU单向传输数据的速度，不使用CPU的L1、L2 cache，将cache资源留给其他程序使用，在PCI-E总线传输期间不会被来自CPU的监视打断
<ul>
<li>将多次写操作写到固定内存的buffer中，将多次写合并；但实际上性能会比普通的write-back更糟糕, 主要是由于其没有使用cache, 而是直接写回内存</li>
</ul>
</li>
</ul>
</li>
<li>零拷贝内存虽然不需要显式的将主机的数据复制到设备上，但是设备也不能直接访问主机端的数据，需要通过<code>cudaHostGetDevicePointer</code>函数主机上的地址，然后才能通过<code>pDevice</code>访问主机上的零拷贝内存
<ul>
<li><code>cudaHostGetDevicePointer(void **pDevice, void *pHost, unsigned int flags);</code>
<ul>
<li><code>flags</code>设置为0</li>
</ul>
</li>
<li>如果使用统一内存，则无须使用<code>cudaHostGetDevicePointer</code></li>
</ul>
</li>
<li>使用零拷贝内存需要注意同步主机和设备之间的内存访问</li>
<li>零拷贝内存适合用于少量的数据传输</li>
</ul>
</li>
</ul>
<h3 id="统一内存">统一内存</h3>
<ul>
<li>
<p>发展：</p>
<ul>
<li>统一寻址（Unified Address）：Fermi架构中提出了统一的地址空间，将全局内存、局部内存、共享内存放在一个地址空间中</li>
<li>统一虚拟地址(UVA)：CUDA 4（开普勒架构，麦克斯韦架构）引入，将CPU和GPU的内存映射到统一的虚拟地址上，可以使用指针访问对方的地址</li>
<li>统一内存(UM)：CUDA 6（帕斯卡架构之后）引入，实现了一个CPU和GPU之间的内存池
<ul>
<li>对于第一代统一内存，主机与设备不能并发访问统一内存。因此，在主机调用核函数之后，必须加上一个同步函数（比如<code>cudaDeviceSynchornize</code>），确保核函数对统一内存的访问已经结束，然后才能主机访问统一内存变量</li>
<li>对于第二代统一内存，主机与设备可以并发访问统一内存</li>
</ul>
</li>
</ul>
</li>
<li>
<p>语法相关：</p>
<ul>
<li>统一内存在device中当作全局内存来使用，必须由主机来定义或分配内存，不能在设备端（核函数或<code>__device_</code>函数中）进行。因此，在核函数中由malloc分配的堆内存不属于同一内存，因而如果CPU需要访问，需要手工进行移动</li>
<li>同一个程序中可以同时使用统一内存和非统一内存</li>
</ul>
</li>
<li>
<p>统一内存的分配</p>
<ul>
<li>动态分配：<code>cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned flags = 0);</code>
<ul>
<li>参数<code>flags</code>默认为<code>cudaMemAttachGlobal</code>，表示分配的全局内存可以由任何设备通过任何CUDA流访问</li>
</ul>
</li>
<li>静态分配：<code>__device__ __managed__</code>修饰，而且只能是全局变量</li>
</ul>
</li>
<li>
<p>超量分配：</p>
<ul>
<li>编译选项：<code>-DUNIFIED</code></li>
<li><code>cudaMallocManaged</code>申请内存只是表示预定了一段空间，统一内存的实际分配发生在第一次访问预留的内存时</li>
</ul>
</li>
<li>
<p>优化使用统一内存的程序</p>
<ul>
<li>可以手动给编译期一些提示，避免数据缺页、内存抖动，保持数据局部性等，可以使用<code>cudaMemAdvice</code>和<code>cudaMemPrefetchAsync</code></li>
<li><code>cudaError_t cudaMemPrefetchAsync(const void *devPtr, size_t count, int dstDevice, cudaStream_t stream)</code>
<ul>
<li>在CUDA流中将统一内存缓冲区devPtr内count字节的内存迁移到设备dstDevice（<code>cudaCpuDeviceId</code>表示主机的设备号）中的内存区域，从而防止或减少缺页异常，提高数据局部性</li>
<li>尽可能多的使用<code>cudaMemPrefetchAsync</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>优势：</p>
<ul>
<li>简化编程
<ul>
<li>编程更简单：比如之前多GPU，针对某一个数据使用零拷贝内存，每个设备都需要有对应的一个指针，容易混乱（针对零拷贝的改进）</li>
<li>方便代码移植</li>
<li>支持更完整的C++语言要素：比如核函数参数可以使用引用，可以直接使用拷贝构造函数而不用手工进行拷贝或进行很多重载</li>
</ul>
</li>
<li>可能会提供比手工移动数据更好的性能，比如可能会将某部分数据放置到离某个存储器更近的位置
<ul>
<li>可以进行超量分配，超出GPU显存的部分可以放在主机内存中（但是反过来不行）</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>流和同步</title>
      <link>https://qinganzhang.github.io/posts/cuda-learning-notes/%E6%B5%81%E5%92%8C%E5%90%8C%E6%AD%A5/</link>
      <pubDate>Thu, 01 Feb 2024 22:01:43 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/cuda-learning-notes/%E6%B5%81%E5%92%8C%E5%90%8C%E6%AD%A5/</guid>
      <description>CUDA事件 事件：标记stream执行过程的某个特定的点，比如用于计时 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cudaEvent_t start, stop; CHECK(cudaEventCreate(&amp;amp;start)); // 创建cuda 事件对象。 CHECK(cudaEventCreate(&amp;amp;stop)); CHECK(cudaEventRecord(start, 0)); //</description>
      <content:encoded><![CDATA[<h2 id="cuda事件">CUDA事件</h2>
<ul>
<li>事件：标记stream执行过程的某个特定的点，比如用于计时
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:22:38.png" style="zoom: 50%;" /></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">cudaEvent_t</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="nf">CHECK</span><span class="p">(</span><span class="nf">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">start</span><span class="p">));</span> <span class="c1">// 创建cuda 事件对象。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="nf">CHECK</span><span class="p">(</span><span class="nf">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stop</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="nf">CHECK</span><span class="p">(</span><span class="nf">cudaEventRecord</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="mi">0</span><span class="p">));</span>  <span class="c1">// 将事件start关联到指定的流0
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="nf">cudaEventQuery</span><span class="p">(</span><span class="n">start</span><span class="p">);</span>  <span class="c1">// 强制刷新 cuda 执行流，因为WDDM模式下，CUDA流中的操作显式提交到一个软件队列中（TCC模式不用）
</span></span></span><span class="line"><span class="cl"><span class="c1">// 此处不能使用CHECK，因为它可能返回cudaErrorNotReady，但是又不代表程序出错
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// run code.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="nf">CHECK</span><span class="p">(</span><span class="nf">cudaEventRecord</span><span class="p">(</span><span class="n">stop</span><span class="p">,</span> <span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="nf">CHECK</span><span class="p">(</span><span class="nf">cudaEventSynchronize</span><span class="p">(</span><span class="n">stop</span><span class="p">));</span> <span class="c1">// 强制同步，让主机等待cuda事件执行完毕。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">elapsed_time</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="nf">CHECK</span><span class="p">(</span><span class="nf">cudaEventElapsedTime</span><span class="p">(</span><span class="o">&amp;</span><span class="n">elapsed_time</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">));</span> <span class="c1">// 计算 start 和stop间的时间差（ms）。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="nf">CHEKC</span><span class="p">(</span><span class="nf">cudaEventDestroy</span><span class="p">(</span><span class="n">start</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="nf">CHEKC</span><span class="p">(</span><span class="nf">cudaEventDestroy</span><span class="p">(</span><span class="n">stop</span><span class="p">));</span>    
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="cuda流">CUDA流</h2>
<ul>
<li>
<p>CUDA流：由主机发出的、在一个设备中执行的CUDA操作序列</p>
<ul>
<li><code>kernal_func&lt;&lt;&lt;grid_size, block_size, 0, stream&gt;&gt;&gt;(params);</code></li>
<li>一个CUDA流中各个操作的次序是由主机控制的，但是来自于两个不同CUDA流中的操作顺序无法确定</li>
<li>任何CUDA操作都存在于某个CUDA流中，要么是默认流（也成为空流），要么明确指定的流</li>
</ul>
</li>
<li>
<p>相关函数</p>
<ul>
<li><code>cudaError_t cudaStreamCreate(cudaStream_t *stream);</code></li>
<li><code>cudaError_t cudaStreamDestory(cudaStream_t stream);</code></li>
<li><code>cudaError_t cudaStreamSynchronize(cudaStream_t stream);</code>
<ul>
<li>同步等待一个流中的所有操作完成</li>
</ul>
</li>
<li><code>cudaError_t cudaStreamQuery(cudaStream_t stream);</code>
<ul>
<li>查询一个流中的操作是否全部完成，不会阻塞；若是，则返回 <code>cudaSuccess</code>;  否则，返回 <code>cudaErrorNotReady</code>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="stream对并行性的影响">Stream对并行性的影响</h3>
<ul>
<li>调度队列的个数：
<ul>
<li>单调度队列：虽然Fermi架构支持最多16个流，但是实际调度过程中，所有的流被塞进同一个调度队列，当选中一个操作执行时，runtime会查看操作之间的依赖关系，如果当操作依赖于前面的操作，而且由于只有一个调度队列，因此调度队列阻塞（后面所有操作都等待，即使这些操作来自不同的流）
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-22:02:57.png" alt="image-20231205220257004" style="zoom:40%;" /></li>
<li>Hyper-Q：最多32个调度队列和32个流
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-22:03:46.png" alt="image-20231205220346922" style="zoom:40%;" />
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-28-13:49:52.png" alt="image-20231228134952484" style="zoom: 33%;" /></li>
</ul>
</li>
<li>多个流的操作的发射顺序
<ul>
<li>左边将多个流以DFS方式发射，右边将多个流以BFS方式发射
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-22:22:17.png" alt="image-20231205222217200" style="zoom:40%;" /></li>
<li>以DFS方式发射时，流的发射顺序对并行性有影响
<ul>
<li>每种资源都有一个队列</li>
<li>每个流内部很可能有依赖关系</li>
<li>比如先发射Stream1，后发射Stream2：
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-22:16:45.png" alt="image-20231205221645005" style="zoom: 40%;" /></li>
<li>比如先发射Stream2，后发射Stream1：
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-22:17:33.png" alt="image-20231205221733253" style="zoom:40%;" /></li>
</ul>
</li>
</ul>
</li>
<li>每个操作操作具体占用的资源大小差异对并行性也有影响
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-22:25:42.png" alt="image-20231205222542219" style="zoom: 40%;" /></li>
</ul>
<h3 id="使用流隐藏延迟">使用流隐藏延迟</h3>
<h4 id="在默认流中重叠主机和设备计算">在默认流中重叠主机和设备计算</h4>
<ul>
<li>一些cuda runtime api具有隐式同步的效果（比如<code>cudaMemcpy</code>函数），会导致主机阻塞等待</li>
<li>核函数的调用是非阻塞的</li>
</ul>
<h4 id="用多个流重叠多个核函数的执行">用多个流重叠多个核函数的执行</h4>
<ul>
<li>制约加速比的因素：（假设每个CUDA流都执行相同规模的计算）
<ul>
<li>GPU的计算资源（SM数量，每个SM最多允许的线程数量）
<ul>
<li>当CUDA流较少时，增加CUDA流的数量，总耗时只是略微增加，加速比线性增加，此时加速比没有饱和</li>
<li>当CUDA流的个数到达瓶颈，继续增加CUDA流的数量时，总耗时线性增加，加速比饱和
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:24:15.png" style="zoom:80%;" /></li>
</ul>
</li>
<li>单个GPU中能够并发运行的核函数个数的上限
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:24:16.png" style="zoom:80%;" />
<ul>
<li>比如此时能够并发运行的核函数上限为32，Tesla K40有15个SM，每个SM最多允许2048个线程</li>
<li>比如此时一个核函数开1024线程，理论上最多并发运行的核函数$=\min{ \frac{15\times2048}{1024}, 32}=30$，此时限制因素为GPU的计算资源</li>
<li>比如此时一个核函数开128线程，理论上最多并发运行的核函数$=\min { \frac{15 \times 2048}{128}, 32 }=32$，此时限制因素为并发运行核函数的上限</li>
</ul>
</li>
</ul>
</li>
<li>参考
<ul>
<li>《CUDA编程：基础与实践》</li>
</ul>
</li>
</ul>
<h4 id="用多个流重叠核函数的执行与数据传递">用多个流重叠核函数的执行与数据传递</h4>
<ul>
<li>将数据与相应操作分成若干份，每个流中依次进行操作，形成流水线
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:24:19.png" style="zoom:60%;" />
<ul>
<li>理论上最大加速比为3（假设H2D,KER,D2H运行时间相同）</li>
</ul>
</li>
</ul>
<h2 id="同步">同步</h2>
<h3 id="核函数或grid之间的同步">核函数（或grid）之间的同步</h3>
<ul>
<li>背景：连续发射两个核函数，其调度行为未知
<ul>
<li>使用cuda graph显示指定核函数调度顺序（？不确定）</li>
</ul>
</li>
<li>相关函数
<ul>
<li><code>cudaDeviceSynchronize</code>：阻塞host端，直到所有的kernel调用完毕
<ul>
<li>原理是device设置了<code>cudaDeviceScheduleBlockingSync</code>标志，将host线程阻塞</li>
<li>在device中使用 <code>cudaDeviceSynchronize</code>已经被逐渐废弃</li>
</ul>
</li>
<li><code>cudaStreamSynchornize</code>：阻塞host端，直到流中的kernel调用完毕</li>
<li><code>cudaSetDeviceFlags</code>：记录标志，作为活动的host线程执行device代码时使用的标志</li>
<li><code>cudaLaunchKernel</code>：在CPU端使用<code>&lt;&lt;&lt;&gt;&gt;&gt;</code>launch核函数时，实际上调用的是该函数，launch核函数到GPU上执行</li>
</ul>
</li>
</ul>
<h3 id="线程块或block内部的同步">线程块（或Block）内部的同步</h3>
<ul>
<li>barrier：<code>__syncthreads()</code>同步Block内所有线程
<ul>
<li>注意死锁问题：<code>__syncthreads</code>必须能被块内所有线程访问到，即不要将<code>__syncthreads</code>放到if-else语句中</li>
</ul>
</li>
<li><code>__syncthreads</code>的变种：<code>syncthreads_xxx(int predicate)</code>
<ul>
<li>与<code>__syncthreads</code>相同，但是有一个额外的功能：</li>
<li>predicate是一个条件表达式，该变种函数对所有线程评估predicate：
<ul>
<li><code>__syncthreads_or</code>：如果有任意一个线程的predicate值非零，返回非零</li>
<li><code>__syncthreads_and</code>：如果对所有线程的predicate值非零，返回非零</li>
<li><code>__syncthreads_count</code>：统计所有线程中predicate值非零的线程数量</li>
</ul>
</li>
<li>应用：last-block guard确定最后一个线程块（编号最后的线程块未必是最后运行结束的）
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">int</span> <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">bool</span> <span class="nf">lastBlock</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">counter</span><span class="p">){</span> <span class="c1">// 方法一：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">last</span><span class="p">;</span>  <span class="c1">// 表示当前已经调度发射了多少个线程块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nf">__threadfence</span><span class="p">();</span> <span class="c1">// 确保之前计算的结果已经写入内存，对所有线程块可见
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="c1">// 每个块中第一个线程维护last的值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">last</span> <span class="o">=</span> <span class="nf">atomicAdd</span><span class="p">(</span><span class="n">counter</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 原子更新全局内存中的变量，将更新后的值返回到共享内存中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nf">__syncthreads</span><span class="p">();</span> <span class="c1">// 块内所有线程同步，有必要。如果没有线程块内同步，则一个线程块内对last的访问有的是新值，有的是旧值，但是又必须要求一个线程块内部的last值都相同。注意没有保证不同的线程块之间是同步的
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="n">last</span> <span class="o">==</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">bool</span> <span class="nf">lastBlock</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">counter</span><span class="p">){</span> <span class="c1">// 方法二：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nf">__threadfence</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">last</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">// 寄存器变量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">last</span> <span class="o">=</span> <span class="nf">atomicAdd</span><span class="p">(</span><span class="n">counter</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 块内线程不需要完全同步
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="nf">__syncthreads_or</span><span class="p">(</span><span class="n">last</span> <span class="o">==</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span> <span class="c1">// 仍需要使用__syncthreads_or，因为一个线程块内部，只有0号线程的last是用来维护计数的。因此只要0号线程计算完即可确定当前线程块是否为最后一个
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
</ul>
<h3 id="线程块或block之间的同步">线程块（或Block）之间的同步</h3>
<h4 id="全局锁原子操作">全局锁+原子操作</h4>
<p>线程块内选一个代表，通过维护锁变量，代表先进行同步，从而线程块同步</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">__device__</span> <span class="k">volatile</span> <span class="kt">int</span> <span class="n">g_mutex</span><span class="p">;</span> <span class="c1">// 全局锁变量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__deviec__</span> <span class="kt">void</span> <span class="nf">__gpu_sync</span><span class="p">(</span><span class="kt">int</span> <span class="n">goalVal</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">tid_in_block</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">tid_in_block</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span> <span class="c1">// 每个线程块中的0号线程 作为线程块的代表
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="nf">atomicAdd</span><span class="p">((</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">g_mutex</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> 
</span></span><span class="line"><span class="cl">        <span class="k">while</span><span class="p">(</span><span class="n">g_mutex</span> <span class="o">!=</span> <span class="n">goalVal</span><span class="p">){</span> <span class="cm">/* Do nothing */</span><span class="p">}</span> <span class="c1">// 死循环，直到g_mutex到达goalVal的值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 这里，goalVal个线程块之间达成同步
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="无锁方法">无锁方法</h4>
<ul>
<li>将块间同步转换为块内同步
<ul>
<li>为每个线程块分配一个同步变量，形成一个数组<code>Arrayin</code>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-27-11:22:43.png" style="zoom:40%;" /></li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">void</span> <span class="nf">__gpu_sync</span><span class="p">(</span><span class="kt">int</span> <span class="n">goalVal</span><span class="p">,</span> <span class="k">volatile</span> <span class="kt">int</span><span class="o">*</span> <span class="n">Arrayin</span><span class="p">,</span> <span class="k">volatile</span> <span class="kt">int</span><span class="o">*</span> <span class="n">Arrayout</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">tid_in_block</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span> <span class="c1">// 线程在block中的id
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">nBlockNum</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">y</span><span class="p">;</span> <span class="c1">// block数量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">bid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span> <span class="c1">// 线程块id
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">tid_in_block</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="c1">// 每个线程块的0号线程，基于自己线程块的索引，更新Arrayin数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">Arrayin</span><span class="p">[</span><span class="n">bid</span><span class="p">]</span> <span class="o">=</span> <span class="n">goalVal</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 0号线程块进行控制
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">bid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span> <span class="c1">// 将块间同步转换为线程块0号内部的块内同步
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span><span class="p">(</span><span class="n">tid_in_block</span> <span class="o">&lt;</span> <span class="n">nBlockNum</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 0号线程块中，每个线程控制一个线程块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">while</span><span class="p">(</span><span class="n">Arrayin</span><span class="p">[</span><span class="n">tid_in_block</span><span class="p">]</span> <span class="o">!=</span> <span class="n">goalVal</span><span class="p">)</span> <span class="p">{</span> <span class="cm">/* Do nothing */</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="nf">__syncthreads</span><span class="p">();</span> <span class="c1">// 0号线程块内进行同步。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 0号线程块执行到这里，表示所有线程块已经完成初始化Arrayin数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span><span class="p">(</span><span class="n">tid_in_block</span> <span class="o">&lt;</span> <span class="n">nBlockNum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">Arrayout</span><span class="p">[</span><span class="n">tid_in_block</span><span class="p">]</span> <span class="o">=</span> <span class="n">goalVal</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">tid_in_block</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span> <span class="c1">// 每个线程块的0号线程
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">while</span><span class="p">(</span><span class="n">Arrayout</span><span class="p">[</span><span class="n">bid</span><span class="p">]</span> <span class="o">!=</span> <span class="n">goalVal</span><span class="p">)</span> <span class="p">{</span> <span class="cm">/* Do nothing */</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="nf">__syncthreads</span><span class="p">();</span> <span class="c1">// 同步所有块内线程
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="内存fence">内存fence</h4>
<ul>
<li>背景：CUDA 编程模型假定了一种<a href="https://zhuanlan.zhihu.com/p/94421667">弱顺序(weakly-ordered)</a>一致性的内存模型</li>
</ul>
<blockquote>
<ul>
<li>内存一致性（memory consistency）：访存操作在全局中生效（或观察到的）顺序问题， 是指令集所规范的，是软硬件接口的一部分</li>
<li>缓存一致性（cache coherence）：同一个地址在不同的缓存中一致性问题，是完全的硬件实现策略，程序员无关，是集成电路设计者考虑的东西。</li>
</ul>
</blockquote>
<ul>
<li>内存fence：读写操作可能进行重排or优化，添加fence之后，fence之前的op一定比fence之后的op先执行。即抑制编译器重排、抑制乱序。</li>
</ul>
<blockquote>
<ul>
<li>内存fence：The CUDA programming model assumes a device with a weakly-ordered memory model. Memory fence functions can be used to enforce a <a href="https://en.cppreference.com/w/cpp/atomic/memory_order">sequentially-consistent</a> ordering on memory accesses.</li>
<li><code>volatile</code>：声明一个变量，防止编译器优化，防止这个变量存入缓存，如果恰好此时被其他线程改写，那就会造成内存缓存不一致的错误，所以volatile声明的变量始终在全局内存中。</li>
</ul>
</blockquote>
<ul>
<li>内存fence只会影响自己线程中内存操作的顺序，保证自己的数据fence后能够被其他线程安全的访问，并不能像<code>__syncthreads</code>那样保证内存操作对于同block中的其他线程可见</li>
<li>相关函数
<ul>
<li><code>__threadfence_block()</code>：该函数调用后，该线程在此语句前对全局存储器或共享存储器的访问已经全部完成，且结果对block内所有线程可见。</li>
<li><code>__threadfence()</code>：该函数调用后，该线程在此语句前对全局存储器或共享存储器的访问已经全部完成，且结果对grid内所有线程可见。</li>
<li><code>__threadfence_system()</code>：该函数调用后，该线程在此语句前对全局存储器或共享存储器的访问已经全部完成，且结果对system（CPU+GPU）内所有线程可见。</li>
</ul>
</li>
<li>参考：
<ul>
<li><a href="https://www.bilibili.com/read/cv18722474/">CUDA内存栅栏（Memory Fence）理解</a></li>
</ul>
</li>
</ul>
<h3 id="warp同步">warp同步</h3>
<h4 id="warp内inter-warp同步">warp内（inter-warp）同步</h4>
<ul>
<li>
<p>barrier：<code>__syncwarps()</code>同步一个warp中的线程</p>
</li>
<li>
<p>线程束内函数都有 <code>_sync</code> 后缀，表示这些函数都具有隐式的同步功能。</p>
<ul>
<li>线程束表决函数（warp vote functions）
<ul>
<li><code>unsigned __ballot_sync(unsigned mask, int predicate)</code>：如果线程束内第n个线程参与计算（旧掩码）且predicate值非零，则返回的无符号整型数（新掩码）的第n个二进制位为1，否则为0</li>
<li><code>int __all_sync(unsigned mask, int predicate)</code>：线程束内所有参与线程的predicate值均非零，则返回1，否则返回0</li>
<li><code>int __any_sync(unsigned mask, int predicate)</code>：线程束内所有参与线程的predicate值存在非零，则返回1， 否则返回0</li>
</ul>
</li>
<li>线程束匹配函数（warp match functions）</li>
<li>线程束洗牌函数（warp shuffle functions）：最后一个参数表示逻辑上的warp大小
<ul>
<li><code>T __shfl_sync(unsigned mask, T v, int srcLane, int w = warpSize)</code>：参与线程返回标号为 srcLane 的线程中变量 v 的值。该函数将一个线程中的数据广播到所有线程。</li>
<li><code>T __shfl_up_sync(unsigned mask, T v, unsigned d, int w=warpSize)</code>：标号为t的参与线程返回标号为 t-d 的线程中变量v的值，t-d&lt;0的线程返回t线程的变量v。该函数是一种将数据向上平移的操作，即将低线程号的值平移到高线程号。
<ul>
<li>例如当w=8、d=2时，2-7号线程将返回 0-5号线程中变量v的值；0-1号线程返回自己的 v。</li>
</ul>
</li>
<li><code>T __shfl_down_sync(unsigned mask, T v, unsigned d, int w=warpSize)</code>：标号为t的参与线程返回标号为 t+d 的线程中变量v的值，t+d&gt;w的线程返回t线程的变量v。该函数是一种将数据向下平移的操作，即将高线程号的值平移到低线程号。</li>
</ul>
<ul>
<li>例如当w=8、d=2时，0-5号线程将返回2-7号线程中变量v的值，6-7号线程将返回自己的 v。</li>
</ul>
<ul>
<li><code>T __shfl__xor_sync(unsigned mask, T v, int laneMask, int w=warpSize)</code>：标号为t的参与线程返回标号为 t^laneMask 的线程中变量 v 的值。该函数让线程束内的线程两两交换数据。</li>
</ul>
</li>
<li>线程束矩阵函数（warp matrix functions）</li>
</ul>
</li>
<li>
<p>例子：使用warp shuffle函数进行规约：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">reduce_shfl</span><span class="p">(</span><span class="k">const</span> <span class="n">real</span> <span class="o">*</span><span class="n">d_x</span><span class="p">,</span> <span class="n">real</span> <span class="o">*</span><span class="n">d_y</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// tid从0到blockDim.x
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">bid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">bid</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">extern</span> <span class="n">__shared__</span> <span class="n">real</span> <span class="n">s</span><span class="p">[];</span> <span class="c1">// 比如大小128
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">s</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="o">?</span> <span class="n">d_x</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">:</span> <span class="mf">0.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">FULL_MASK</span> <span class="o">=</span> <span class="mh">0xffffffff</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="nf">__syncthreads</span><span class="p">();</span> <span class="c1">// 线程块同步函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;=</span> <span class="mi">32</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">offset</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="n">s</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">s</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">offset</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">real</span> <span class="n">y</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">+=</span> <span class="nf">__shfl_down_sync</span><span class="p">(</span><span class="n">FULL_MASK</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">offset</span><span class="p">);</span> <span class="c1">// 线程tid返回线程tid+offset中寄存器变量y的值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nf">atomicAdd</span><span class="p">(</span><span class="n">d_y</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h4 id="协作组">协作组</h4>
<p>协作组（cooperative groups）:提供了线程块以上级别的同步</p>
<ul>
<li>
<p><code>thread_group</code></p>
<ul>
<li>协作组编程模型中最基本的类型，是线程块级别的协作组</li>
<li>成员函数：
<ul>
<li><code>void sync()</code>，同步组内所有线程；（相当于<code>__syncthreads</code>函数）</li>
<li><code>unsigned size()</code>，返回组内总的线程数目，即组的大小；</li>
<li><code>unsigned thread_rank()</code>，返回当前调用该函数的线程在组内的标号（从0计数）</li>
<li><code>bool is_valid()</code>，如果定义的组违反了任何cuda限制，返回 false，否则true</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>thread_block</code>继承于<code>thread_group_base&lt;T&gt;</code>，<code>thread_group_base&lt;T&gt;</code>继承于<code>thread_group</code></p>
<ul>
<li><code>dim3 group_index()</code>，返回当前调用该函数的线程的线程块指标，等价于 <code>blockIdx</code>；</li>
<li><code>dim3 thread_index()</code>，返回当前调用该函数的线程的线程指标，等价于 <code>threadIdx</code>；</li>
<li><code>this_thread_block()</code>：初始化一个<code>thread_block</code>对象</li>
<li><code>tiled_partition()</code> ：将一个<code>thread_block</code>划分为若干片（tile），每片构成一个<code>thread_group</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cooperative_groups.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">using</span> <span class="n">namespace</span> <span class="n">cooperative_groups</span><span class="p">;</span> <span class="c1">// 相关变量和函数定义在该命名空间下
</span></span></span><span class="line"><span class="cl"><span class="c1">// namespace cg = cooperative_groups; // 取别名
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="n">thread_block</span> <span class="n">g</span> <span class="o">=</span> <span class="nf">this_thread_block</span><span class="p">();</span>  <span class="c1">// g相当于一个之前的线程块，这里将其包装为一个类型
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">thread_group</span> <span class="n">myWarp</span> <span class="o">=</span> <span class="nf">tiled_partition</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">32</span><span class="p">);</span> <span class="c1">// 将thread_block划分为thread_group
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">thread_group</span> <span class="n">g4</span> <span class="o">=</span> <span class="nf">tiled_partition</span><span class="p">(</span><span class="n">myWarp</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span> <span class="c1">// 可以将thread_group进一步细分
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>thread_block_tile</code></p>
<ul>
<li>使用模板，在编译期划分 <strong>线程块片（thread block tile）</strong>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">thread_block_tile</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span> <span class="n">g32</span> <span class="o">=</span> <span class="n">tiled_partition</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="nf">this_thread_block</span><span class="p">());</span>
</span></span><span class="line"><span class="cl"><span class="n">thread_block_tile</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span> <span class="n">g4</span> <span class="o">=</span> <span class="n">tiled_partition</span><span class="o">&lt;</span><span class="mi">4</span><span class="o">&gt;</span><span class="p">(</span><span class="nf">this_thread_block</span><span class="p">());</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
<li>
<p>线程块片具有额外的函数（类似线程束内函数）：</p>
<ul>
<li><code>unsigned ballot(int predicate)</code>;</li>
<li><code>int all(int predicate)</code>;</li>
<li><code>int any(int predicate)</code>;</li>
<li><code>T shfl(T v, int srcLane)</code>;</li>
<li><code>T shfl_up(T v, unsigned d)</code>;</li>
<li><code>T shfl_down(T v, unsigned d)</code>;</li>
<li><code>T shfl_xor(T v, unsigned d)</code>;</li>
</ul>
<ul>
<li>与一般的线程束不同，线程组内的所有线程都要参与代码运行计算；同时，线程组内函数不需要指定宽度，因为该宽度就是线程块片的大小。</li>
</ul>
</li>
<li>
<p>例子：使用协作组进行规约：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">reduce_cp</span><span class="p">(</span><span class="k">const</span> <span class="n">real</span> <span class="o">*</span><span class="n">d_x</span><span class="p">,</span> <span class="n">real</span> <span class="o">*</span><span class="n">d_y</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// tid从0到blockDim.x
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">bid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">bid</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">extern</span> <span class="n">__shared__</span> <span class="n">real</span> <span class="n">s</span><span class="p">[];</span> <span class="c1">// 比如大小128
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">s</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="o">?</span> <span class="n">d_x</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">:</span> <span class="mf">0.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="nf">__syncthreads</span><span class="p">();</span> <span class="c1">// 线程块同步函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;=</span> <span class="mi">32</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">offset</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="n">s</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">s</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">offset</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">real</span> <span class="n">y</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">thread_block_tile</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span> <span class="n">g</span> <span class="o">=</span> <span class="n">tiled_patition</span><span class="o">&lt;</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="nf">this_thread_block</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">+=</span> <span class="n">g</span><span class="p">.</span><span class="nf">shfl_down</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span> <span class="c1">// 使用协作组的成员函数与使用warp shuffle函数具有等价的执行效率
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nf">atomicAdd</span><span class="p">(</span><span class="n">d_y</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>more</p>
<ul>
<li><a href="https://www.zhihu.com/question/586453330/answer/3232856921">https://www.zhihu.com/question/586453330/answer/3232856921</a></li>
</ul>
</li>
</ul>
<h3 id="原子操作">原子操作</h3>
<ul>
<li>两类原子函数：
<ul>
<li>atomicAdd_system：将原子函数的作用范围拓展到所有节点（host和device）</li>
<li>atomicAdd_block：将原子函数的作用范围缩小至一个线程块</li>
<li>一个特殊的原子函数：<code>atomicCAS</code>，所有其他原子函数都可以使用它来实现</li>
</ul>
</li>
<li>相关语法：
<ul>
<li>原子函数的返回值都是原来的旧值</li>
<li>原子函数都是<code>__device__</code>函数，只能在核函数中使用</li>
<li>原子函数操作的地址可以位于全局内存，也可以位于共享内存</li>
<li>原子操作开销与是否存在竞争相关，且参与竞争者越少，开销越小</li>
</ul>
</li>
<li>例子：使用原子函数进行规约
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">__global__</span> <span class="nf">reduce_shared</span><span class="p">(</span><span class="n">real</span> <span class="o">*</span><span class="n">d_x</span><span class="p">,</span> <span class="n">real</span> <span class="o">*</span><span class="n">d_y</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">bid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">extern</span> <span class="n">__shared__</span> <span class="n">real</span> <span class="n">s_y</span><span class="p">[];</span> <span class="c1">// 动态共享内存
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">s_y</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="o">?</span> <span class="n">d_x</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">:</span> <span class="mf">0.0</span><span class="p">;</span> <span class="c1">// 将全局内存中的数据拷贝到线程块对应的共享内存中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nf">__syncthreads</span><span class="p">();</span> <span class="c1">// 保证一个线程块中的同步，但是不能保证不同线程块之间的同步
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">offset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">s_y</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">s_y</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">offset</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nf">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s_y</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="c1">// 使用原子操作，将结果累加到d_y[0]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>GPU架构发展、兼容性和编译</title>
      <link>https://qinganzhang.github.io/posts/cuda-learning-notes/gpu%E6%9E%B6%E6%9E%84%E5%8F%91%E5%B1%95%E5%85%BC%E5%AE%B9%E6%80%A7%E5%92%8C%E7%BC%96%E8%AF%91/</link>
      <pubDate>Thu, 01 Feb 2024 22:00:30 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/cuda-learning-notes/gpu%E6%9E%B6%E6%9E%84%E5%8F%91%E5%B1%95%E5%85%BC%E5%AE%B9%E6%80%A7%E5%92%8C%E7%BC%96%E8%AF%91/</guid>
      <description>CUDA编译链和兼容性 兼容性 CPU与GPU的区别 CPU只有少量的计算核心，有更多晶体管用于数据缓存和流程控制， GPU有大量计算能力较弱的计算</description>
      <content:encoded><![CDATA[<h2 id="cuda编译链和兼容性">CUDA编译链和兼容性</h2>
<h3 id="兼容性">兼容性</h3>
<ul>
<li>
<p>CPU与GPU的区别</p>
<ul>
<li>CPU只有少量的计算核心，有更多晶体管用于数据缓存和流程控制，</li>
<li>GPU有大量计算能力较弱的计算核心，用于控制和缓存的晶体管较少</li>
</ul>
</li>
<li>
<p>GPU系列：</p>
<ul>
<li>Tesla系列：使用ECC内存，用于科学计算。后来也叫Data Center GPUs。</li>
<li>Quadro系列：专业级，用于OpenGL、CAD等需要高精度计算的场景。后来也叫Workstation GPUs。</li>
<li>GeForce系列：消费级，用于游戏和计算，但是没有ECC</li>
<li>Tegra系列：移动处理器</li>
<li>Jetson系列：嵌入式</li>
</ul>
</li>
<li>
<p>GPU架构、计算能力与对应系列</p>
<ul>
<li>计算能力（Compute Capability）决定了GPU硬件支持的功能，反映了设备支持的指令集及其他规范，也称SM version，注意GPU计算能力不等价于计算性能
<table>
<thead>
<tr>
<th style="text-align:center">架构</th>
<th style="text-align:center">计算能力Compute Capability</th>
<th style="text-align:center">发布时间</th>
<th style="text-align:center">Tesla系列</th>
<th style="text-align:center">Quadro系列</th>
<th style="text-align:center">GeForce系列</th>
<th style="text-align:center">Jetson系列</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Tesla</td>
<td style="text-align:center">X = 1</td>
<td style="text-align:center">2006</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Fermi</td>
<td style="text-align:center">X = 2</td>
<td style="text-align:center">2010</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Kepler</td>
<td style="text-align:center">X = 3</td>
<td style="text-align:center">2012</td>
<td style="text-align:center">Kepler K系列</td>
<td style="text-align:center">Quadro K系列</td>
<td style="text-align:center">GeForce 600/700系列</td>
<td style="text-align:center">Tegra K1</td>
</tr>
<tr>
<td style="text-align:center">Maxwell</td>
<td style="text-align:center">X = 5</td>
<td style="text-align:center">2014</td>
<td style="text-align:center">Maxwell M系列</td>
<td style="text-align:center">Quadro M系列</td>
<td style="text-align:center">GeForce 900系列</td>
<td style="text-align:center">Tegra X1</td>
</tr>
<tr>
<td style="text-align:center">Pascal</td>
<td style="text-align:center">X = 6</td>
<td style="text-align:center">2016</td>
<td style="text-align:center">Pascal P系列</td>
<td style="text-align:center">Quadro P系列</td>
<td style="text-align:center">GeForce 10系列</td>
<td style="text-align:center">Tegra X2</td>
</tr>
<tr>
<td style="text-align:center">Volta</td>
<td style="text-align:center">X = 7</td>
<td style="text-align:center">2017</td>
<td style="text-align:center">Tesla V系列</td>
<td style="text-align:center">-</td>
<td style="text-align:center">TITAN V</td>
<td style="text-align:center">AGX Xavier</td>
</tr>
<tr>
<td style="text-align:center">Turing</td>
<td style="text-align:center">X.Y = 7.5</td>
<td style="text-align:center">2018</td>
<td style="text-align:center">Tesla T系列</td>
<td style="text-align:center">Quadro RTX系列</td>
<td style="text-align:center">GeForce 16系列，GeForce 20系列</td>
<td style="text-align:center">AGX Xavier</td>
</tr>
<tr>
<td style="text-align:center">Ampere</td>
<td style="text-align:center">X = 8</td>
<td style="text-align:center">2020</td>
<td style="text-align:center">Tesla A系列</td>
<td style="text-align:center">RTX A系列</td>
<td style="text-align:center">GeForce 30系列</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Ada Lovelace</td>
<td style="text-align:center">X.Y = 8.9</td>
<td style="text-align:center">2022</td>
<td style="text-align:center">L4、L40</td>
<td style="text-align:center">RTX Ada系列</td>
<td style="text-align:center">GeForce 40系列</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Hopper</td>
<td style="text-align:center">X = 9</td>
<td style="text-align:center">2022</td>
<td style="text-align:center">H100</td>
<td style="text-align:center"></td>
<td style="text-align:center">-</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
</ul>
<h3 id="cuda开发平台">CUDA开发平台</h3>
<ul>
<li>CUDA 提供两个编程接口
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-27-15:12:04.png" alt="image-20231227151204463" style="zoom:50%;" />
<ul>
<li>CUDA driver API：low-level
<ul>
<li>CUDA (driver) library由NVIDIA driver安装，比如常用的共享库<code>libcuda.so</code>，对应头文件为<code>cuda.h</code>，里面提供的API称为CUDA driver API
<ul>
<li>同时可以看到NVIDIA driver的版本：<code>find / -name libcuda.*</code></li>
</ul>
</li>
<li>NVIDIA driver同时包含了<code>nvidia-smi</code>命令，可以看到NVIDIA driver的版本，以及当前NVIDIA driver支持的最高CUDA版本（向下兼容）</li>
<li>或者使用函数<code>cudaDriverGetVersion(int* driverVersion)</code></li>
</ul>
</li>
<li>CUDA runtime API：high-level
<ul>
<li>CUDA Runtime library由CUDA Toolkit安装，比如常用的共享库<code>libcudart.so</code>，对应头文件为<code>cuda_runtime.h</code>，里面提供的API称为CUDA runtime API
<ul>
<li><code>cuda_runtime_api.h</code>是纯C版本，<code>cuda_runtime.h</code>是C++版本</li>
<li>离线安装的CUDA工具包会默认携带与之匹配特定的驱动程序</li>
</ul>
</li>
<li>CUDA Toolkit中同时包含了一些工具比如编译器nvcc，<code>nvcc -V</code>显示的CUDA版本是runtime API版本
<ul>
<li>cuda driver API版本（即驱动支持的最高cuda版本）应该高于cuda runtime API版本（即当前安装的cuda toolkit版本）</li>
</ul>
</li>
<li>或者使用函数<code>cudaRuntimeGetVersion(int *runtimeVersion)</code></li>
</ul>
</li>
</ul>
</li>
<li>注意不要将GPU计算能力与CUDA (driver/runtime)版本混淆</li>
<li>参考
<ul>
<li><a href="https://leimao.github.io/blog/CUDA-Driver-VS-CUDA-Runtime/"># CUDA Driver VS CUDA Runtime</a></li>
<li><a href="https://blog.csdn.net/jslove1997/article/details/113737934"># cuda 的driver API 和 runtime API</a></li>
</ul>
</li>
</ul>
<h3 id="编译相关">编译相关</h3>
<h4 id="编译过程">编译过程</h4>
<ul>
<li>
<p><a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#the-cuda-compilation-trajectory">编译过程</a>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-27-23:36:39.png" alt="image-20231227233638914" style="zoom:67%;" /></p>
<ul>
<li>编译device code
<ul>
<li>首先将预处理之后的C++ code经过CICC compiler编译成PTX code
<ul>
<li>PTX可以视为虚架构的汇编，虚架构体现了应用程序对GPU计算能力的要求，版本尽量低，因此可以适用于更加广泛的GPU架构</li>
</ul>
</li>
<li>再使用ptxas (PTX optimizing assembler)，根据实架构，将PTX code编译成cubin二进制机器码
<ul>
<li>.cubin：CUDA device code binary file (CUBIN) for a single GPU architecture</li>
</ul>
</li>
<li>将PTX code和cubin放到fatbin.c文件中
<ul>
<li>.fatbin：CUDA fat binary file that may contain multiple PTX and CUBIN files</li>
</ul>
</li>
</ul>
</li>
<li>编译host code
<ul>
<li>将预处理之后的C++ code，使用cudafe++将host和device部分分离</li>
<li>分离后的host代码，结合device code部分得到的fatbin.c文件，进行编译
<ul>
<li>在host code看来，device code其实就是一段数据。</li>
</ul>
</li>
</ul>
</li>
<li>对每一个.cu文件都执行单独的host code和device code编译</li>
<li>链接：
<ul>
<li>使用nvlink将所有.o目标文件中的device code重新链接到一个cubin文件中，并通过fatbinary转换为.fatbin.c文件</li>
<li>将.fatbin.c文件，结合一些其他的文件，编译得到device code最终对应的目标文件</li>
<li>将host code的目标文件和device code最终的目标文件链接起来，得到最终的可执行文件</li>
</ul>
</li>
</ul>
</li>
<li>
<p>使用</p>
<ul>
<li>
<p>需要选项 <code>-arch=compute_XY</code> 指定一个PTX虚拟架构的计算能力，虚架构版本：<a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#virtual-architecture-feature-list">Virtual Architecture Feature List</a></p>
<table>
<thead>
<tr>
<th style="text-align:center">Architecture</th>
<th style="text-align:center">虚架构</th>
<th style="text-align:center">实架构</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Maxwell</td>
<td style="text-align:center"><code>compute_50</code>，<code>compute_52</code>，<code>compute_53</code></td>
<td style="text-align:center"><code>sm_50</code>，<code>sm_52</code>，<code>sm_53</code></td>
</tr>
<tr>
<td style="text-align:center">Pascal</td>
<td style="text-align:center"><code>compute_60</code>，<code>compute_61</code>，<code>compute_62</code></td>
<td style="text-align:center"><code>sm_60</code>，<code>sm_61</code>，<code>sm_62</code></td>
</tr>
<tr>
<td style="text-align:center">Volta</td>
<td style="text-align:center"><code>compute_70</code>，<code>compute_72</code></td>
<td style="text-align:center"><code>sm_70</code>，<code>sm_72</code></td>
</tr>
<tr>
<td style="text-align:center">Turing</td>
<td style="text-align:center"><code>compute_75</code></td>
<td style="text-align:center"><code>sm_75</code></td>
</tr>
<tr>
<td style="text-align:center">Ampere</td>
<td style="text-align:center"><code>compute_80</code>，<code>compute_86</code>，<code>compute_87</code></td>
<td style="text-align:center"><code>sm_80</code>，<code>sm_86</code>，<code>sm_87</code></td>
</tr>
<tr>
<td style="text-align:center">Ada Lovelace</td>
<td style="text-align:center"><code>compute_89</code></td>
<td style="text-align:center"><code>sm_89</code></td>
</tr>
<tr>
<td style="text-align:center">Hopper</td>
<td style="text-align:center"><code>compute_90</code>，<code>compute_90a</code></td>
<td style="text-align:center"><code>sm_90</code>，<code>sm_90a</code></td>
</tr>
</tbody>
</table>
</li>
<li>
<p>需要选项 <code>-code=sm_ZW</code> 指定一个真实架构的计算能力，实架构版本：<a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list">GPU Feature List</a></p>
<ul>
<li>CUDA二进制兼容性只能保证局限在相同大版本计算能力的架构中</li>
<li>实架构的计算能力必须大于等于虚架构的计算能力</li>
</ul>
</li>
<li>
<p>如果希望编译出来的文件能在更多的GPU上运行，可以使用<code>-gencode</code>同时指定多组计算能力，生成多个PTX版本代码，例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">-gencode arch=compute_35, code=sm_35
</span></span><span class="line"><span class="cl">-gencode arch=compute_50, code=sm_50
</span></span><span class="line"><span class="cl">-gencode arch=compute_60, code=sm_60
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>此时，编译出来的可执行文件将包含3个二进制版本，在不同架构的GPU中运行时会自动选择对应的二进制版本</li>
<li><code>-code=</code>可以指定虚架构，此时将进行即时编译，只会包含PTX代码</li>
</ul>
</li>
<li>
<p>如果在运行期间找不到当前架构的二进制版本代码，则使用即时编译</p>
<ul>
<li>即时编译推迟cubin的生成，将PTX代码在runtime内编译成cubin然后执行，因为runtime时已经知道当前运行在哪种GPU架构中，因此可以直接生成</li>
<li>缺点是增加了程序的启动延迟，但是可以使用编译缓存来缓解</li>
</ul>
</li>
<li>
<p>默认cuda以whole program compilation mode来编译</p>
</li>
</ul>
</li>
<li>
<p>reference and more reading</p>
<ul>
<li><a href="https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/"># Matching CUDA arch and CUDA gencode for various NVIDIA architectures</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/409154399"># CUDA 编译与 NVVM IR 笔记</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/432674688"># 银河系CUDA编程指南(2.5)——NVCC与PTX</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/623756901"># NVCC编译流程+中间文件+GDB调试cuda程序初探</a></li>
<li><a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#">nvcc文档</a></li>
<li><a href="https://blog.csdn.net/feng__shuai/article/details/111319185"># CUDA编译过程</a></li>
</ul>
</li>
</ul>
<h4 id="nvcc编译选项">nvcc编译选项</h4>
<ul>
<li><code>-g</code>：在host端生成调试信息</li>
<li><code>-G</code>：在device端生成调试信息。如果<code>-dopt</code>未指定，则关闭编译优化。</li>
<li><code>-lineinfo</code>：为device端生成行号，同时将source information嵌入到可执行文件中</li>
<li><code>-dopt</code>：如果<code>-G</code>没有指定，则<code>-dopt=on</code>，允许device端代码编译优化。如果<code>-G</code>指定，enables limited debug information generation for optimized device code</li>
</ul>
<p>常用编译命令：</p>
<ul>
<li><code>nvcc -lineinfo -arch=compute_86 -code=sm_86 </code>
<ul>
<li>或者<code>alias mynvcc='nvcc -lineinfo -arch=compute_86 -code=sm_86'</code></li>
</ul>
</li>
</ul>
<h2 id="架构发展">架构发展</h2>
<h3 id="overview">Overview</h3>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-27-11:38:55.png" alt="image-20231227113855018" style="zoom: 80%;" />
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-27-11:39:54.png" alt="image-20231227113954626" style="zoom:80%;" />
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-27-11:40:15.png" alt="image-20231227114015125" style="zoom:80%;" />
<h3 id="tesla">Tesla</h3>
<h4 id="g80">G80</h4>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-16-18:23:37.png" alt="image-20231216182335365" style="zoom:50%;" />
<ul>
<li>SP（Streaming Processor）：scalar ALU for a single CUDA thread
<ul>
<li>ALU执行是流水线化的，即一项操作会被分为X个步骤由X个组件去处理，每个步骤都耗费1周期。虽然一条指令要X周期才能执行完，但对于每个组件只要1周期就执行完了，所以每个周期都能送入一份数据进ALU。
<ul>
<li>SP的频率是调度单元（以及外部纹理单元等）的2倍，所以在调度单元看来，是需要2周期去消化1条指令。</li>
</ul>
</li>
</ul>
</li>
<li>SM（Streaming Mulitprocessor）：每个线程块分配到一个SM上
<ul>
<li>SM的频率是GPU频率的两倍</li>
</ul>
</li>
</ul>
<h4 id="scoreboarding">scoreboarding</h4>
<ul>
<li>
<p>作用：在指令发射阶段，检查待发射的指令是否与正在执行但尚未写回寄存器的指令之间存在数据相关。三种数据相关：
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-26-18:56:45.png" alt="image-20231226185644918" style="zoom:70%;" /></p>
</li>
<li>
<p>大致原理：scoreboard为每个warp寄存器分配一个bit来记录相应寄存器的写完成状态，</p>
<ul>
<li>如果后序指令不存在数据相关，则进入流水线</li>
<li>如果存在数据相关，通过检查标识位，后续指令就会stall而无法发射，此时可以切换其他warp的指令进行调度</li>
</ul>
</li>
<li>
<p>参考</p>
<ul>
<li><a href="https://blog.csdn.net/angus_huang_xu/article/details/105746777"># ILP——指令级并行2：记分牌（Scoreboard）技术</a></li>
<li>通用图形处理器设计3.5</li>
</ul>
</li>
</ul>
<h3 id="fermi">Fermi</h3>
<p>以<a href="https://www.ece.lsu.edu/gp/refs/gf100-whitepaper.pdf">GF100</a>为例，架构：
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-16:18:45.png" alt="image-20231220161845919" style="zoom:80%;" /></p>
<h4 id="特点">特点</h4>
<ul>
<li><strong>第三代流式多处理器（SM）</strong>
<ul>
<li>每个SM有<strong>32</strong>个CUDA核心，比GT200多<strong>4倍</strong></li>
<li><strong>双精度浮点</strong>峰值性能比GT200提高<strong>8倍</strong></li>
<li><strong>两个warp调度器</strong>，可以同时调度和分发指令给两个独立的warp</li>
<li><strong>64KB RAM</strong>，可供共享内存和L1缓存配置化划分使用</li>
</ul>
</li>
<li><strong>第二代并行线程执行ISA（Instruction Set Architecture指令集架构）</strong>
<ul>
<li>统一地址空间，完全支持C++（比如虚函数、new/delete等）</li>
<li>针对OpenCL和DirectCompute进行了优化</li>
<li>完全支持<strong>IEEE 754-2008</strong> 32位和64位精度</li>
<li>具有64位扩展的完整32位整数路径</li>
<li>内存访问指令支持向64位寻址的过渡</li>
<li>通过<strong>预测</strong>提高性能
<ul>
<li>Predication enables short conditional code segments to execute efficiently with no branch instruction overhead</li>
</ul>
</li>
</ul>
</li>
<li><strong>改进的内存子系统</strong>
<ul>
<li>具有<strong>可配置L1和统一L2高速缓存</strong>的NVIDIA Parallel DataCacheTM层次结构
<ul>
<li>之前Tesla架构没有L1、L2 cache</li>
</ul>
</li>
<li>支持<strong>ECC</strong>内存的第一款GPU</li>
<li>大大提高原子内存操作性能</li>
</ul>
</li>
<li><strong>NVIDIA GigaThread引擎</strong>
<ul>
<li>应用程序上下文<strong>切换速度提高了10倍</strong></li>
<li>并发内核执行</li>
<li>无序线程块执行</li>
<li><strong>双向可重叠</strong>的内存传输引擎</li>
</ul>
</li>
</ul>
<h4 id="sm">SM</h4>
<ul>
<li>
<p>SM架构：有4个执行端口
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-10:29:56.png" alt="image-20231220102952702" style="zoom: 80%;" /></p>
<ul>
<li>core
<ul>
<li>每个CUDA处理器都有完全<strong>流水线化</strong>的整数算术逻辑单元（ALU）和浮点单位（FPU）</li>
</ul>
</li>
<li>LD/ST单元
<ul>
<li>每个SM有16个Load/Store单元，允许16个线程每个时钟周期计算源和目的地址，支持将每个地址的数据读取和存储到缓存或DRAM中</li>
</ul>
</li>
<li>SFU</li>
</ul>
</li>
<li>
<p>dual warp scheduler
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-10:58:34.png" alt="image-20231220105834360" style="zoom:50%;" /></p>
<ul>
<li>每个SM有两个warp scheduler和两个instruction dispatch unit，每个周期可以同时issue和execute两个warp
<ul>
<li>warp scheduler：选择warp</li>
<li>Instruction Dispatch Unit：将指令发送到对应的端口（16个core、或16个LD/ST、或4个SFU中）</li>
<li>由于SP（或者core）的频率是调度单元的2倍，因此调度单元一个周期选择一个warp，一个周期内2倍频率的core连续两次在half-warp上执行</li>
<li>由于SFU只有4个，因此一个warp在SFU中计算需要消耗8个周期，但是此时它不阻塞调度</li>
</ul>
</li>
<li>由于warp之间独立运行，因此warp scheduler不需要检查指令流中的依赖关系</li>
<li>大多数指令可以这样同时dual issue，两个整数指令、两个浮点指令或混合发出整数、浮点、加载、存储和 SFU 指令；但是双精度指令不支持dual dispatch with any other operation</li>
</ul>
</li>
<li>
<p>G80/GT200/Fermi对比
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-11:01:59.png" alt="image-20231220110159265" style="zoom: 40%;" /></p>
</li>
</ul>
<h4 id="isa">ISA</h4>
<ul>
<li>Fermi是第一个支持PTX2.0的架构
<ul>
<li>PTX2.0统一了各种内存空间的寻址</li>
</ul>
</li>
</ul>
<h4 id="gigathread">GigaThread</h4>
<ul>
<li>两级thread scheduler
<ul>
<li>thread scheduler：将线程块调度和分配到SM，GigaThread</li>
<li>warp scheduler：将warp调度和分配到执行单元</li>
</ul>
</li>
<li>特点：
<ul>
<li>应用程序上下文切换速度更快</li>
<li>concurrent(并发) kernel执行：（感觉下面的图画得有些confused？）
<ul>
<li>同一应用程序上下文的不同kernel可以并行在GPU上执行</li>
<li>不同应用程序上下文的kernel可以顺序执行
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-11:24:02.png" alt="image-20231220112402824" style="zoom:80%;" /></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="kepler">Kepler</h3>
<p><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf">GK110/210</a>是Kepler架构中高端型号，用于科学计算，因此主要以这两种型号为基础来介绍kepler架构。总体架构：15个SMX
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-14:00:18.png" alt="image-20231220140018532" style="zoom: 50%;" /></p>
<h4 id="smx">SMX</h4>
<ul>
<li>
<p>架构
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-14:18:50.png" alt="image-20231220141850070" style="zoom:80%;" /></p>
</li>
<li>
<p>SMX中的core的频率与主GPU频率相同，以增大面积为代价，降低功耗
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-16:58:59.png" alt="image-20231220165859291" style="zoom:33%;" /></p>
</li>
<li>
<p>warp scheduler
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-15:04:24.png" alt="image-20231220150424654" style="zoom:40%;" /></p>
<ul>
<li>4个warp scheduler和8个instruction dispatch unit
<ul>
<li>warp scheduler中调度的warp，对应的2个instruction dispatch unit可以在一个周期分配给该warp两个独立的指令</li>
<li>两个指令中允许双精度指令与其他指令dual dispatch</li>
</ul>
</li>
<li>kepler架构针对warp scheduler在降低功耗方面的优化：从硬件的动态调度转向编译器辅助的静态调度
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-16:49:07.png" alt="image-20231220164907711" style="zoom: 50%;" /></li>
<li>Fermi用硬件scoreboard来记录寄存器的使用信息，从而确定指令之间的依赖关系
<ul>
<li>硬件scoreboard就是记录各个组件（寄存器、执行单元）当下的情况，并自动根据指令涉及的操作数、ALU去匹配。</li>
</ul>
</li>
<li>到了Kepler架构，因为指令的执行周期是可以预计的，所以调度信息其实在编译期就能确定了。于是ISA就做了更改，每7条指令为一组，附加一段调度信息（Control Code），把因为数据依赖需要等待的cycle数记录进去。硬件上许多动态调度的模块被砍掉了，节省了功耗。
<ul>
<li>访存指令的延迟依旧是没法预计的，因为不知道有没有cache miss，所以遇到访存指令势必需要一个等待数据就绪的同步过程，可以借助软件scoreboard来完成。</li>
<li>软件scoreboard可以看作是预分配几个信条量，有依赖关系的指令会显式声明对哪几个信号量做操作，这样一来要记录维护的信息变少了，逻辑也简单了。同时软件scoreboard没有dependency check，一方面可以将这部分卸载到编译器，另一方面考虑到dependency不多</li>
</ul>
</li>
</ul>
</li>
<li>
<p>cache</p>
<ul>
<li>Kepler的L1 Cache是用来为reg spill或者stack data服务的，即访存数据其实并不会缓存在L1里。</li>
<li>对于那些readonly的global memory，允许借用Tex Cache</li>
</ul>
</li>
<li>
<p>shuffle指令：warp可以读取来自warp内其他线程中任意排列的值，因此节省了共享内存</p>
</li>
</ul>
<h4 id="dynamic-parallelism">Dynamic Parallelism</h4>
<h4 id="hyper-q">Hyper-Q</h4>
<ul>
<li>之前架构中只有一个CPU与GPU的工作分配器（CWD）之间的硬件工作队列，多个流复用一个队列，可能造成虚假的依赖性</li>
<li>现在有32个硬件工作队列</li>
</ul>
<h4 id="grid-management-unit">Grid Management Unit</h4>
<ul>
<li>为了支持动态并行，需要改变对grid的管理</li>
</ul>
<h4 id="nvidia-gpudirect">NVIDIA GPUDirect</h4>
<ul>
<li>可以实现 GPU 与其他设备（例如网络接口卡 (NIC) 和存储设备）之间的直接通信和数据传输，但是中间数据不经过CPU</li>
</ul>
<h3 id="maxwell">Maxwell</h3>
<p>以GM204为例，4个GPC，每个GPC有4个SMM
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-17:44:14.png" alt="image-20231220174414182" style="zoom: 50%;" /></p>
<h4 id="特点-1">特点</h4>
<ul>
<li>更高效的SM（Maxwell SM，也称SMM）：core数量减少但是效率增加
<ul>
<li>指令调度提升
<ul>
<li>所有核心的SMM功能单元都分配给特定的调度器，没有共享单元。</li>
<li>每个分区中的core数量是32，warp scheduler方便调度</li>
<li>支持双发射（两个独立的指令，比如一个计算一个访存），也支持单发射（此时正好调度到一个warp）</li>
</ul>
</li>
<li>现有代码的占用率增加：每个SM上active的block数量翻倍</li>
<li>减少算数指令延迟</li>
</ul>
</li>
<li>更大的专用共享内存：
<ul>
<li>每个SMM有64KB的共享内存，4个processing block共享；但是每个线程块只能用48KB</li>
<li>L1缓存专职服务于texture，L2缓存大小激增</li>
</ul>
</li>
<li>快速的共享内存原子操作</li>
<li>支持动态并行：Kepler只在高端GPU中支持，Maxwell在低功率芯片中也支持</li>
</ul>
<h4 id="smm">SMM</h4>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-17:49:42.png" alt="image-20231220174942639" style="zoom:67%;" />
<h3 id="pascal">Pascal</h3>
<p>以<a href="https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper-v1.2.pdf">GP100</a>为例，6个GPC，每个GPC有5个TPC，每个TPC有2个Pascal SM（但是P100有56个SM）
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-21:36:02.png" alt="image-20231220213601888" style="zoom:50%;" /></p>
<h4 id="架构">架构</h4>
<ul>
<li>SMP
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-21:36:31.png" alt="image-20231220213631880" style="zoom:60%;" />
<ul>
<li>每个SM有64个core</li>
<li>每个SM中寄存器数量保持不变，因为SM数量更多，所以总的寄存器数量也变多</li>
<li>每个SM中共享内存从GM200的96KB下降到64KB，但是因为SM数量更多，因此共享内存总量更大</li>
<li>每个SM中有32个双精度FP64 CUDA core</li>
</ul>
</li>
<li>支持FP16</li>
<li>有专用的共享内存（64KB/SM），L2 cache进一步增大</li>
</ul>
<h4 id="unified-memory">Unified Memory</h4>
<h4 id="compute-preemption">Compute preemption</h4>
<ul>
<li>计算抢占：允许在GPU上运行的计算任务在指令级别粒度上被中断</li>
<li>在Pascal架构之前：
<ul>
<li>仅仅在线程块粒度可以被中断</li>
<li>如果GPU上同时运行计算任务和显示任务，则长时间的计算可能会使得显示任务变得不响应和非交互</li>
</ul>
</li>
<li>Pascal中
<ul>
<li>支持计算抢占，因此显示任务会保持流畅运行</li>
<li>同时，计算抢占允许在单个GPU上交互式调试kernel</li>
</ul>
</li>
</ul>
<h4 id="硬件结构">硬件结构</h4>
<ul>
<li>内存从原来的GDDR5更换到HBM</li>
<li>NVLink：可以GPU之间连接，也可以CPU和GPU之间连接
<ul>
<li>通过NVLink连接的GPU，程序可以直接访问另一个GPU的显存</li>
</ul>
</li>
</ul>
<h3 id="volta">Volta</h3>
<p>以V100为例，有6个GPC，每个GPC有7个TPC，每个TPC有2个SM</p>
<h4 id="sm和tensor-core">SM和Tensor Core</h4>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-21:47:39.png" alt="image-20231220214739795" style="zoom:90%;" />
<ul>
<li>
<p>core的变化：允许FP32和INT32操作同时执行</p>
<ul>
<li>原来SM是core（ALU+FPU）+DPU的结构，因此FP32与INT32无法同时运行
<ul>
<li>由于ALU都是流水线化、分阶段的，因此虽然ALU和FPU可以同时运行，但是可能处于不同阶段</li>
</ul>
</li>
<li>现在SM是FP32+FP64+INT的结构，分离了ALU和FPU
<ul>
<li>因此FP32与INT32可以同时运行</li>
<li>而且FP32和INT32可以满吞吐运行
<ul>
<li>对于1个warp共32个线程，交给16个单元去执行的话，要像G80等架构提到的那样占用连续的两个周期来完成issue。不过在第二个周期，dispatch unit可以继续发射指令到其他单元，比如INT32。两者交错起来，就正好能达到满吞吐。</li>
<li>虽然增加了1周期的延迟，但是Volta大多数指令延迟都从6个周期降低到4个周期，总体还是快</li>
</ul>
</li>
</ul>
</li>
<li>意义：很多程序具有执行指针算术（整数内存地址计算）与浮点计算相结合的内部循环，流水线循环的每次迭代都可以更新地址（INT32指针算术）并为下一次迭代加载数据，同时在FP32中处理当前迭代。</li>
</ul>
</li>
<li>
<p>Tensor Core</p>
<ul>
<li>每个tensor core在每个时钟周期内，可以执行64个浮点FMA操作（<code>4*4*4</code>的GEMM）</li>
<li>每个tensor core执行浮点FMA操作：<code>D=A*B+C</code>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-21:58:40.png" alt="image-20231220215840623" style="zoom: 33%;" /></li>
</ul>
</li>
<li>
<p>enhanced L1 data cache and shared memory</p>
<ul>
<li>Instruction Cache
<ul>
<li>原来SM中有一个Instruction Cache，每个processing block中有一个Instruction Buffer</li>
<li>现在SM中有一个L1 Instruction Cache，每个processing block中有一个L0 Instruction Cache</li>
</ul>
</li>
<li>提高了L1 data cache的带宽，降低了其延迟</li>
<li>共享内存
<ul>
<li>将共享内存和L1 data cache整合起来，一共128KB，其中共享内存可以分配到96KB</li>
<li>纹理内存、全局内存都会经过L1 data cache</li>
</ul>
</li>
<li>之前的GPU只有load caching，GV100中引入了write caching</li>
</ul>
</li>
</ul>
<h4 id="independent-thread-scheduling">Independent Thread Scheduling</h4>
<ul>
<li>
<p>之前的SIMT模型</p>
<ul>
<li>一个warp使用一个共享的程序计数器，作用于32个线程，使用一个活动掩码，masked thread就是inactive的thread。各个分支依次执行，最后reconverge（同步）</li>
<li>由于divergence处理成顺序的执行，因此，来自不同区域或不同执行状态的 Warp 中的线程不能相互发送信号或交换数据，同时需要由锁或互斥锁保护的细粒度数据共享的算法很容易导致死锁
<ul>
<li>例子：比如0~3号线程在执行完A之后，需要使用到X的计算结果，此时无法实现
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-22:29:49.png" alt="image-20231220222949079" style="zoom:50%;" /></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Volta的SIMT模型：引入独立线程调度，每个线程都有自己的程序计数器和调用堆栈
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-22:33:17.png" alt="image-20231220223317559" style="zoom:50%;" /></p>
<ul>
<li>Volta的独立线程调度允许GPU放弃任何线程的执行，以便更好地利用执行资源或允许一个线程等待另一个线程生成数据，现在线程可以按照子warp粒度进行分支和重新汇聚，同时Volta中的收敛优化器仍会将执行相同代码的线程组合在一起、并行运行以达到最大效率。
<ul>
<li>可以使用CUDA 9的warp同步函数<code>__syncwarp()</code>来强制warp重新汇聚，因此假设了warp同步的代码不再安全</li>
<li><code>void __syncwarp(unsigned mask = 0xffffffff)</code>
<ul>
<li>二进制位1表示对应的线程参与同步</li>
</ul>
</li>
</ul>
</li>
<li>虽然一个SM中拆分为了4个processing block，每个processing block16个FP32/INT32，而且每个线程都有自己的PC和stack，看起来half-warp在1个周期内可以直接调度和dispatch到一个processing block；但是每次调度仍然是一个warp（32个线程），消耗2个周期（1个周期调度到1个processing block，2个周期将完整的warp调度完毕）。
<ul>
<li>前面的方法会增加调度硬件的复杂性，而且这种运行时的动态信息会改变各个组件的可用情况，也可能会破坏编译器静态调度的预设状态。</li>
</ul>
</li>
<li>例子1：可以实现warp内部细粒度的同步
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-05-15:23:05.png" style="zoom:60%;" /></li>
<li>例子2：分支间交错执行，可以掩盖stall
<ul>
<li>独立线程调度使得假设了warp同步的代码不再安全，比如此时在执行Z的时候，一个warp中的32个线程没有reconverge（同步），而是保持原来的branch执行
<ul>
<li>这是因为调度程序必须保守地假设Z可能会产生其他分叉执行分支所需的数据，如果是这种情况，自动强制重新汇聚将不安全。
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-21-15:17:24.png" alt="image-20231221151724143" style="zoom: 60%;" />
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-22:58:51.png" alt="image-20231220225851393" style="zoom:50%;" /></li>
</ul>
</li>
<li>此时需要使用<code>__syncwarp()</code>强制汇聚，可以提高SIMT效率 <br>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-22:59:19.png" alt="image-20231220225919107" style="zoom:50%;" /></li>
<li>因此，从CUDA 9开始，原来的warp shuffle指令<code>__shfl</code>都变成了deprecated，推荐使用<code>__shfl_sync</code>，里面加入了mask参数</li>
</ul>
</li>
<li>例子3：无饥饿算法，多线程环境下双向链表插入节点
<ul>
<li>Volta的独立线程调度确保即使线程T0当前持有节点A的锁，同一warp中的另一个线程T1也可以成功等待锁变得可用，而不会妨碍线程T0的进展。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">void</span> <span class="nf">insert_after</span><span class="p">(</span><span class="n">Node</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="n">Node</span> <span class="o">*</span><span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Node</span> <span class="o">*</span><span class="n">c</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">lock</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">lock</span><span class="p">(</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">b</span><span class="o">-&gt;</span><span class="n">prev</span> <span class="o">=</span> <span class="n">a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">b</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">c</span><span class="o">-&gt;</span><span class="n">prev</span> <span class="o">=</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">unlock</span><span class="p">(</span><span class="n">c</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">unlock</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>缺点：增加了寄存器负担，单个线程的程序计数器一般要占用两个寄存器</li>
<li>参考
<ul>
<li><a href="https://www.zhihu.com/question/290660113">https://www.zhihu.com/question/290660113</a></li>
<li><a href="https://developer.nvidia.com/blog/inside-volta/">https://developer.nvidia.com/blog/inside-volta/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/186192189">https://zhuanlan.zhihu.com/p/186192189</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="multi-process-servicemps">Multi-Process Service(MPS)</h4>
<ul>
<li>
<p>MPS：实现多个计算应用程序共享GPU时的性能提升和隔离</p>
</li>
<li>
<p>特点</p>
<ul>
<li>保证服务质量：限制每个应用程序只使用GPU资源的一部分，从而降低或消除排队阻塞</li>
<li>独立地址空间：不同应用程序进行地址隔离
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-21-00:11:58.png" alt="image-20231221001158198" style="zoom:50%;" /></li>
</ul>
</li>
<li>
<p>发展</p>
<ul>
<li>
<p>Volta之前都是通过软件方法，使用时间片的方式Time-slice scheduling
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-23:49:35.png" alt="image-20231220234935121" style="zoom:40%;" /></p>
<ul>
<li>从Kepler GK110 GPU开始，NVIDIA引入了基于软件的Multi-Process Service（MPS）和MPS Server，MPS Server允许将多个不同的CPU进程（应用程序上下文）组合成单个应用程序上下文并运行在GPU上，从而实现更高的GPU资源利用率。</li>
<li>对于Pascal，CUDA Multi-Process Service是一个CPU进程，它代表已经请求和其他GPU应用程序同时共享执行资源的GPU应用程序。该进程充当中介，将工作提交到GPU内部的工作队列中以进行并发内核执行。</li>
</ul>
</li>
<li>
<p>Volta MPS：
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-20-23:53:30.png" alt="image-20231220235330434" style="zoom:50%;" /></p>
<ul>
<li>Server CUDA Context管理GPU硬件资源，多个MPS Clients会将它们的任务通过MPS Server传入GPU</li>
<li>Volta MPS对MPS server的关键部分使用硬件加速，使得MPS客户端能够直接将工作提交到GPU内部的工作队列中，同时将MPS客户端的最大数量从Pascal上的16增加到Volta上的48</li>
<li>Volta MPS旨在将GPU共享在单个用户的应用程序之间，并不适用于多用户或多租户用例</li>
<li>如果其中一个运行出错，则可能导致运行的任务都失败，即Volta MPS不提供客户端之间的致命故障隔离。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>应用：</p>
<ul>
<li>No Batching的推理场景中，允许许多单独的单个推理任务同时提交到GPU，提升GPU利用率</li>
<li>支持linux下的统一内存，
<ul>
<li>在GPU执行时，之前的MPS client都是运行在一个单独的地址空间，与访问独立CPU进程内存不兼容</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Hyper-Q与MPS</p>
<ul>
<li>Hyper-Q：多流优化，同一个应用程序下多个stream中，没有依赖的操作可以并行执行</li>
<li>MPS：同时并行运行多个应用程序，多个应用程序共享同一个GPU context</li>
</ul>
</li>
<li>
<p>more reading and reference</p>
<ul>
<li><a href="https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf">MULTI-PROCESS SERVICE</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/346389176"># 教你如何继续压榨GPU的算力</a></li>
<li><a href="https://asphelzhn.github.io/2019/04/14/tensor_09_MPS/">https://asphelzhn.github.io/2019/04/14/tensor_09_MPS/</a></li>
<li><a href="https://blog.csdn.net/cleanarea/article/details/112691820">https://blog.csdn.net/cleanarea/article/details/112691820</a></li>
</ul>
</li>
</ul>
<h4 id="cooperative-groupscg">Cooperative Groups(CG)</h4>
<p>协作组是CUDA 9引入的新特性，允许自定义线程通信的粒度
<a href="https://zhuanlan.zhihu.com/p/673304744"># CUDA 编程模型之协作组（Cooperative Groups）</a>
<a href="https://blog.csdn.net/kunhe0512/article/details/128927355"># CUDA协作组详解</a></p>
<h3 id="turing">Turing</h3>
<p>以<a href="https://images.nvidia.com/aem-dam/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf">TU102</a>为例，有6个GPC，每个GPC有6个TPC，每个TPC有2个SM
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-21-16:35:55.png" alt="image-20231221163555318" style="zoom:75%;" /></p>
<ul>
<li>SM
<ul>
<li>添加了独立的integer datapath，可以与浮点数指令同时运行</li>
<li>Uniform Register：将共享内存、texture cache、memory load cache（L1 data cache？）重新设计，统一到一起</li>
</ul>
</li>
<li>第二代Tensor Core
<ul>
<li>添加了INT8和INT4精度模式，增强了推理性能</li>
<li>支持DLSS（Deep Learning Super Sampling）</li>
</ul>
</li>
<li>实时光线追踪、渲染管线、RT Core、DLSS等图像相关</li>
</ul>
<h3 id="ampere">Ampere</h3>
<p>以<a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">GA100</a>为例，有8 GPCs, 8 TPCs/GPC, 2 SMs/TPC</p>
<h4 id="sm-1">SM</h4>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-21-17:25:04.png" alt="image-20231221172504272" style="zoom:80%;" />
<ul>
<li>第三代Tensor Core
<ul>
<li>Tensor Core Sparsity利用2:4的细粒度结构化稀疏性，使得吞吐量翻倍
<ul>
<li>稀疏矩阵定义：2:4稀疏矩阵，即每个四元组中有两个非零值
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-21-20:41:04.png" alt="image-20231221204104503" style="zoom:67%;" /></li>
<li>过程：使用稠密权重进行训练，然后进行细粒度结构化剪枝，最后通过额外的训练步骤对剩余的非零权重进行微调。</li>
<li>具体而言，A100使用Sparse MMA(Matrix Multiply-Accumulate)指令，跳过对带零值的输入进行计算，从而使 Tensor Core 的计算吞吐量翻倍
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-21-21:01:09.png" alt="image-20231221210109424" style="zoom:50%;" /></li>
</ul>
</li>
<li>支持所有数据类型：FP16、BF16、TF32、FP64、INT8、INT4 和 INT1，且比V100有进一步的加速效果
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-21-20:37:22.png" alt="image-20231221203721459" style="zoom:50%;" />
<ul>
<li>如果不使用Tensor Core，默认使用FP32；如果使用Tensor Core，则默认使用TF32</li>
<li>支持FP16/FP32、BF16/FP32混合精度，且两种混合精度速度一样快</li>
<li>TF32一方面保持了FP16的精度，另一方面保持了FP32的范围，因此很适合训练</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="memory方面的改进">memory方面的改进</h4>
<ul>
<li>
<p>Data sharing improvements：
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-00:00:12.png" alt="image-20231222000012376" style="zoom: 50%;" /></p>
<ul>
<li>数据可以在一个warp中32个线程共享，原来Volta只能在8个线程之间共享
<ul>
<li>因此节省了寄存器和带宽</li>
<li>同时，A100 Tensor Core将矩阵乘法指令的k维变为原来的4倍</li>
</ul>
</li>
<li>表格中的数据怎么来的？<a href="https://zhuanlan.zhihu.com/p/614429902">Nvidia tensorCore 计算过程</a></li>
</ul>
</li>
<li>
<p>Data Fetch improvement
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-10:22:10.png" alt="image-20231222102210009" style="zoom: 50%;" /></p>
<ul>
<li>新的异步拷贝指令可以直接将数据从全局内存（通常是DRAM和L2缓存）中加载到共享内存中，绕过L1缓存和寄存器</li>
<li>原来Volta中，数据先经过L1缓存读取到寄存器，然后再写到共享内存中</li>
<li>异步拷贝指令与异步barrier搭配使用：异步拷贝完成后，通过异步barrier通知程序拷贝完成</li>
<li>Compute Data Compression
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-11:24:01.png" alt="image-20231222112401558" style="zoom:33%;" />
<ul>
<li>Combined L1 cache and shared memory
<ul>
<li>L1 data cache和共享内存整合到一起，一共192KB</li>
<li>FP32和INT32可以同时运行、且满吞吐运行（与Volta与Turing架构相同）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>L2 cache improvement</p>
<ul>
<li>设计改进
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-11:28:03.png" alt="image-20231222112803822" style="zoom:67%;" /></li>
<li>Residency Control：ping-pong buffer（或称double buffer）
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-11:16:24.png" alt="image-20231222111624574" style="zoom:50%;" />
<ul>
<li>ping-pong buffer常驻于L2缓存上，减少对内存的写回，保持L2中数据重用</li>
<li>比如推理场景中，权重分段轮流装载到L2缓存上，让计算与权重装载并行。此时，多batch可以共用更多的权重</li>
</ul>
</li>
</ul>
</li>
<li>
<p>总结
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-11:31:53.png" alt="image-20231222113153880" style="zoom: 50%;" /></p>
</li>
</ul>
<h4 id="multi-instance-gpumig">Multi-Instance GPU(MIG)</h4>
<ul>
<li>
<p>背景：Volta MPS虽然支持多个应用程序同时运行，但是可能一个应用程序占用太多内存带宽或是L2缓存，对其他应用程序造成影响</p>
</li>
<li>
<p>MIG
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-16:01:25.png" alt="image-20231222160125125" style="zoom:50%;" /></p>
<ul>
<li>MIG可以将每个A100 划分为最多7个GPU Instance，每个instance可以为client（虚拟机、容器、进程等）提供定义的服务质量和故障隔离</li>
<li>每个instance由若干个GPU slices组成，GPU slices的结构
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-16:16:03.png" alt="image-20231222161603049" style="zoom:50%;" />
<ul>
<li>Sys Pipe：GigaThread Engine的一部分</li>
<li>一个GPC（7个TPC，14个SM）</li>
<li>一个L2 slice group（包括10个L2 cache slices）</li>
<li>对一部分frame buffer memory的访问</li>
</ul>
</li>
<li>每个instance内部可以再细分为compute instance
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-16:27:19.png" alt="image-20231222162719640" style="zoom:67%;" />
<ul>
<li>compute instance可以自行配置和封装计算资源，默认每个instance创建一个compute instance，因此该compute instance使用该instance的全部资源</li>
<li>每个compute instance包括一个Sys Pipe和若干个GPC，所有共享一个compute instance的应用程序共享一个Sys Pipe，每个compute instance都可以单独进行上下文切换</li>
<li>每个compute instance都支持MPS，MPS client的最大数量与compute instance大小成正比</li>
</ul>
</li>
</ul>
</li>
<li>
<p>应用场景：</p>
<ul>
<li>Multi-Tenant</li>
<li>Single Tenant, Single User：一个用户运行多个GPU应用程序</li>
<li>Single Tenant, Multi-User：比如对外部提供AI服务</li>
</ul>
</li>
</ul>
<h4 id="cuda-advances">CUDA Advances</h4>
<ul>
<li>
<p>Task Graph Acceleration</p>
<ul>
<li>背景：对于深度学习等应用场景，有iterative structure（即same workflow is executed repeatedly）
<ul>
<li>以前只能在每个iteration中，CPU重新提交任务到GPU。尤其是很多小的kernel在整个运行过程中，launch、init等开销占了相当一部分时间。</li>
<li>现在定义一个task graph（若干个操作、相应依赖关系和一些内存操作），可以define-once/run-repeatedly，即先将多个kernel预先构建为一个task graph，然后CPU一次性launch，减少了launch、init的时间
<ul>
<li>kernel的执行流程可以分为三个步骤：launch，grid initialization，kernel execution
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-26-16:09:41.png" alt="image-20231226160941218" style="zoom:50%;" /></li>
</ul>
</li>
</ul>
</li>
<li>加速原理：
<ul>
<li>launch optimization：submit multiple work items to the GPU in a single operation</li>
<li>execution dependency optimization：可以优化复杂的graph（比如workflow fork and re-join，在一个fork分支中可以有多个dependency）
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-26-17:31:09.png" alt="image-20231226173109417" style="zoom:50%;" /></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="https://blog.csdn.net/kunhe0512/article/details/125509926">异步数据拷贝</a>和异步barrier：<code>memcpy_asnyc</code></p>
<ul>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-data-copies">异步数据拷贝</a>：
<ul>
<li><code>memcpy_asnyc</code>：从global memory到shared memory的异步数据拷贝</li>
<li><code>cudaMemcpyAsync</code>：从CPU memory到GPU global memory的异步数据拷贝</li>
</ul>
</li>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-barrier">异步barrier</a>：<code>arrival</code>和<code>wait</code>是分开的
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-19:33:51.png" alt="image-20231222193351615" style="zoom:40%;" />
<ul>
<li><code>arrival</code>：最快线程到达barrier</li>
<li><code>wait</code>：等待其他线程（或者最慢的线程）到达barrier</li>
<li>普通的barrier由于各线程快慢不一，中间有idle；异步barrier中间原来idle的部分现在进行其他independent work</li>
</ul>
</li>
<li><a href="https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/">Controlling Data Movement to Boost Performance on the NVIDIA Ampere Architecture</a>：两阶段的pipeline，将计算与拷贝重叠
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-21:21:41.png" alt="image-20231222212140951" style="zoom:50%;" /></li>
</ul>
</li>
<li>
<p>L2 cache residency control</p>
<ul>
<li>两种数据：
<ul>
<li>persisting data：数据重复使用，比如深度学习场景，或生产者-消费者场景</li>
<li>streaming data：数据只使用一次
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2023-12-22-21:19:04.png" alt="image-20231222211904317" style="zoom:33%;" /></li>
</ul>
</li>
<li>L2 cache中专门留出一部分给persisting data使用，persistent access优先访问这部分，具体见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-l2-access-management">Device Memory L2 Access Management</a></li>
</ul>
</li>
<li>
<p>参考</p>
<ul>
<li><a href="https://blog.csdn.net/han2529386161/article/details/106411138"># NVIDIA GPU A100 Ampere(安培) 架构深度解析</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/467466998"># CUDA效率优化之：CUDA Graph</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/145477249"># NV Ampere GPU架构学习与思考</a></li>
</ul>
</li>
</ul>
<h3 id="ada-lovelace">Ada Lovelace</h3>
<ul>
<li>cuda core数量增加</li>
<li>第四代Tensor Core
<ul>
<li>Hopper FP8 Transformer Engine</li>
</ul>
</li>
</ul>
<h3 id="hopper">Hopper</h3>
<p><a href="https://resources.nvidia.com/en-us-tensor-core">white paper</a></p>
<h3 id="参考">参考</h3>
<ul>
<li><a href="https://mp.weixin.qq.com/s/qakvAfNV4KkmNa3P56i-dQ">https://mp.weixin.qq.com/s/qakvAfNV4KkmNa3P56i-dQ</a></li>
<li><a href="https://www.zhihu.com/people/zhang-huan-11-88/posts">江南泣相关（翻译）博客</a>和对应<a href="https://developer.nvidia.com/key-technologies">white paper</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/258196004">NVIDIA GPU的一些解析（一）</a>的相关解读</li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
