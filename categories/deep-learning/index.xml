<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>deep learning on Paul&#39;s Blog</title>
    <link>https://qinganzhang.github.io/categories/deep-learning/</link>
    <description>Recent content in deep learning on Paul&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 07 Sep 2024 14:45:54 +0800</lastBuildDate><atom:link href="https://qinganzhang.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Encoder Decoder和decoder Only架构训练和推理浅析</title>
      <link>https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/</link>
      <pubDate>Sat, 07 Sep 2024 14:45:54 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/</guid>
      <description>之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐to</description>
      <content:encoded><![CDATA[<p>之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐token生成。欸，那之前传统机器翻译不也是这样吗，输出一个中文句子，然后再自回归地逐token生成英文句子，有点混乱了。下面我们来重新梳理一下，LLM的发展过程、典型模型架构和训练过程。</p>
<h1 id="开山之作attention-is-all-you-need">开山之作：Attention is all you need</h1>
<h2 id="模型架构和训练过程">模型架构和训练过程</h2>
<p>模型架构自不必多说，就像下面最左边和最右边经典的模型架构图所示，是典型的encoder-decoder架构。Transformer一开始，解决的是一个seq2seq的任务（下面以中译英机器翻译任务为例），模型结构中包含了encoder和decoder（将此时的decoder称为naive-decoder），下面图中是encoder-decoder的Transformer的训练过程，左边是encoder部分，右边是decoder部分。在训练阶段，将中文句子经过encoder生成编码矩阵C，将shifted right的ground truth（开头加了BOS的英文句子）作为decoder的输入，整体模型的监督信号是对应的英文句子。因此，训练是并行的，一个iteration就可以将一个中英pair训练完（假设batch size=1），训练任务就是next token predicate。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:01:57.png" alt="encoder-decoder架构的训练过程（以机器翻译为例）" style="zoom:120%;" />
<h2 id="推理过程">推理过程</h2>
<p>训练完成后，实际使用中开始进行推理。首先encoder部分要输入一个中文句子，还是经过encoder生成编码矩阵C。然后是decoder部分，一开始输入一个BOS，通过自回归的方式逐token生成，渐变的颜色就表示token生成的不同次序。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:02:08.png" alt="encoder-decoder架构的推理过程（以机器翻译为例）" style="zoom:80%;" />
<p>这里有一个问题需要明确一下，每次decoder部分的输入，是一个token呢，还是前面的所有token呢？比如要预测“a”这个token，那么此时decoder的输入是“have”这个token呢，还是BOS+“I”+“have”这三个token都输入呢？其实都可以，</p>
<ul>
<li>
<p>比如每次decoder部分的输入是前面所有的token（比如BOS+“I”+“have”这三个token都输入），那么Masked MHA、MHA部分的Q、K、V都有多行（比如现在是三行），最终decoder出来的这个矩阵，我们只需要最后一个向量（下面红框的这个向量），进行linear+softmax+采样，得到next token</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:39:09.png" alt="image-20240905153902066" style="zoom:50%;" />
</li>
<li>
<p>也可以每次decoder部分的输入是一个token（比如预测“a”这个token，只输入“have”这个token），那么此时就需要将之前token的K、V向量缓存下来（即KV-Cache），一个Q向量和concat(当前的KV向量，之前缓存下来的KV向量)算注意力，Masked MHA、MHA部分的attn-score只有一行，然后最终decoder出来的矩阵也只是一个向量，直接进行linear+softmax+采样，得到next token</p>
</li>
</ul>
<p>采用kv-cache的方式，可以在推理过程中减少很多不必要的运算，而且从显存占用来看，kv-cache也是划算的。如果不使用kv-cache，那么每次都要输入之前生成的所有token，Q、K、V都是完整的矩阵；如果使用kv-cache，K、V显存占用与之前相同，Q只是一个行向量而原来只是一个多行的矩阵，使用kv-cache的显存占用相对于不使用kv-cache的峰值显存占用是更少的。虽然说“相对峰值显存更少”，但是需要留意的是，kv-cache还是很占显存的，尤其是大batch_size和长序列的情况下，后面产生了MQA、GQA等优化，这是后话了。</p>
<p>这样看来，似乎在很早以前就有kv-cache的概念，但是似乎在LLM中才真正被普遍应用起来，我想有这么几个原因（not for sure, hope your discussion）：</p>
<ol>
<li>之前可能更关注模型架构的改进，之前encoder-decoder架构、encoder-only架构、decoder-only架构没有一个特别突出的模型，直到GPT-3（decoder-only）的出现，掀起了LLM的时代，现在的LLM基本上都是decoder-only的架构</li>
<li>在推理场景中，之前的模型上下文长度（或者叫最长序列长度）比较小，计算强度没那么大（但是比如语音流式识别等场景中也会用到kv-cache，不是很确定），LLM时代的transformer上下文长度普遍较大，而且往往是chat场景，对延迟有要求，因此要想法设法减少计算</li>
</ol>
<h1 id="encoder-only的代表bert">encoder-only的代表：Bert</h1>
<p>Bert是典型的encoder-only架构，其训练和推理过程很相似，训练怎么训，往往推理就是直接一个前向的过程。现在LLM基本都是生成式任务，encoder-only的架构总是感觉很别扭，decoder-only架构就很自然。可以关注知乎问题：<a href="https://www.zhihu.com/question/588325646">为什么现在的LLM都是Decoder only的架构？</a></p>
<h1 id="llm时代">LLM时代</h1>
<h2 id="模型架构和训练过程-1">模型架构和训练过程</h2>
<p>在模型架构方面，GPT-3和Llama只是Decoder-only架构的两个典型代表，基本保持了Vanilla-Transformer-decoder的结构，但是中间很多地方做了改动，比如去掉了与encoder的cross-attn，在masked-MHA的输入QK前面在加上旋转位置编码RoPE，将LayerNomr调整为post-norm结构，MLP部分可能会进行一些调整等，如果仅仅是走一遍训练和推理的流程，那么不会有影响。</p>
<p>从LLM训练过程来看，预训练阶段与之前的语言模型基本一致（如下图，这里不讨论微调、RLHF等过程，更侧重工程和流程方面），都是next token predicate任务，没有了encoder部分，模型结构看起来更加简洁。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-06-23:26:59.png" alt="decoder-only架构的训练过程（以语言模型为例）"  /></p>
<h2 id="推理过程-1">推理过程</h2>
<p>Decoder-only架构的推理过程和训练过程基本保持了相同的形式，推理过程就是先输入prompt，然后自回归的逐token生成。这里提一下prompt，LLM可以认为是一种特殊的语言模型，语言模型通俗来说就是续写，但是LLM包含了in-context learning的能力，prompt可以认为是一个指令，一个问题，使得“续写”的内容能够反映prompt的意图。</p>
<p>这里我们可以尝试分析一下文章开头提出的疑惑：</p>
<ul>
<li>在encoder-decoder架构的机器翻译任务中，
<ul>
<li>训练过程：中文句子输入到encoder，decoder部分的训练就是语言模型</li>
<li>推理过程：中文句子输入到encoder，BOS输入到decoder，然后自回归的逐token进行生成</li>
</ul>
</li>
<li>在decoder-only架构的LLM中，
<ul>
<li>预训练过程：没有encoder，就是训练语言模型</li>
<li>推理过程：prompt输入到decoder（prefill阶段），然后自回归的逐token进行生成（decode阶段）</li>
</ul>
</li>
</ul>
<p>LLM推理过程分为prefill阶段和decode阶段，不仅仅是从推理过程上看起来可以分成两个阶段，更重要的是，这两个阶段的特点不同，性质不同，为了尽可能推理加速，才有必要分成两个阶段。</p>
<ul>
<li>prefill阶段：输入prompt，生成第一个token。由于prompt往往较长，或者实际使用中将多个prompt打成一个batch（小batch），以提高模型吞吐，所以这个阶段计算量较大。衡量该阶段的一个指标是首字延迟（TTFT，Time To First Token）。还有一个需要注意的是，为了减小decode阶段的计算量，prefill阶段在计算prompt的注意力机制的时候，会将K、V矩阵缓存下来，空间换时间，即kv-cache。</li>
<li>decode阶段：后续自回归的逐token进行生成的过程，直到生成一个终止符（或者达到长度限制）。该阶段中，每次自回归过程中，输入一个token，然后生成q、k、v向量，k、v向量更新到kv-cache中，然后q向量和矩阵的计算（gemv），计算量较小，访存逐渐称为bottleneck。为了提高计算强度，往往会将多个请求decoder阶段的计算组成一个大batch。衡量该阶段的一个指标是TPOT（Time Per Output Token，生成每个token的耗时）。</li>
</ul>
<p>上面TTFT和TPOT指标是针对streaming generate场景，如果是non-streaming generate场景，则还是使用经典的延迟（Latency，生成一个完整输出的耗时）、吞吐（Throughput，每秒可以生成几个完整的输出）作为指标。</p>
<p>reference and more reading：</p>
<p><a href="https://zhuanlan.zhihu.com/p/685706549">一些已成为LLM 推理引擎中事实标准的方法</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/696850285">大模型高效推理 I 推理技术框架总结</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/683359705">LLM推理到底需要什么样的芯片？（1）</a>  <a href="https://zhuanlan.zhihu.com/p/683908169">LLM推理到底需要什么样的芯片？（2)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/704408423">大模型推理原理&amp;流程详解</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>LLM时代的transformer参数量、计算量、激活值的分析</title>
      <link>https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/</link>
      <pubDate>Sat, 07 Sep 2024 14:43:19 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/</guid>
      <description>导读：本文可以看作是对分析transformer模型的参数量、计算量、中间激活、KV cache的详细说明 定性分析 GPU上都存了哪些东西 首先我</description>
      <content:encoded><![CDATA[<p>导读：本文可以看作是对<a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a>的详细说明</p>
<h1 id="定性分析">定性分析</h1>
<h2 id="gpu上都存了哪些东西">GPU上都存了哪些东西</h2>
<p>首先我们来从全局整体的角度看一看，在训练阶段GPU显存上都有哪些内容：</p>
<ul>
<li>Model States：模型训练过程中必须存储的states
<ul>
<li>params（下面有时也叫做weights）：模型参数，记参数量为$\Phi$</li>
<li>grads：模型梯度，梯度数量同参数量$\Phi$</li>
<li>optimizer states：Adam优化器中的momentum和variance，数量分别是$\Phi$，共$2\Phi$</li>
</ul>
</li>
<li>Residual States：模型训练过程中，中间临时的、动态产生的states
<ul>
<li>activation：中间激活值，这个部分可能在训练过程中占据很大一部分显存，下面会详细分析。但是激活值不是必须存储的，可以使用重计算（recompute，也叫做activation checkpoint），在反向算梯度的时候，再重新算一遍，当然计算增加了，时间换空间，实际使用中可以部分选择性的进行重计算。</li>
<li>temporary buffers：临时存储，比如cuda、nccl等临时申请的显存。</li>
<li>unusable fragment memory：内存碎片导致的内存浪费</li>
</ul>
</li>
</ul>
<p>推理阶段就相对简单一些，最主要的是Model States中的params和Residual States中的activation。</p>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/618865052">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></p>
<h2 id="混合精度训练">混合精度训练</h2>
<p>上面只是列出了训练过程中，显存中存放的内容和保存的数值数量，但是实际训练过程中，为了节省显存，以及考虑到训练过程中间某些过程对精度不是特别敏感，所以中间有些部分会使用fp32，有些部分会使用fp16/bf16。下面以Megatron为例，简单分析混合精度训练的一个大致流程。</p>
<p>首先我们来看一下不使用混合精度训练的场景，数值精度全使用fp32，作为一个分析的baseline。具体过程是：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:18:21.png" alt="fp32精度训练" style="zoom: 40%;" />
<p>占用显存为：$4\Phi$（fp32 weights）+$4\Phi$（fp32 momentum）+$4\Phi$（fp32 variance）+$4\Phi$（fp32 grad）+fp32 activation（可能很大）=$16\Phi$ Bytes + fp32 activation（4代表fp32的4Bytes，2代表fp16/bf16的2Bytes）</p>
<p>如果使用fp16的混合精度训练（bf16应该也可以，但是实际Megatron有点不同，下面会提到），具体过程是：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:15.png" alt="fp16混合精度训练" style="zoom:50%;" />
<p>占用显存为：$4\Phi$（fp32 weights）+$4\Phi$（fp32 momentum）+$4\Phi$（fp32 variance）+$2\Phi$（fp16 grad）+$2\Phi$（fp16 scaled grad）+$4\Phi$（fp32 unscaled and cliped grad）+fp16 activation（可能很大）=$20\Phi$ Bytes + fp16 activation</p>
<p>需要说明的有两点：</p>
<ol>
<li>当fp16 scaled grad转为为fp32 unscaled and cliped grad后，fp16 scaled grad就没用了，但是此时Megatron中仍然保留着一份fp16 scaled grad，所以显存占用中这两部分都会计算在内，这也符合Megatron offical readme中的描述：</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:22.png" alt="image-20240907213340085" style="zoom:80%;" />
<ol start="2">
<li>
<p>注意到上面流程中多了一个scale/unscale的操作，这叫做“loss scaling”</p>
<p>​	在使用混合精度训练时，如果直接使用fp16的grad来更新fp16的梯度，一是会产生舍入误差（比如梯度很小，权重更新后，由于精度不够，累加上的lr * grad被舍入，权重没变，一句话来说就是<strong>大数吃小数</strong>），二是会产生梯度下溢（比如梯度过小，fp16范围不够，导致很小的梯度下溢成为0，而这样的小梯度占比很大，一句话来说就是<strong>下溢成0</strong>）。对于舍入误差，可以在更新权重时，将fp16的梯度转换为fp32，再更新fp32的权重，从而避免精度问题。对于梯度下溢，需要使用loss scale。</p>
<p>​	loss scale就是FWD计算出loss后，对loss放大若干倍，由于求导的链式法则，放大的若干倍同样会传导到fp16梯度，这样fp16梯度就不会产生梯度下溢。在更新权重时，将fp16的梯度转换为fp32，同时进行unscale。</p>
</li>
</ol>
<p>刚才说到bf16有一点点特殊，我们看相应的代码：（Megatron中的arguments.py）</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:49:42.png" alt="image-20240907214939077" style="zoom: 80%;" />
<p>注意到如果使用bf16，那么会强行设置accumulate_allreduce_grads_in_fp32=True，这与上面Megatron offical readme截图（Distributed Optimizer）表格中的第二行【bf16 param, fp32 grads】相对应。具体过程应该是（not for sure, hope for discuss）：</p>
<blockquote>
<p>accumulate_allreduce_grads_in_fp32：If true, do the gradient accumulation and communication in fp32. <a href="https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/distributed.html">from here</a></p>
<p>gradient accumulation：在若干次iteration中，每次都会反向得到一份梯度，将这若干次iteration得到的梯度进行累加、求平均，在最后一次iteration才更新权重。gradient accumulation与data parallel是等价的，gradient accumulation在时间维度上训练多个mini-batch，而data parallel在相同时间内将不同mini-batch放在不同的机器上训练，结果都是一样的。</p>
<p>参考：</p>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/595716023">聊聊梯度累加(Gradient Accumulation)</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/650710443">梯度累积算法</a></p>
</li>
<li>
<p><a href="https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation">Hugging Face:Performing gradient accumulation with 🤗 Accelerate </a></p>
</li>
</ul>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:37.png" alt="bf16混合精度训练" style="zoom:50%;" />
<p>这里找到一个为什么要将bf16与accumulate_allreduce_grads_in_fp32绑定的<a href="https://github.com/NVIDIA/Megatron-LM/issues/372">issue</a>，里面提到“We found this to lead to more stable training before, but you could also try to perform the all-reduce in <code>bf16</code> (it might hurt convergence but will be faster).”</p>
<p>参考：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/618865052">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/662700424">图解大模型训练系列之：Megatron源码解读3，分布式混合精度训练</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training">NVIDIA Docs Hub：Train With Mixed Precision</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/441591808">全网最全-混合精度训练原理</a></li>
</ul>
<h1 id="量化分析">量化分析</h1>
<h2 id="transformer结构详解">transformer结构详解</h2>
<p>LLM中的transformer一般是decoder-only结构，所以下面的transformer block主要是decoder，但是与Vanilla Transformer中的decoder不同的是，这里没有了cross-attn，因此结构看起来反而有点像encoder（但不是，因为有casual mask）。</p>
<p>下面图中的Transformer，没有上kv-cache、GQA等优化，这部分后面会分析。其中，参数量$\Phi$表示有多少个参数；中间激活值$A$的单位是Bytes，主要参考的是<a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a></p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-15:58:31.png" alt="transformer详细分析" style="zoom:150%;" />
<p>在<a href="https://arxiv.org/pdf/2205.05198">Reducing Activation Recomputation in Large Transformer Models</a> 4.1节中也对transformer激活值进行了一个分析，但是该论文中，self-attention block部分softmax之前没有加mask，上图中添加了mask，具体在Attention部分stage SA_3，其中mask由于是整个transformer共享的，所以就省略了，$QK^T$的乘积被mask原地修改，所以$wbas^2$也省略了，这样激活值与原论文中仍然是一样的。</p>
<h2 id="kv-cache对参数量计算量激活值的影响">KV cache对参数量、计算量、激活值的影响</h2>
<p>关于KV Cache的来龙去脉，<a href="https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/">Encoder Decoder和decoder Only架构训练和推理浅析</a>中简单捋了一下。简单来说，kv cache在推理过程中使用，而且模型只能是decoder-only架构。由于自回归的方式逐token生成，self-attention部分必须使用casual mask，因此Q矩阵部分只需要计算最新token的q向量即可，K、V矩阵部分只需要拼接新token的k、v向量即可：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-14:04:56.png" alt="kv_cache" style="zoom: 50%;" />
<p>上面又重新回顾了一下kv cache。首先kv cache不会对参数量有影响，kv cache主要是用来减少不必要的计算的，显存因此也可能有相应的减少，上面只是一个示意图，中间省略了一些部分，详细的量化分析见下图，需要说明的有两点：</p>
<ol>
<li>kv cache使用场景是推理场景，LLM推理分为prefill阶段和decode阶段，prefill阶段创建kv-cache，decode阶段更新kv-cache。在输入prompt的这个prefill阶段中，with kv-cache和without kv-cache的计算量是相同的（显存占用由于分配kv-cache，可能with kv-cache会更多一点）。计算量的减少主要体现在decode阶段，因此下面的分析主要是针对单次decode阶段的，因此固定$s==1$</li>
<li>下图中说的“相对于原来“指的是without kv-cache时，每次都输入之前所有的token，计算完整的attention-score方阵，因而此时的序列长度$s=s_n \le s_m$。在最终分析时，取最大值$s=s_m$进行比较，对应decode阶段的最后一个token的生成过程，有的博客可能会将输入序列长度（prompt长度）和输出序列长度分开，这里合起来了，注意区别。</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-16:10:01.png" alt="transformer详细分析（kv cache）" style="zoom:150%;" />
<table>
<thead>
<tr>
<th></th>
<th>原来（without kv-cache）</th>
<th>现在（with kv-cache）</th>
<th>变化</th>
</tr>
</thead>
<tbody>
<tr>
<td>参数量</td>
<td>$2Vh+(12h^2+13h)l$</td>
<td>$2Vh+(12h^2+13h)l$</td>
<td>不变</td>
</tr>
<tr>
<td>中间激活</td>
<td>$2bsh+(34bs_mh+5bas_m^2)l$</td>
<td>$2bsh+(30bh+4bs_mh+5bas_m)l$</td>
<td>减少了$(30bh(s_m-1)+5bas_m(s_m-1))l$，原来中间激活是最长序列长度$s_m$的二次方，现在随着$s_m$线性增长</td>
</tr>
<tr>
<td>计算量</td>
<td>$(24h+4s_m)bs_mhl+2bs_mhV$</td>
<td>$(24h+4s_m)bhl+2bhV$</td>
<td>减少了$(24h+4s_m)bhl(s_m-1)+2bhV(s_m-1)$，原来计算量是最长序列长度$s_m$的二次方，现在随着$s_m$线性增长</td>
</tr>
</tbody>
</table>
<p>code:  from <a href="https://zhuanlan.zhihu.com/p/667763542">【手撕LLM-KVCache】显存刺客的前世今生&ndash;文末含代码</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># author: xiaodongguaAIGC</span>
</span></span><span class="line"><span class="cl"><span class="c1"># KV-Cache + Generation + decoder </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">LlamaModel</span><span class="p">,</span> <span class="n">LlamaConfig</span><span class="p">,</span> <span class="n">LlamaForCausalLM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">D</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># single-head-dim</span>
</span></span><span class="line"><span class="cl"><span class="n">V</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># vocab_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xiaodonggua_kv_cache</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">V</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">V</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>     
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Wk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>     
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Wv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">V</span><span class="p">)</span> <span class="c1"># LM_head</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># initial</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Q</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="bp">self</span><span class="o">.</span><span class="n">Wk</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="bp">self</span><span class="o">.</span><span class="n">Wv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input_Q:&#34;</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input_K:&#34;</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input_V:&#34;</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Easy KV_Cache</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span> <span class="c1"># first time</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">=</span> <span class="n">K</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span> <span class="o">=</span> <span class="n">V</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span><span class="p">,</span> <span class="n">V</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span>
</span></span><span class="line"><span class="cl">            <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;cache_K:&#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;cache_V:&#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># ignore proj/MLP/scaled/mask/multi-head when calculate Attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span><span class="n">Q</span><span class="nd">@K.transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="nd">@V</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># output</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">xiaodonggua_kv_cache</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="c1"># 创建数据、不使用tokenizer</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Generation </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> step input_shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">：&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">next_token</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>reference and more reading：</p>
<p><a href="https://blog.csdn.net/weixin_65514978/article/details/141399339">【大模型理论篇】Transformer KV Cache原理深入浅出</a></p>
<p><a href="https://juejin.cn/post/7362789570217885759#heading-3">大模型推理优化技术-KV Cache</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/686183300">一文读懂KVCache</a></p>
<h2 id="mqa和gqa对显存占用的影响">MQA和GQA对显存占用的影响</h2>
<p>在实际推理场景中，kv-cache已经是默认的选项。但是kv-cache是很占显存的，占用显存为$2 w_{kv} b s_m (a h_a) l$（其中$h=a * h_a$），后面会有case study分析。针对kv cache的各种优化层出不穷，下面的参考中有几篇博客总结了一下对kv cache的各种优化，简单来说，从上面的显存分析入手，有以下几种优化方法：</p>
<ul>
<li>针对attention 窗口（或者叫做context，上下文，或者当作最长序列长度$s_m$）$s_m$的优化，比如window attention，sparse attention，StreamingLLM</li>
<li>针对注意力头$a$的优化，比如MQA，GQA共享kv-cache（sharing）</li>
<li>针对层数$l$的优化，比如YOCO层间共享kv-cache（sharing）</li>
<li>针对精度$w_{kv}$的优化，比如kv-cache采用int8量化</li>
<li>针对内存分配的优化，减少内存碎片等，比如PagedAttention</li>
<li>其他优化。。。</li>
</ul>
<p>其中MQA/GQA在LLM中广泛使用，比如Llama2中就使用到了GQA。下面简单分析一下。</p>
<p>GQA方法很简单，原来MHA中每个q向量对应一个k向量和v向量，进行attention计算；现在好几个q向量对应（或者说共享）一个k向量和v向量，这“好几个q向量”构成一组，一共有g组，每组就有$\frac{a}{g}$个q向量。如果g=1，那么就是MQA，a个q向量构成一组，共享一个k、v向量；如果g=a，那么就是MHA，每个q向量构成一组，对应一个k、v向量。实际场景中，往往g=8，比如推理场景中单卡放不下，正好单机八卡，每张卡对应一组q向量。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-16:40:24.png" alt="image-20240908164016647" style="zoom: 67%;" />
<p>虽然MQA/GQA是针对推理过程中kv-cache的优化，但是在训练中也能用，也能省显存。下面对GQA在推理场景中的使用（with kv_cache）进行一个量化分析。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-17:24:56.png" alt="image-20240908172449500" style="zoom:150%;" />
<p>因为GQA只影响self-attention计算部分，因此其他部分省略，下面的表格也是只分析这个变化的部分。可以看出，由于kv-cache在长序列的情况下会占用很多显存，GQA针对中间激活的优化与序列长度相关，实际上GQA对中间激活的优化就是将kv-cache变为原来的$\frac{g}{a}$倍。</p>
<table>
<thead>
<tr>
<th></th>
<th>原来（MHA）-现在（GQA）</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>参数量</td>
<td>$\left [3(h^2+h) \right ]l - \left [ (\frac{2g}{a}+1)(h^2+h) \right ]l=2(1-\frac{g}{a})(h^2+h)l$</td>
<td></td>
</tr>
<tr>
<td>中间激活</td>
<td>$\left [ wbsh+2w_{kv}bs_mh \right]l - \left [ wbsh + 2w_{kv}bs_mh \times\frac{g}{a} \right ]l = 2w_{kv}bs_mhl(1-\frac{g}{a})$</td>
<td>尤其当长序列（$bs_m$较大），大模型（$hl$较大）时，前面系数较大，整体激活减少比较可观</td>
</tr>
<tr>
<td>计算量</td>
<td>$\left [ 6bsh^2 \right ]l - \left [ 2bsh^2 (\frac{2g}{a}+1) \right ] l = 4bsh^2l(1-\frac{g}{a}) \overset{s=1}{=} 4bh^2l(1-\frac{g}{a}) $</td>
<td></td>
</tr>
</tbody>
</table>
<p>在训练场景中，同样给出量化分析。需要说明的是，上述分析是在推理场景+kv_cache+GQA的情况下进行的分析，下面公式是针对的是训练场景+GQA。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-17:47:01.png" alt="transformer训练场景分析（GQA）"  /></p>
<p>code： from <a href="https://zhuanlan.zhihu.com/p/717838262">MHA，MQA，GQA注意力</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GroupedQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span> <span class="o">=</span> <span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># attention weights</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># n == num_heads or num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, n, head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">num_groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">//</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">merge_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param x: (batch_size, num_heads, seq_len, head_dim)
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># (batch_size, seq_len, num_heads, head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># ( batch_size, seq_len, embed_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">causal_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 分割注意力头</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意力计算</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># causal mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">attn_weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">causal_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq_len</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">mask_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 归一化</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 合并注意力头</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merge_heads</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">attn_output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>参考：</p>
<p><a href="https://zhuanlan.zhihu.com/p/685853516">大模型百倍推理加速之KV cache篇</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/659770503">LLM（二十）：漫谈 KV Cache 优化方法，深度理解 StreamingLLM</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/697311739">[KV Cache优化]🔥MQA/GQA/YOCO/CLA/MLKV笔记: 层内和层间KV Cache共享</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/708120479">大模型推理加速：KV Cache 和 GQA</a></p>
<h2 id="case-study">case study</h2>
<p>我们以GPT和Llama为例，进行case study。</p>
<h3 id="关于参数量的分析">关于参数量的分析</h3>
<h4 id="gpt-3">GPT-3</h4>
<p>GPT-3模型结构就大致上面【transformer结构详解】中的结构，但是多了一个可学习的position embedding，包含$n_{ctx} * h$个参数，其中$n_{ctx}=2048$，rectified这一列是加上这些参数后的参数量。</p>
<table>
<thead>
<tr>
<th>params</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>V <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">from GPT-2</a></th>
<th>calculated params=$Vh+(12h^2+13h)l$</th>
<th>rectified</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 Small: 125M</td>
<td>768</td>
<td>12</td>
<td>64</td>
<td>0.5M</td>
<td>50257</td>
<td>123651840  $\approx$ 123.7M</td>
<td>125224704  $\approx$ 125.2M</td>
</tr>
<tr>
<td>GPT-3 Medium: 350M</td>
<td>1024</td>
<td>24</td>
<td>64</td>
<td>0.5M</td>
<td>50257</td>
<td>353772544 $\approx$353.8M</td>
<td>355869696 $\approx$ 355.9M</td>
</tr>
<tr>
<td>GPT-3 Large: 760M</td>
<td>1536</td>
<td>24</td>
<td>96</td>
<td>0.5M</td>
<td>50257</td>
<td>757151232 $\approx$ 757.1M</td>
<td>760296960 $\approx$ 760.3M</td>
</tr>
<tr>
<td>GPT-3 2.7B</td>
<td>2560</td>
<td>32</td>
<td>80</td>
<td>1M</td>
<td>50257</td>
<td>2646305280 $\approx$ 2.64B</td>
<td>2651548160 $\approx$ 2.65B</td>
</tr>
<tr>
<td>GPT-3 6.7B</td>
<td>4096</td>
<td>32</td>
<td>128</td>
<td>2M</td>
<td>50257</td>
<td>6650007552 $\approx$ 6.65B</td>
<td>6658396160 $\approx$ 6.67B</td>
</tr>
<tr>
<td>GPT-3 13B</td>
<td>5140</td>
<td>40</td>
<td>128</td>
<td>2M</td>
<td>50257</td>
<td>12942401780 $\approx$ 12.94B</td>
<td>12952928500 $\approx$ 12.95B</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>3.2M</td>
<td>50257</td>
<td>174579068928 $\approx$ 174.58B</td>
<td>174604234752 $\approx$ 174.60B</td>
</tr>
</tbody>
</table>
<blockquote>
<p>说明：</p>
<ol>
<li>GPT-3词表大小V在论文中没找到，所以用的GPT-2的词表大小，这里论文中是提到的</li>
</ol>
<p>more relative reading：</p>
<ul>
<li><a href="https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters">How does GPT-3 spend its 175B parameters?</a></li>
</ul>
</blockquote>
<h4 id="llama-1-llama--open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971">Llama 1: <a href="https://arxiv.org/pdf/2302.13971">LLaMa:  Open and Efficient Foundation Language Models</a></h4>
<p>模型结构：<a href="https://huggingface.co/docs/transformers/model_doc/llama">from hugging face transformers LLaMA</a></p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-22:35:04.png" alt="llama1" style="zoom: 40%;" />
<p>论文中说，该模型与Vanilla Transformer有三处区别：</p>
<ol>
<li>
<p>Pre-normalization and RMSNorm</p>
 <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-04-22:22:22.png" alt="image-20240904222219836" style="zoom: 50%;" />
<p>​	原始Transformer中使用post-norm居多，后来使用pre-norm居多，而且往往在FFN之前也加一个norm。尤其在大模型中，可能在通过LN之后MHA之前，Q和K还要加上旋转位置编码。</p>
<blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/474988236">【重新了解Transformer模型系列_1】PostNorm/PreNorm的差别</a></p>
</blockquote>
</li>
<li>
<p>SwiGLU activation function</p>
<p>SwiGLU激活函数不太像传统的ReLU等激活函数那样简单，比如ReLU都不带参数，而SwiGLU乍一看上去不明觉厉，实际上将SwiGLU理解成对传统FFM的替换，感觉更合适一些。直接看公式有点懵，看图更容易理解，下面是FFM和SwiGLU的对比</p>
 <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-23:03:55.png" alt="SwiGLU" style="zoom: 50%;" />
<p>SwiGLU写成公式就是$SwiGLU(x) = \left [ SiGU \left( gate_proj(x) \right) \odot up_proj(x)  \right] \times down_proj(x)$，其中可能有点困惑的是这个$\frac{8h}{3}$是怎么来的，实际上就是为了左右这两个结构的参数量相等：$2 \times h \times 4h \equiv 2 \times h \times \frac{8h}{3} + \frac{8h}{3} \times h$</p>
</li>
<li>
<p>Rotary Embedding</p>
</li>
</ol>
<p>下面是模型配置，验证一下前面推出来的参数量相关的公式能否对上：</p>
<table>
<thead>
<tr>
<th>params</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>calculated params=$2Vh+(12h^2+13h)l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>6.7B</td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_7b.yaml">11008</a></td>
<td>6706298880  $\approx$ 6.71B</td>
</tr>
<tr>
<td>13.0B</td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_13b.yaml">13824</a></td>
<td>12913254400  $\approx$ 12.91B</td>
</tr>
<tr>
<td>32.5B</td>
<td>6656</td>
<td>60</td>
<td>52</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_30b.yaml">17920</a></td>
<td>32328857600 $\approx$ 32.33B</td>
</tr>
<tr>
<td>65.2B</td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_65b.yaml">22016</a></td>
<td>64957317120 $\approx$ 64.96B</td>
</tr>
</tbody>
</table>
<p>每次总是差一点，但是差的不多，差在了哪里呢？MLP部分，理论上intermediate_size=$\frac{8h}{3}$，但是实际上可能会比这个值大一些，往往向上取到256、512、1024等的倍数，对矩阵乘法性能更好，因此来修正一下参数量、计算量、激活值的量化分析：</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-16:15:19.png" alt="transformer详细分析(llama)"  /></p>
<p>重新计算一下，这次参数量就很接近了</p>
<table>
<thead>
<tr>
<th>params</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>calculated params=$2Vh+(4h+4+3I)hl$</th>
</tr>
</thead>
<tbody>
<tr>
<td>6.7B</td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_7b.yaml">11008</a></td>
<td>6738673664  $\approx$ 6.74B</td>
</tr>
<tr>
<td>13.0B</td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_13b.yaml">13824</a></td>
<td>13016268800  $\approx$ 13.02B</td>
</tr>
<tr>
<td>32.5B</td>
<td>6656</td>
<td>60</td>
<td>52</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_30b.yaml">17920</a></td>
<td>32529735680 $\approx$ 32.53B</td>
</tr>
<tr>
<td>65.2B</td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_65b.yaml">22016</a></td>
<td>65286963200 $\approx$ 65.29B</td>
</tr>
</tbody>
</table>
<h4 id="llama-2-llama-2-open-foundation-and-fine-tuned-chat-modelshttpsarxivorgpdf230709288">Llama 2: <a href="https://arxiv.org/pdf/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></h4>
<p>Llama2在模型结构方面与Llama1相差不大，只是将MHA替换为GQA，将attention的context length从2k提升到4k。下面是Llama2的模型配置</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>MHA or GQA</th>
<th>calculated params=$2Vh+(12h^2+13h)l$</th>
<th>calculated params=$2Vh+(4h+4+3I)hl$</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B, <a href="https://gitee.com/hf-models/Llama-2-7b-hf/blob/main/config.json">config</a></td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>4M</td>
<td>32K</td>
<td>11008</td>
<td>MHA</td>
<td>6706298880  $\approx$ 6.71B</td>
<td>6738673664  $\approx$ 6.74B</td>
</tr>
<tr>
<td>13B, <a href="https://gitee.com/hf-models/Llama-2-13b-hf/blob/main/config.json">config</a></td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>4M</td>
<td>32K</td>
<td>13824</td>
<td>MHA</td>
<td>12913254400  $\approx$ 12.91B</td>
<td>13016268800  $\approx$ 13.02B</td>
</tr>
</tbody>
</table>
<p>至于70B的<a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a>（h=8192, l=80, a=64, b=4M, V=32K, intermediate_size=28672, g=8）使用了group=8的GQA，只有attention部分的参数量会发生一些变化，调整公式后，分别计算一下：</p>
<ul>
<li>calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$ = 5556092928 $\approx$ 55.56B，相差较大</li>
<li>llama calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$ = 68977950720 $\approx$ 68.98B，比较接近了</li>
</ul>
<p>因此，对于transformer而言，</p>
<ul>
<li>如果MLP是传统FFN那样的结构，calculated params=$2Vh+(12h^2+13h)l$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$</li>
</ul>
</li>
<li>如果MLP是SwiGLU那样的结构，calculated params=$2Vh+(4h+4+3I)hl$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</li>
</ul>
</li>
</ul>
<p>但是总的来说，transformer的复杂度还是$O(h^2l)$级别的</p>
<blockquote>
<p>more relative reading：</p>
<p><a href="https://medium.com/@saratbhargava/mastering-llama-math-part-1-a-step-by-step-guide-to-counting-parameters-in-llama-2-b3d73bc3ae31">“Mastering Llama Math (Part-1): A Step-by-Step Guide to Counting Parameters in Llama-2”</a></p>
<p><a href="https://bitddd.blog.csdn.net/article/details/132161203">LLM - Transformer &amp;&amp; LLaMA2 结构分析与 LoRA 详解</a></p>
</blockquote>
<h4 id="llama-3-the-llama-3-herd-of-modelshttpsarxivorgpdf240721783">Llama 3: <a href="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd of Models</a></h4>
<p>Llama3的改进相对于Llama2和Llama1，主要体现在使用了更高质量的数据和更大规模的训练，模型结构基本没变。下面是模型配置，</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>V</th>
<th>intermediate_size</th>
<th>GQA group</th>
<th>calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>8B, <a href="https://gitee.com/hf-models/llava-llama-3-8b-hf/blob/main/config.json">config</a></td>
<td>32</td>
<td>4096</td>
<td>32</td>
<td>4M-&gt;8M-&gt;16M</td>
<td>128K</td>
<td>14336</td>
<td>8</td>
<td>8028422144 $\approx$ 8.03B</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Meta-Llama-3-70B/blob/main/config.json">config</a></td>
<td>80</td>
<td>8192</td>
<td>64</td>
<td>4M-&gt;8M-&gt;16M</td>
<td>128K</td>
<td>28672</td>
<td>8</td>
<td>70550814720 $\approx$ 70.55B</td>
</tr>
<tr>
<td>405B</td>
<td>126</td>
<td>16384</td>
<td>128</td>
<td>4M-&gt;8M-&gt;16M</td>
<td>128K</td>
<td>53248</td>
<td>8</td>
<td>405849112576 $\approx$ 405.85B</td>
</tr>
</tbody>
</table>
<p>参考：</p>
<p><a href="https://blog.csdn.net/weixin_54338498/article/details/135269411">LLaMa-1/2/3 原理+源码——拆解 (KV-Cache, RoPE, RMSNorm, GQA, SwiGLU)</a></p>
<h3 id="关于激活的分析">关于激活的分析</h3>
<p>前面总说中间激活可能很占显存，我们来分析几个case。</p>
<p>GPT-3</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>s</th>
<th>V <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">from GPT-2</a></th>
<th>activation $\approx (34bsh+5bas^2)l$</th>
<th>activation （with GQA）$\approx \left [  (28+\frac{4g}{a})bsh+5bas^2\right]l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 Small: 125M</td>
<td>768</td>
<td>12</td>
<td>64</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>15972.0MB  $\approx 67.0 \times 2\Phi$</td>
<td>15873.0MB $\approx 66.58 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Medium: 350M</td>
<td>1024</td>
<td>24</td>
<td>64</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>32352.0MB $\approx 48.5 \times 2\Phi$</td>
<td>32088.0 $\approx 48.1 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Large: 760M</td>
<td>1536</td>
<td>24</td>
<td>96</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>48528.0 MB  $\approx 33.5 \times 2\Phi$</td>
<td>48120.0MB $\approx 33.2 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 2.7B</td>
<td>2560</td>
<td>32</td>
<td>80</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>55.3GB $\approx 11.0 \times 2\Phi$ wrong</td>
<td>54.4GB $\approx 10.82 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 6.7B</td>
<td>4096</td>
<td>32</td>
<td>128</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>88.5GB  $\approx 7.10 \times 2\Phi$</td>
<td>87.1GB $\approx 6.98 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 13B</td>
<td>5140</td>
<td>40</td>
<td>128</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>113.3GB $\approx 4.68 \times 2\Phi$</td>
<td>111.1GB $\approx 4.59 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>316.5GB $\approx 0.97 \times 2\Phi$</td>
<td>303.6GB $\approx 0.93 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>8</td>
<td>2048</td>
<td>50257</td>
<td>2532.0GB $\approx 7.77 \times 2\Phi$</td>
<td>2428.5GB $\approx 7.45 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>64</td>
<td>2048</td>
<td>50257</td>
<td>19.78TB $\approx 62.14 \times 2\Phi$</td>
<td>18.97TB $\approx 59.60 \times 2 \Phi$</td>
</tr>
</tbody>
</table>
<p>Llama-2：</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>s</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>GQA: group</th>
<th>activation （with GQA）$\approx \left [  (13+\frac{4g}{a})bsh+5bas^2 + 6bsI\right]l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B, <a href="https://gitee.com/hf-models/Llama-2-7b-hf/blob/main/config.json">config</a></td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>1</td>
<td>4096</td>
<td>32K</td>
<td>11008</td>
<td>32(MHA)</td>
<td>96.6GB $\approx 7.4 \times 2\Phi$</td>
</tr>
<tr>
<td>13B, <a href="https://gitee.com/hf-models/Llama-2-13b-hf/blob/main/config.json">config</a></td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>1</td>
<td>4096</td>
<td>32K</td>
<td>13824</td>
<td>40(MHA)</td>
<td>150.9GB $\approx 6.2 \times 2\Phi$</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a></td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>1</td>
<td>4096</td>
<td>32K</td>
<td>28672</td>
<td>8</td>
<td>486.25GB $\approx 3.7 \times 2\Phi$</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a></td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>8</td>
<td>4096</td>
<td>32K</td>
<td>28672</td>
<td>8</td>
<td>3890.0GB $\approx 29.8 \times 2\Phi$</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a></td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>64</td>
<td>4096</td>
<td>32K</td>
<td>28672</td>
<td>8</td>
<td>30.39TB $\approx 238.7 \times 2\Phi$</td>
</tr>
</tbody>
</table>
<blockquote>
<p>由于前面分析过，intermediate_size往往会略微大于$\frac{8h}{3}$，因此根据前面分析的llama结构，重新推导一下激活的计算公式，这里省略了。</p>
</blockquote>
<p>可以看出，当大batch、长序列的情况下，中间激活可以是模型参数所占显存的很多倍，即使使用了GQA。</p>
<p>上面都是在训练场景下的激活值分析，在推理阶段中，可以使用kv-cache减少模型计算量，同时中间激活也大幅度减少，kv-cache的大小为$2w_{kv}bs_mh$（单层），我们也来量化分析一下（假设$w_{kv}$=2，且s=1，推理context长度最后一个token的情况，即最坏情况）</p>
<table>
<thead>
<tr>
<th>config</th>
<th>b</th>
<th>$s_m$</th>
<th>h</th>
<th>a</th>
<th>l</th>
<th>kv_cache size=$2w_{kv}bs_mhl$</th>
<th>without kv-cache activation$\approx (34bs_mh+5bas_m^2)l$</th>
<th>with kv-cache activation $\approx (30bh+4bs_mh+5bas_m)l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 Small: 125M</td>
<td>1</td>
<td>2048</td>
<td>768</td>
<td>64</td>
<td>12</td>
<td>72MB $\approx 0.30 \times 2\Phi$</td>
<td>15972.0MB $\approx 67.0 \times 2\Phi$</td>
<td>79.8MB $\approx 0.33 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Medium: 350M</td>
<td>1</td>
<td>2048</td>
<td>1024</td>
<td>64</td>
<td>24</td>
<td>192MB $\approx 0.29 \times 2\Phi$</td>
<td>32352.0MB $\approx 48.5 \times 2\Phi$</td>
<td>207.7MB $\approx 0.31 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Large: 760M</td>
<td>1</td>
<td>2048</td>
<td>1536</td>
<td>96</td>
<td>24</td>
<td>288MB $\approx 0.20 \times 2\Phi$</td>
<td>48528.0MB $\approx 33.5 \times 2\Phi$</td>
<td>311.6MB $\approx 0.21 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 2.7B</td>
<td>1</td>
<td>2048</td>
<td>2560</td>
<td>80</td>
<td>32</td>
<td>640MB $\approx 0.12 \times 2\Phi$</td>
<td>55.3GB $\approx 11.0 \times 2\Phi$</td>
<td>667.3MB $\approx 0.13 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 6.7B</td>
<td>1</td>
<td>2048</td>
<td>4096</td>
<td>128</td>
<td>40</td>
<td>1280MB $\approx 0.1 \times 2\Phi$</td>
<td>110.6GB $\approx 8.9 \times 2 \Phi$</td>
<td>1334.7MB $\approx 0.1 \times 2 \Phi$</td>
</tr>
<tr>
<td>GPT-3 13B</td>
<td>1</td>
<td>2048</td>
<td>5140</td>
<td>128</td>
<td>96</td>
<td>3.76GB $\approx 0.15 \times 2\Phi$</td>
<td>272.0GB $\approx 11.2 \times 2\Phi$</td>
<td>3.89GB $\approx 0.16 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>1</td>
<td>2048</td>
<td>12288</td>
<td>128</td>
<td>96</td>
<td>9.0GB $\approx 0.02 \times 2\Phi$</td>
<td>316.5GB $\approx 0.97\times 2\Phi $</td>
<td>9.15GB $\approx 0.03 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>8</td>
<td>2048</td>
<td>12288</td>
<td>128</td>
<td>96</td>
<td>72.0GB $\approx 0.22 \times 2\Phi$</td>
<td>2532.0GB $\approx 7.77 \times 2\Phi$</td>
<td>73.2GB $\approx 0.22 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>64</td>
<td>2048</td>
<td>12288</td>
<td>128</td>
<td>96</td>
<td>576.0GB $\approx 1.77 \times 2\Phi$</td>
<td>19.78TB $\approx 62.1 \times 2\Phi$</td>
<td>585.6GB $\approx 1.80 \times 2\Phi$</td>
</tr>
</tbody>
</table>
<p>可以看出在推理时，kv-cache大幅度减少了中间激活。而且使用了kv-cache以后，kv-cache在激活中占据了绝大部分的比例，kv-cache甚至可以超过模型所占内存。</p>
<h3 id="关于计算量的分析">关于计算量的分析</h3>
<p>量化分析模型的计算量，主要是为了预估模型训练时间。根据前面的分析，一个FWD+BWD的iteration训练过程中，计算量FLOPs=$6 \times \Phi \times 输入tokens数量$，因此可以大致估计训练时间=$\frac{6 \times \Phi \times 输入tokens数量}{GPU数量\times GPU算力(flops) \times MFU}$。</p>
<h2 id="其他说明">其他说明</h2>
<h6 id="1-layernorm的计算">1. LayerNorm的计算</h6>
<p>LayerNorm的计算过程见<a href="https://blog.csdn.net/weixin_39228381/article/details/107939602">pytorch LayerNorm参数详解，计算过程</a>，总结一下就是：</p>
<ol>
<li>比如输入是<code>[b,s,h]</code>，LN的<code>normalized_shape=[h]</code>，此时就是对每一个大小为<code>h</code>的向量分别进行归一化（一共<code>b*s</code>个）</li>
<li>然后如果LN的<code>elementwise_affine=True</code>，就需要对每个大小为<code>h</code>的向量elementwise的乘上$\gamma: [h]$，再elementwise的加上$\beta:[h]$，$\gamma$和$\beta$就是该LN层的两个可学习的参数。如果LN的<code>elementwise_affine=False</code>，则只会进行第一步的归一化，不会进行第二步的affine</li>
</ol>
<p>一个有趣的问题是，<a href="https://zhuanlan.zhihu.com/p/707778968">Transformer中的LayerNorm可以并行吗？</a></p>
<p>关键词： Welford online Algorithm，当一个集合新增加一个元素$x_N$的时候，可以通过前N-1个样本的corrected sum of squares($\sum_{i=1}^{N-1}(x_i-\bar{x})^2$)，计算出前N个样本的corrected sum of squares，从而只需要one pass就可以完成LN的计算（之前navie的方法是two pass）</p>
<h6 id="2-关于dropout的位置">2. 关于dropout的位置</h6>
<p>一共（可能）在有四个地方有dropout：</p>
<ol>
<li>在PositionalEmbedding中有一个dropout：<code>dropout(x + PositionEmbedding(x))</code>，不过好像LLM现在使用旋转位置编码RoPE多一些，在计算attention之前在Q和K上加上RoPE，一开始输入的embedding不加PositionalEmbedding了</li>
<li>在softmax计算得到的attention score之后有一个droput：$dropout( softmax(\frac{QK^T}{scale}+casual_mask) )$</li>
<li>在sublayer（Attention和MLP）计算完之后，各有一个dropout：<code>x+dropout(sublayer(norm(x)))</code></li>
</ol>
<h1 id="总结">总结</h1>
<p>transformer的参数量的复杂度是$O(h^2l)$级别的，粗略估计可以认为是$12h^2l$或者$(4h+3I)hl$，如果要详细分析，就要看一看每个部分的结构，是否使用了bias，使用的不同优化，比如：</p>
<ul>
<li>如果MLP是传统FFN那样的结构，calculated params=$2Vh+(12h^2+13h)l$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$</li>
</ul>
</li>
<li>如果MLP是SwiGLU那样的结构，calculated params=$2Vh+(4h+4+3I)hl$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</li>
</ul>
</li>
</ul>
<p>对transformer中间激活的分析要分训练场景和推理场景</p>
<ul>
<li>在训练场景中，中间激活可以是模型参数所占显存的很多倍，尤其在大batch、长序列的情况下。
<ul>
<li>中间激活值所占显存粗略估计可以认为是$(34bsh+5bas^2)l$或者$(17bsh+5bas^2+6bsI)l$，可以看出与输入token数量（batch和seq_len）、隐藏层维度、头数、intermediate_size、层数相关，因此相对参数量的分析稍微复杂一点。</li>
</ul>
</li>
<li>在推理场景中，prefill阶段基本同训练场景，decode阶段每次输入的序列长度为1，而且默认使用kv-cache。由于使用kv-cache，中间激活相对于训练时的中间激活大幅度减小，但是在大batch、长序列的情况下，kv-cache的显存占用仍然可能超过模型参数的显存占用。还有一点需要注意，推理场景中kv-cache在中间激活中占据了绝大部分。
<ul>
<li>中间激活值所占显存粗略估计可以认为是$(30bh+4bs_mh+5bas_m)l$或者$(13bh+4bs_mh+5bs_ma+6bI)l$</li>
</ul>
</li>
</ul>
<p>对transformer的计算量的分析比较简单，transformer中计算较为规整，计算量体现在若干个大块矩阵的乘法。一般量化分析计算量主要是为了预估模型训练时间，所以一般分析的不多（一般也没有机会训练大模型，如果训练普通规模的网络，尝试跑几个iteration就能估计）。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>A survey of Efficient Transformer on Inference</title>
      <link>https://qinganzhang.github.io/posts/a_survey_of_efficient_transformer_on_inference/</link>
      <pubDate>Mon, 04 Mar 2024 20:57:14 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/a_survey_of_efficient_transformer_on_inference/</guid>
      <description>Abstract Transformer模型架构在自然语言处理、计算机视觉、强化学习等领域表现出了强大的能力，已经成为当前深度学习很多模型的核心，当前发展迅</description>
      <content:encoded><![CDATA[<h2 id="abstract">Abstract</h2>
<p>Transformer模型架构在自然语言处理、计算机视觉、强化学习等领域表现出了强大的能力，已经成为当前深度学习很多模型的核心，当前发展迅速的大模型更加凸显出这一点。由于Transformer较高的复杂度，限制了其在很多场景中的应用。因此，为了提高模型的高效性，针对Transformer的改进层出不穷。本文从模型算法的角度出发，关注于模型推理的场景，从不同层次梳理当前提高模型效率的方法，包括设计复杂度更低的注意力机制、提出更加高效的网络设计、进行模型压缩和优化的方法，并针对每一种方法进一步做了分类和总结，并选取具有代表性的方法进行说明。本文最后探讨了Transformer未来可能的发展方向。</p>
<h1 id="1-introduction">1. Introduction</h1>
<p>近年来，深度学习发展迅速，尤其是以Transformer为核心的结构，构成了当前深度学习架构的核心，在计算机视觉、自然语言处理等领域，SOTA的模型均以Transformer架构为核心，而且当前诸如ChatGPT等大模型，核心同样是基于RLHF的Transformer，显示出了Transformer强大的能力。</p>
<p>但是，受限于Transformer相对于序列长度平方的计算复杂度，在图片、视频等需要长序列的场景下，相对于传统的CNN架构，Transformer仍不够有效，无法得到有效的应用。Transformer的平方复杂度来源于注意力机制，因此，许多研究关注于改进注意力机制，降低注意力机制的复杂度，提出新的注意力机制。除此之外，不同的Transformer架构被提出，这些架构在Vanilla Transformer架构上做出改进来提高计算和访存效率，这可以归结为efficient attention或efficient Transformer网络架构的设计。</p>
<p>除此之外，为了进一步降低Transformer模型的复杂度，提高模型的推理速度，efficient Transformer的网络架构还可以使用一些模型压缩的方法，比如剪枝、量化、蒸馏、神经架构搜索（NAS）等，这些方法可以在基本保持模型效果的同时，降低模型复杂度，减小模型大小，进一步加速模型的推理。</p>
<p>需要说明的是，efficiency是一个比较宽泛的用词，包括data-efficiency, model-efficiency（efficient architecture），training-efficiency，inference-efficiency。其中data-efficiency一般指充分利用、挖掘数据，从小规模数据中进行学习；model-efficiency侧重于降低模型的复杂度或是参数量；training-efficiency指使用更少的资源（或提高资源利用效率）、使用更少的时间来进行训练；inference-efficiency通常也被成为模型推理加速，它针对训练好的模型，尽可能提高模型的推理速度、吞吐量等。本综述中只涉及到model-efficiency，并介绍一些针对Transformer的模型压缩方法。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/613685663">Blog: Efficient Deep Learning 高效深度学习</a></p>
</blockquote>
<h1 id="2-model-efficiency">2. Model Efficiency</h1>
<p>Model efficiency主要侧重于提出新的架构，或者改善现有架构，从而降低模型复杂度或参数量。不同于训练场景只关注于模型的参数量，在推理场景中，模型在访存、计算等方面同样需要高效，2.1节说明了在推理场景中模型所关注的几种不同的efficiency。为了能够量化的来比较模型在推理时的efficiency，2.2节总结了一些评估模型推理性能的指标。</p>
<h2 id="21-kinds-of-efficiency">2.1 Kinds of Efficiency</h2>
<p>模型的高效是一个相对的概念，但是有几个发展方向是确定的，比如高效的模型一般具有一下几个特征：模型中存在较多的计算密集型算子而非访存密集型算子（有助于充分发挥硬件性能），模型计算复杂度尽量低（可以应用于更加广泛的场景），模型参数量尽量少（可以减少存储空间和内存的占用），受限于模型的结构、应用的场景，在应用中需要先对这几个方向进行分析，然后才能够做进一步的分析和优化。</p>
<h3 id="211-memory-efficiency">2.1.1 Memory Efficiency</h3>
<p>访问内存的开销是影响模型推理速度的一个关键因素。Transformer中许多操作，比如频繁的reshape，element-wise相加，归一化等操作，这些操作或算子是访存密集型的，即大部分时间花费在访存上，而计算耗时占比很小，此时模型推理速度主要受到内存带宽限制。减少模型推理过程在访存上的时间开销，就是提高memory efficiency。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/414430541">Blog: 深度学习模型大小与模型推理速度的探讨</a></p>
</blockquote>
<h3 id="212-computation-efficiency">2.1.2. Computation Efficiency</h3>
<p>模型的computation efficiency往往指的是模型的算法复杂度低。特别的，针对Transformer而言，当序列长度序列较小时，此时模型的计算开销主要集中在FFN模块，计算复杂度近似地线性于序列长度。但是在很多使用Transformer的场景中，比如图片、视频等场景中，输入序列长度较大。此时，模型的计算开销会集中于自注意力层，产生相对于序列长度平方的复杂度，限制了Transformer在很多场景中的应用。</p>
<blockquote>
<p><a href="https://0809zheng.github.io/2021/07/12/efficienttransformer.html">Blog: Efficient Transformers</a></p>
<p><a href="https://arxiv.org/pdf/1706.03762.pdf">Paper: Attention is all you need</a> 中FFN与Attention复杂度对比</p>
</blockquote>
<h3 id="213-parameter-efficiency">2.1.3 Parameter Efficiency</h3>
<p>Parameter Efficiency主要指的是模型的轻量化和较少的参数量。使用参数量较少的模型，可以减少模型在磁盘上存储的空间和模型加载后内存的占用。需要注意的是，随着大模型的发展，受限于大模型训练的成本，大模型的微调技术PEFT（Parameter-efficient fine-tuning）发展迅速。PEFT旨在最小化微调参数的数量和计算复杂度，以减少大模型微调的成本，来提高模型在新任务上的性能。这里所说的Parameter Efficiency更加类似于模型轻量化的概念。</p>
<h2 id="22-metrics">2.2 Metrics</h2>
<p>设计神经网络架构的主要考虑因素之一就是效果和成本的权衡。一般情况下，一个模型的参数量越多，计算量越大，模型的容量越大，该模型的效果就越好。但是，不同模型在不同硬件平台上的推理效果往往无法直接比较。因此，在比较模型推理性能时，经常会使用一些指标，从不同角度对模型的推理性能进行比较。</p>
<h3 id="221-计算量">2.2.1 计算量</h3>
<p>计算量是评价模型efficiency最常用的指标，包括很多文献进行对比时，常常会将计算量和参数量作为最重要的比较依据。计算量是模型所需的计算次数，模型的整体计算量等于模型中每个算子的计算量之和。衡量计算量主要有两个指标：</p>
<ul>
<li>
<p>FLOPs（Floating Point Operations，浮点计算次数）：计算量一般用OPs（Operations，计算次数）来表示，由于最常用的格式为float32，因此也常被写作为FLOPs。</p>
</li>
<li>
<p>MACs（Multiply-Accumulate Operations，乘加累计操作数）：1个MACs包括一个乘法操作与一个加法操作，大约相当于2FLOPs。在很多硬件上，Multiply-Accumulate可以使用单独一个指令完成，而且很多对tensor的操作也是Multiply-Accumulate操作。FLOPs通常用于模型的理论上计算量的分析，MACs更加贴近真实的计算量。</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation">Multiply–accumulate_operation from wiki</a></p>
</blockquote>
</li>
</ul>
<h3 id="222-参数量">2.2.2 参数量</h3>
<p>参数量是模型中参数的总和，直接反应了模型在磁盘中存储的大小。虽然参数量并不直接影响推理性能，但是参数量一方面会影响内存占用，另一方面会影响程序初始化时间。而且，在某些场景下，参数量是很重要的指标。比如在嵌入式或移动端场景下，磁盘空间极其有限，此时往往会对模型的参数量有比较严格的限制。在这种情况下，除了在设计时减少参数量，还可以通过压缩模型权重的方式进一步降低打包后模型的大小，但是这样会带来解压缩开销，会在一定程度上增加程序初始化的时间。</p>
<h3 id="223-访存量">2.2.3 访存量</h3>
<p>访存量往往是最容易被忽略的指标，但它对推理性能有着极大的影响。访存量是指模型推理时所需访问内存的数据量，反应了模型对存储带宽的要求。访存量有时也称作MAC（Memory Access Cost）或者MOPs（Memory Operations），一般用Bytes（或KM/MB/GB）来表示，即模型需要读取/写入多少Bytes的内存数据。和计算量一样，模型整体访存量等于模型各个算子的访存量之和。</p>
<h3 id="224-运行速度">2.2.4 运行速度</h3>
<p>运行速度是衡量模型efficiency最有效的指标，但是需要基于相同的硬件平台进行对比，而且，即使使用相同的硬件平台，使用不同的软件环境、使用流水线的效率等因素也对最终的推理速度有极大的影响，所以往往在实践中难以直接进行比较。运行速度主要有两种形式进行反应：</p>
<ul>
<li>吞吐量（Throughput）：在单位时间内处理的样本个数，相当于可以并行处理的任务量，充分利用流水线可以极大提高模型推理的吞吐量。</li>
<li>延迟（Latency）：通常指单个样本或单个batch处理完成的时间，相当于串行处理一个任务所需要的时间。相对于吞吐量，流水线无法减少延迟。因此，对于需要实时推理的模型而言，需要考虑延迟而非提高吞吐量。</li>
</ul>
<blockquote>
<p>[Paper: THE EFFICIENCY MISNOMER]</p>
</blockquote>
<p>需要注意的是，使用单个指标对模型进行评估往往会导致不全面的结论，甚至评价指标无法真实地比较模型在硬件上的推理速度。比如在下图中，相较于其他网络，在保持类似精确度的情况下，EfficientNet具有相对较小的计算量（GFLOPs）和参数量（Million Parameters），但是模型的推理速度并没有相对于其他模型很明显的提升，甚至有时其他模型推理速度更快一些。虽然如此，但是固定某些指标进行比较，仍是一个相对公平的方法。而且通过分析模型的推理瓶颈，可以针对性的提升模型的某些指标，从而加速推理。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-03-04-21:13:15.png" alt="image-20230620112102735" style="zoom:80%;" />
<blockquote>
<p><a href="">Paper: THE EFFICIENCY MISNOMER</a> Figure5</p>
</blockquote>
<h2 id="23-find-the-bottleneck">2.3 Find the Bottleneck</h2>
<p>不同的模型具有不同的特征，即使同一个模型的不同部分也有不同的特征，比如某些部分是计算密集性的，有些部分是访存密集型的，这里选取Bert和GPT-2两个典型的模型进行分析。</p>
<p>为了综合衡量计算密集型与访存密集型，通常使用算数强度（arithmetic intensity，也称计算密度，计算强度，计算访存比等）来表示。算数强度表示从内存加载的每个字节可以进行的浮点运算的数量，反映了程序相对于访存而言计算的密集程度，可以通过计算量FLOPs除以访存量来计算得到。RoofLine模型是基于算数强度，来评估程序在硬件上能达到性能上界的模型，即给定一个硬件资源的限制（算力、内存带宽），模型在该硬件上可以达到的最大计算速度。</p>
<p>当模型的计算密度较小时，访存相对较多，计算相对较少，模型性能主要受到内存带宽限制，此时模型是访存密集型的。反之如果模型的计算密度较大，访存相对较少，计算相对较多，模型性能主要受到硬件算力的限制，此时模型是计算密集型的。一般而言，模型的计算密度越大，越有可能提升硬件的计算效率，充分发挥硬件性能。对于访存密集型算子，推理时间跟访存量呈线性关系，而对于计算密集型算子，推理时间跟计算量呈线性关系。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/411522457">Blog: 深度学习模型大小与模型推理速度的探讨</a></p>
<p>[Paper: Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures]</p>
</blockquote>
<figure>
    <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-03-04-21:13:38.png" width=400/>
    <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-03-04-21:13:50.png" width=400/>
</figure>
<p>BERT是Encoder-only的模型，而GPT-2是Decoder-only的模型，如图a所示，这个区别导致两类模型的计算密度差异很大，而两种不同大小的BERT模型的计算密度差异反而不是很大。究其原因，是由于Decoder模型中，每次都是逐个token输入并解码，导致实际矩阵乘法退化为矩阵与向量的乘法，数据重用有限，使其更容易受到内存带宽的限制。因此，如图b所示，当使用高算力的硬件进行推理性能测试时，以BERT-Base的推理时间为基准，尽管相对于BERT-Base，GPT-2具有更少的计算量，但是由于访存量的激增，导致计算密度变低，最终在实际推理时，推理延时远远慢于BERT-Base。因此，针对模型进行优化时，需要综合不同的指标，分析模型的特点，找到模型的瓶颈，从而进行针对性的优化，才能对最终的推理性能有较大提升。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/619596323">Blog: LLM Inference CookBook</a></p>
<p>[Paper: Full Stack Optimization of Transformer Inference: a Survey] Figure 6 9</p>
</blockquote>
<h1 id="3-efficient-transformer">3. Efficient Transformer</h1>
<p>虽然当前很多SOTA模型都是基于Transformer，而且很多大模型也都是以Transformer为基础，但是由于Transformer相对于输入序列的平方的复杂度，使得在很多需要长序列的场景中，比如处理图片、视频时受到很大的限制，因此很多方法被提出来改善模型的复杂度，比如降低模型的时间复杂度，减少模型的参数量，设计更适合于硬件的模型来减少访存等。本章节从三个不同角度来讨论使得模型在设计上更加高效的方法。</p>
<h2 id="31-efficient-attention">3.1 Efficient Attention</h2>
<p>注意力机制作为Transformer的核心，它使得模型可以捕捉全局信息，进行长距离建模。但是注意力机制最核心的操作是进行矩阵相乘，由于词向量维数一般固定且不是很大，可以认为是常数，因此时间复杂度可以认为是输入序列长度的平方。本节讨论一些方法，侧重于改善注意力机制的时间复杂度，并根据核心思想进行分类和总结。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/263031249">Blog: Transformers大家族——Efficient Transformers: A Survey</a></p>
<p><a href="https://blog.csdn.net/weixin_44808865/article/details/119173304">Blog: 「ArXiv2020」【Efficient Transformers: A Survey】论文笔记（更新中）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/602394470">Blog: Efficient Transformers: A Survey</a></p>
<p><a href="https://blog.csdn.net/triplemeng/article/details/113821740">Blog: 进击的Transformer &mdash; 一文介绍Efficient Transformers</a></p>
</blockquote>
<ul>
<li>
<p>Fixed Patterns</p>
<p>将注意力机制从全局变为局部，限制注意力机制的范围，从而降低复杂度。根据限制的范围和形式，可以分为blockwise pattern， strided pattern，compressed pattern。</p>
<p>Blockwise pattern将输入序列切成多个block，只在每个block内部进行注意力机制的计算，显著降低了计算复杂度，比如Blockwise Attention、Local Attention等。但是这样简单的切割会导致序列不连贯，缺乏block之间的信息交互，注意力机制能力有限。虽然很简单，但是确实后续很多改进的基础。</p>
<p>Strided pattern采用滑动窗口的形式，每个token与周围相邻的几个token计算注意力，即按固定间隔进行注意力机制的计算。比如，Sparse Transformer使用类似strided形式的滑动窗口，LongFormer使用类似dilated形式的滑动窗口。相较于Blockwise pattern，考虑到自然语言很多情况下都是局部相关性较高，因此在一个窗口范围内计算注意力可能不会丢失太多信息。</p>
<p>Compressed pattern则是先通过卷积、池化等CNN操作进行下采样，从而有效减小序列长度，将输入序列转换到固定的模式，降低计算注意力机制的复杂度。</p>
<blockquote>
<p>Blockwise attn: <a href="https://arxiv.org/abs/1911.02972">Blockwise Self-Attention for Long Document Understanding</a></p>
<p>Local attn: <a href="https://arxiv.org/abs/1802.05751">Image Transformer</a></p>
<p>Sparse Trans: <a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a></p>
<p>LongFormer: <a href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer</a></p>
</blockquote>
</li>
<li>
<p>Combination of Patterns</p>
<p>对于输入的token，可以在不同维度、不同区域上组合使用不同的注意力机制，从而学习到更好的特征。比如，Sparse Transformer将一半的注意力头使用strided pattern，另一半注意力头使用local pattern。类似的，在Axial Tranformer中不是像多数注意力模块一样先将多维输入展平，而是每次沿着特征图的单个维度计算自注意力，然后组合多个维度的特征图以得到覆盖全局感受野的特征图。</p>
<blockquote>
<p>Axial Trans: <a href="https://arxiv.org/abs/1912.12180">Axial Attention in Multidimensional Transformers</a></p>
</blockquote>
</li>
<li>
<p>Learnable Patterns</p>
<p>Learnable pattern是对fixed pattern的拓展，fixed pattern是提前规定好一些区域，在这些区域中进行注意力，而learnable pattern则是引入可学习参数，让模型自己找到计算注意力的区域，即以数据驱动的方式指导模型的学习过程。比如Reformer引入基于哈希的相似度度量方法来将输入进行切割，Routing Transformer对token向量进行k-means聚类，从而将整体序列分割为多个子序列。因此，从最后注意力计算的角度看，Learnable pattern与fixed pattern是一致的，都是通过将整体序列进行切分，只在子序列中计算注意力，不同的只是子序列的划分方式是提前确定的还是模型学习得到的。</p>
<blockquote>
<p>Reformer: <a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a></p>
<p>Routing Trans: <a href="https://arxiv.org/abs/2003.05997">Efficient Content-Based Sparse Attention with Routing Transformers</a></p>
</blockquote>
</li>
<li>
<p>Neural Memory</p>
<p>Neural memory类似于compressed pattern中先压缩再计算注意力的想法，Set Transformer中第一次使用了这种方法。具体而言，就是初始化k个untrainable向量（k&laquo;n），n个token embedding和这k个trainable向量计算注意力，压缩得到k个向量，然后k个向量再和n个向量计算注意力还原得到n个向量，达到抽取输入序列特征的目的。这k个untrainable向量就可以理解为memory，用于处理临时上下文信息。</p>
<blockquote>
<p>Set Trans: <a href="https://arxiv.org/abs/1810.00825">Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</a></p>
</blockquote>
</li>
<li>
<p>Low-Rank</p>
<p>Low-rank通过矩阵压缩或矩阵近似来降低计算注意力的复杂度。假设$N$是序列长度，$d$是向量维度，$k$是矩阵压缩的超参数。在Linformer中观察到，经过softmax计算之后得到的$N \times N$的attention score矩阵是不满秩的，这意味着不需要一个完整的attention score矩阵，可以使用一个$N \times k$的矩阵来近似$N \times N$的attention score矩阵，同时需要将$N \times d$的key和value向量映射到$k \times d$维空间，由于$k$是固定的超参数，因此将注意力机制的复杂度降低到了线性级别。</p>
<blockquote>
<p>Linformer: <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear Complexity</a></p>
</blockquote>
</li>
<li>
<p>Kernels</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/157490738">Blog: 线性Attention的探索：Attention必须有个Softmax吗？</a></p>
<p>Linear Trans: <a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a></p>
</blockquote>
<p>之前的一些研究中提到制约注意力机制性能的关键因素是其中的softmax操作，Scaled-Dot Attention其实就是对value做加权平均，未来得到attention score，就必须先对query和key进行运算。但是，以核函数变换的形式可以得到一个更加通用的注意力机制的数学表达，通过将相似性度量拆分，可以实现注意力机制线性的复杂度（原来的相似度计算中，指数操作的存在使得query，key，value的矩阵操作无法使用结合律）。由于通过kernel方法计算得到的是注意力矩阵的一种近似形式，因此核方法也可以认为是一种特殊的low-rank方法。</p>
</li>
<li>
<p>Recurrence</p>
<p>Recurrence实际上也是fixed pattern中blockwise的一种延申，本质上仍是对输入序列进行区域划分， 只是它进一步对划分后的block做了一层训练连接，通过这样的层级关系就可以把一个长序列的输出得到更好的表征。Transformer-XL使用segment-level recurrence，将上一个segment的状态缓存下来，然后再计算当前segment的时候重复使用上一个的隐藏状态，虽然加快了推理速度，但是由于需要进行缓存，是一种空间换时间的方案。</p>
<blockquote>
<p>Transformer-XL: <a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p>
</blockquote>
</li>
</ul>
<h2 id="32-efficient-architecture-design">3.2 Efficient Architecture Design</h2>
<p>除了改善Transformer中注意力机制的复杂度，修改Transformer中其他部分可能同样有效。实际上，比如针对Bert-Base而言，从参数量的角度看，模型总的参数量约为104MB，其中多头注意力机制部分的参数量大约为27MB；从计算量和访存量的角度看，即使针对较长的序列而言，多头注意力机制部分也只是占了整个模型计算量/访存量的一半左右。因此，设计更加高效的网络架构，同样可以提高模型运行时的性能。同样，本节根据不同模型架构的设计思路和特点进行分类总结。</p>
<blockquote>
<p><a href="https://blog.csdn.net/qq_28385535/article/details/127213648">Blog: Hydra Attention: Efficient Attention with Many Heads翻译</a></p>
<p><a href="https://blog.csdn.net/huangblog/article/details/119639001">Blog: 一文懂“NLP Bert-base” 模型参数量计算</a></p>
<p>[Paper: Full Stack Optimization of Transformer Inference: a Survey] Table 3</p>
</blockquote>
<ul>
<li>
<p>增加感受野</p>
<p>通过增加感受野，模型可以处理更加高分辨率的图像，但同时需要尽量降低额外带来的计算量。Efficient-ViT使用MobileNetV2中的MBConv作为基本块，使用线性注意力机制替代传统注意力机制，并且在前馈神经网络中使用可变形卷积。EdgeNeXt与之相似，它使用分裂的深度转置注意力模块（Split Depth-wise Transpose Attention， SDTA）来替代传统的多头注意力机制，SDTA将输入通道分成多个通道组，利用深度可分离卷积和跨通道的自注意力来有效增加模型的感受野。</p>
<blockquote>
<p>[Paper: EfficientViT: Lightweight Multi-Scale Attention for On-Device Semantic Segmentation]</p>
<p>[Paper: EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications]</p>
<p>[Paper: MobileNetV2: Inverted Residuals and Linear Bottlenecks]</p>
</blockquote>
</li>
<li>
<p>使用池化层</p>
<p>通常在注意力机制之后使用池化层，来减少推理延迟。NextViT交替使用卷积块和注意力块，其中卷积块由多头卷积注意力和MLP构成，卷积块主要使用了多头自注意力机制，但是注意力机制中key和value都先经过了一个池化层。PoolFormer总结了一种成为MetaFormer的通用架构，通过使用不同的Token-Mixer可以获得不同的具体架构，当Token-Mixer被修改为一个简单的池化层时，PoolFormer以极少的参数同样获得了与其他模型相似的准确度。</p>
<blockquote>
<p>[Paper: Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios]</p>
<p>[Paper: MetaFormer Is Actually What You Need for Vision]</p>
</blockquote>
</li>
<li>
<p>使用局部特征</p>
<p>LeViT再次将充分使用CNN的局部特征，尤其是首先通过卷积来得到低分辨率的特征图，然后通过修改注意力模块进行特征图的下采样。MobileViT网络主要使用MobileViT块和MBConv块堆叠而成，其中MobileViT块负责进行全局信息与局部信息的交互，其中将特征图通过卷积层进行局部建模得到局部信息，然后将局部信息的特征图基于注意力机制进行全局建模，最后进行残差连接。</p>
<blockquote>
<p>[Paper: LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference]</p>
<p>[Paper: MobileViT: Light-Weight, General-Purpose, and Mobile-Friendly Vision Transformer]</p>
</blockquote>
</li>
<li>
<p>保持维度一致性</p>
<p>相对于多头注意力机制角度的计算量对于推理延迟的影响，特征维度一致性对推理延迟同样甚至有更大的影响，比如网络中存在大量低效的reshape操作，反复调整特征的维度，会极大影响推理的速度。EfficientFormer提出了一种维度一致性的设计，将网络分成一个特征图为四维的部分和一个特征图为三维的部分，网络从patch embedding开始，首先进入四维特征图部分，最后进入三维特征图部分。在四维特征图部分，主要通过卷积结构为主；在三维特征图部分，此时网络结构中加入注意力机制和MLP结构。最终四维和三维分区的长度是通过网络架构搜索得到的。</p>
<blockquote>
<p>[Paper: EfficientFormer: Vision Transformers at MobileNet Speed]</p>
</blockquote>
</li>
<li>
<p>并行网络</p>
<p>一些模型可以并行的执行特定的层，从而加快推理速度。比如Mobile-Former的两个并行分支分别提取局部和全局信息，通过双向桥接进行信息的双向融合。MixFormer基于并行分支设计，将局部自注意力和通道分离卷积两个分支进行交互，并且根据不同分支上操作共享参数的维度不同，使用双向交互模块融合不同维度的信息，针对每个分支提供互补的信息来进一步学习到更好的特征。</p>
<blockquote>
<p>[Paper: Mobile-Former: Bridging MobileNet and Transformer]</p>
<p>[Paper: MixFormer: Mixing Features across Windows and Dimensions]</p>
</blockquote>
</li>
</ul>
<h2 id="33-efficient-efforts">3.3 Efficient Efforts</h2>
<p>除了针对注意力机制和Transformer的架构进行改进，通用的模型压缩同样可以提高Transformer的推理性能，同时保持模型精度或将模型精度的下降控制在一个合理范围内。模型压缩主要包括剪枝、蒸馏、量化等。其中，剪枝和蒸馏可以减少模型参数量，量化可以提高模型的访存效率，而且不同的方法可以是正交的，即可以先进行模型的剪枝，再进行模型的量化。许多研究提出了不同的方法来进行Transformer模型的压缩，本节简单进行介绍。由于在自然语言处理领域和计算机视觉领域中，模型压缩的方法可能略有不同，本节更加侧重于视觉方面的模型压缩方法。</p>
<p>此外，由于Transformer的广泛应用，为了提高模型的推理性能，在设计模型架构时有时需要将硬件也纳入考虑，比如考虑到硬件限制的网络架构搜索，软硬件协同设计等，虽然本综述不涉及硬件的描述，但是本节最后介绍一种针对GPU的新型注意力机制FlashAttention，通过优化注意力机制算法的访存过程，来显著提高模型的运行速度、降低所需内存，同时保持对结果不变和对用户的透明。</p>
<h3 id="331-pruning">3.3.1 Pruning</h3>
<p>剪枝方法基于lottery ticket假设，即模型中只有小部分参数起到了核心作用，其他的大部分参数都是无效参数或是不重要的参数，可以去除掉，在减小模型参数量的同时，保持模型原有的精度。剪枝可以分为结构化剪枝与非结构化剪枝。非结构化剪枝允许修建任何参数，定位参数中接近于0的参数，将这些参数归零，使得权重矩阵稀疏化。虽然非结构化剪枝可以极大减少模型参数，但是由于硬件的限制，很多场景中无法完全发挥非结构化剪枝的效果。结构化剪枝是粒度较大的剪枝，修剪模型中结构化的部分，比如权重的整行，多头注意力中不需要的注意力头，多层Transformer中不需要的若干层等。由于存在一定限制，结构化剪枝的模型压缩率较小，但是更加适合于硬件运行。</p>
<p>考虑到Transformer中大部分的计算量是在多头注意力（MSA）和前馈神经网络（FFN）部分，为了简Transformer的结构，Vision Transformer Pruning（VTP）是第一个专门用于Vision Tranormer的剪枝方法。VTP首先使用L1稀疏正则化进行训练，VTP获取每一个Transformer block中Dimension的重要性分数，然后对分数较低的Dimension进行裁剪，这样大量的不重要的Dimension将会被裁剪，最后进行微调。不同于VTP主要关注于通道维度的冗余，PS-ViT方法关注于patch层面的冗余，通过计算patch对于最终分类特征的重要性得分来判断每个patch的有效性，同时保证信息一致性，显著降低了计算量并保持了原始模型的精度。NViT在剪枝时将模型的推理时间纳入考虑，通过重分配使用的参数，进行全局结构性剪枝。后续模型分别针对剪枝范围和粒度、剪枝方法、剪枝过程等做出改进，进一步提高模型的推理性能。</p>
<blockquote>
<p>[Paper: THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS]</p>
<p>[Paper: Vision Transformer Pruning]</p>
<p>[Paper: Vision Transformer with Progressive Sampling]</p>
<p>[Paper: NViT: Vision Transformer Compression and Parameter Redistribution]</p>
</blockquote>
<h3 id="332-distillation">3.3.2 Distillation</h3>
<p>蒸馏是指用教师模型来指导学生模型训练，通过蒸馏的方式让学生模型学习到教师模型的知识。在模型压缩中，教师模型是一个提前训练好的复杂模型，而学生模型则是一个规模较小的模型。由训练好的教师模型，在相同的数据下，通过将教师网络对该样本的预测值作为学生模型的预测目标指导学生模型学习。通过教师模型的指导，让学生模型学习教师模型的泛化能力，以达到或媲美教师模型的准确度。</p>
<p>在计算机视觉领域，DeiT在ViT的基础上，提出了一种专门针对Transformer的蒸馏方法，将distillation token与原始的class token同时加入网络，同时对损失函数进行相应的变化，显著减小了模型训练时间和训练所需的数据量。Mainfold Distiallation方法考虑了视觉Transformer的特点，在模型中间层引入了patch层级的细粒度监督信号，它是一种基于内积计算特征空间的流形结构表示，通过约束学生模型与教师模型的特征空间具有相似的流形结构，可以更好的将教师模型的知识迁移到学生模型中。TaT中进一步考虑到，由于教师模型和学生模型在结构上的异构型，直接对比像素级别的特征图可能导致不对齐的问题，因此使用注意力机制来隐式对齐语义，并提出一种近似的方法来改善方法的复杂度。</p>
<blockquote>
<p>[Training data-efficient image transformers &amp; distillation through attention]</p>
<p>[Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation]</p>
<p>[Knowledge Distillation via the Target-aware Transformer]</p>
</blockquote>
<h3 id="333-quantization">3.3.3 Quantization</h3>
<p>量化的基本思想即使用低精度、低比特的数据类型来代替原本的浮点数据类型，可以量化参数权重，也可以量化激活值，不但显著减小了模型的体积，更为重要的意义是优化了模型在运行时的访存，相较于单个指令的计算，访存耗时要远高于计算，因此可以显著加速模型推理。量化最核心的挑战在于使用更低精度的权重的同时保持模型精度尽可能少的降低。量化主要分为两大类，训练后量化（Post-Training Quantization，PTQ）和量化感知训练（Quantization-Aware Training，QAT）。训练后量化是将训练好的模型中的参数或激活值量化为低精度类型的数值类型，虽然使用简单，但是模型精度精度下降一般要高于量化感知训练。量化感知训练在训练过程中模拟量化过程，进而在更新参数时考虑量化产生的误差，虽然量化感知训练得到的量化模型精度下降较低，但是因为需要重新训练，所以开销较大，在实际使用中需要进行权衡使用。</p>
<p>虽然在卷积神经网络中可以相对简单的使用量化，但是将量化应用于Transformer存在一些挑战。Transformer激活值范围较大，很难使用低精度数据类型表示。传统的卷积神经网络会将异常的离群值截断，但是在Transformer中，这样的离群值有助于深层网络中形成特定的注意力模式，直接截断会改变网络的特性和精度，如果不截断会导致数值分辨率降低，而且注意力机制中存在一些难以量化的算子，进一步导致Transformer模型难以量化。PTQ4ViT提出了使用孪生均匀量化方法来解决激活值范围大的问题，同时为了获得最优的量化参数（而非局部最优），使用Hessian引导度量来评估不同的标定因子，从而以较小的成本提高校准准确率，最终达到了近乎无损的量化效果。针对部分算子难以量化的问题，FQ-ViT中使用Power-of-Two Factor（PTF）来量化LayerNorm，使用Log-Int-Softmax（LIS）来量化softmax，并使用4位量化和BitShift来进行简化，这也是第一个实现Transformer无损全量化的工作。</p>
<blockquote>
<p>[Understanding and Overcoming the Challenges of Efficient Transformer Quantization]</p>
<p>[PTQ4ViT: Post-Training Quantization Framework for Vision Transformers with Twin Uniform Quantization]</p>
<p>[FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer]</p>
</blockquote>
<h3 id="334-flashattention">3.3.4 FlashAttention</h3>
<p>FlashAttention是一种对标准注意力机制进行加速的算法，减少了对HBM（High Bandwidth Memory，通常用于GPU显存）的访问，而且它的训练和推理过程的结果和标准注意力机制完全相同，对用户透明，并且显著减小了标准注意力机制的运行时间和所需内存。</p>
<p>FlashAttention主要从两个方面减少注意力机制的HBM的访问。首先在计算softmax时，FlashAttention可以在不访问整个输入的情况下计算softmax reduction，将输入分割成块，在输入块上多次传递，从而以增量的方式计算softmax reduction。其次，在传统注意力机制中，需要将$QK^T$的计算结果$S$和$softmax(S)$后的计算结果$P$分别存储到显存中，FlashAttention对此做出改进，在反向传播中不存储中间注意力矩阵，避免从显存中读取和写入中间结果矩阵。通过分块写入到HBM中去，存储前向传递的 softmax 归一化因子，在后向传播中快速重新计算片上注意力，这比从HBM中读取中间注意力矩阵的标准方法更快。即使由于重新计算导致 FLOPS 增加，但因为减少了HBM访问，导致运行速度更快并且使用更少的显存（序列长度线性）。</p>
<p>此外，最新的研究SCFA进一步进行拓展，使得FlashAttention可以计算稀疏注意力，特别是针对Hash-based Attention和Query/Key-Dropping Based Attention，都得到了显著的推理加速。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/618533434">Blog: 论文分享：新型注意力算法FlashAttention</a></p>
<p>[Paper: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness]</p>
<p>[Paper: Faster Causal Attention Over Large Sequences Through Sparse Flash Attention]</p>
</blockquote>
<h1 id="4-discussion-on-future-research">4. Discussion on Future Research</h1>
<p>Transformer虽然有很强的建模能力，但是由于其中注意力机制具有序列长度平方的复杂度，限制了Transformer在很多场景中的使用。在未来的工作中，仍然可能会有很多工作对efficient attention、efficient transformer、模型压缩的不同方面进行改进。除此之外，本文观察到另外两个方向未来可能有进一步的发展。</p>
<h2 id="41-early-exit">4.1 Early Exit</h2>
<p>虽然当前很多研究关注于大模型在大量数据上的有效训练，但是经过训练的模型在实际使用中仍然速度较慢，特别是大模型作为基础设施时，越来越多的关注集中于提高模型的推理速度上。从模型来分析，很多大语言模型都是自回归模型，需要根据前面的单词递推的预测下一个单词，这个过程不能并行化，而且考虑到大模型庞大的参数量，整个推理过程需要大量的计算与较高的延迟。</p>
<p>在推理时，有些单词的预测比较轻松，可能在比较浅层的网络中就可以预测出正确的结果，不用计算到最后一层就可以正确预测，即提前退出（early exit），有的单词就需要较多的计算才能预测，但是很多模型在推理时针对这两种情况使用了相同的计算量。有一些工作已经初步在这方面进行了尝试，比如CALM，不是等待所有解码器层完成，而是尝试在某个中间层之后更早地预测下一个单词。 为了决定是进行某个预测还是将预测推迟到后面的层，测量模型对其中间预测的置信度。 只有当模型有足够的信心预测不会改变时，才会跳过其余的计算。</p>
<blockquote>
<p>[Paper: Confident Adaptive Language Modeling]</p>
<p><a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/120340325">Blog: 基于动态提前退出的预训练模型推理加速简介</a></p>
</blockquote>
<h2 id="42-alternatives-to-attention">4.2 Alternatives to Attention</h2>
<p>虽然注意力机制对于Transformer而言至关重要，但是由于其较高的复杂度，一些研究开始寻找注意力机制的替代而非单纯改进注意力机制。在AFT模型中，同样有类似于标准的点积注意力算法，同样由查询向量Q，被查向量K，内容向量V相互作用而成。但不同的是，AFT中的K和V首先与一组学习得到的位置偏差（position bias）结合，然后再进行同位元素对应相乘（element-wise multiplication）。这一新操作的内存复杂度、文本规模、特征维度都是线性的。当前一个较新的尝试是Hyena。Hyena将时域卷积和频域卷积作为一个组合，通过递归进行多次来增大表达能力，其全局卷积网络达到了超越Transformer建模的效果。</p>
<blockquote>
<p>[Paper: An Attention Free Transformer]</p>
<p>[Paper: Hyena Hierarchy: Towards Larger Convolutional Language Models]</p>
</blockquote>
<h1 id="5-conclusion">5. Conclusion</h1>
<p>在本综述中，从推理的角度出发，对efficient transformer进行了粗粒度的调研、分析与总结，并且相对侧重于计算机视觉方面的研究。首先介绍模型不同角度的efficiency和评价efficiency的量化指标。然后从模型算法的角度，从不同层次分析了当前提高模型效率的方法，比如设计复杂度更低的注意力机制，更加高效的网络设计，模型压缩和优化等方法，并针对每种方法进一步做了分类和总结，选取代表性的方法进行具体说明。最后，简单讨论了一些efficient transformer未来可能的发展方向，比如早退机制、注意力机制的替代品等。</p>
<h1 id="6-more-reading">6. More Reading</h1>
<p><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#sparse-attention-patterns">Large Transformer Model Inference Optimization</a></p>
<p><a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#low-rank-attention">The Transformer Family Version 2.0</a></p>
<p><a href="https://medium.com/data-science-at-microsoft/efficient-transformers-survey-of-recent-work-75022cddc86a">Efficient transformers: Survey of recent work</a></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/120292394">Bert/Transformer模型压缩与优化加速</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
