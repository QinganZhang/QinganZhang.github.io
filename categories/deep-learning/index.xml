<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>deep learning on Paul&#39;s Blog</title>
    <link>https://qinganzhang.github.io/categories/deep-learning/</link>
    <description>Recent content in deep learning on Paul&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 09 Sep 2024 23:57:36 +0800</lastBuildDate><atom:link href="https://qinganzhang.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>分布式训练并行技术梳理总结</title>
      <link>https://qinganzhang.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%A1%8C%E6%8A%80%E6%9C%AF%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/</link>
      <pubDate>Mon, 09 Sep 2024 23:57:36 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%A1%8C%E6%8A%80%E6%9C%AF%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/</guid>
      <description>首先声明一些符号： 模型参数量：$\Phi$ GPU数量：$N$（有时候可能误写成了$n$） Pipeline degree（或者说PP size）：$p$ TP siz</description>
      <content:encoded><![CDATA[<p>首先声明一些符号：</p>
<ul>
<li>模型参数量：$\Phi$</li>
<li>GPU数量：$N$（有时候可能误写成了$n$）</li>
<li>Pipeline degree（或者说PP size）：$p$</li>
<li>TP size：$t$</li>
</ul>
<p>除非特殊声明，某些未说明的符号可能遵循<a href="https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/#llama-3-the-llama-3-herd-of-modelshttpsarxivorgpdf240721783">LLM时代的transformer参数量、计算量、激活值的分析</a>的符号表示。</p>
<h1 id="数据并行">数据并行</h1>
<h2 id="传统数据并行dp">传统数据并行DP</h2>
<p>使用场景：每张卡上都有一份完整的模型（因此模型不会太大），通常单机多卡（比如单机八卡进行数据并行）</p>
<p>具体方法：基本使用参数服务器的编程框架，</p>
<ul>
<li>
<p>最基本的范式为：</p>
<ul>
<li>
<p>将一个batch分成若干份mini-batch，每个worker上进行一份mini-batch的计算（前向和反向），得到一份模型参数的梯度</p>
</li>
<li>
<p>AllReduce梯度：每个worker将梯度push到Server上，Server对梯度进行reduce（或者说求平均），再boardcast给每个worker</p>
<blockquote>
<p>Server即称为参数服务器，可以是GPU，也可以是CPU，也可以是多张GPU/CPU。如果是CPU，则通过PCIe通信，慢；如果是GPU，则通过Nvlink通信，更快；如果Server是多张GPU/CPU，则Server之间也要进行通信</p>
</blockquote>
</li>
<li>
<p>每个worker更新参数</p>
<blockquote>
<p>在第一步得到worker上自己的梯度之后，更新梯度有两种方式：
第一种方式就是上面的，AllReduce梯度后，每个worker得到平均后的梯度，然后每个worker再更新参数（下面的DDP也是这种方式）
第二种方式是，每个worker的梯度push到Server上，Server同样对梯度取平均，并且Server上也有一份模型参数，Server对这份模型参数进行更新，最后将更新后的参数boardcast给每个worker。这种方式明显对Server压力更大（原来只要allreduce梯度，现在还要自己有一份模型参数，还有更新参数），但是worker就省了更新参数。</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>另一种参数服务器的范式为：每个GPU都是参数服务器，比如说GPU1负责w1的AllReduce，GPU2负责w2的AllReduce，&hellip;</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li>
<p>每张卡上都要存储一份完整的模型，而且没法与其他并行方式组合使用</p>
</li>
<li>
<p>通讯开销大，因为push和boardcast传输的都是一份完整的参数的梯度</p>
<ul>
<li>每次迭代，每个Worker（共N-1个）push的通讯量为$\Phi$，Server总通讯量（boardcast）为$(N-1)\Phi$，所以更大的问题在于通讯负载不均，系统总通讯量为$(N-1)\Phi + (N-1)\Phi = 2(N-1)\Phi \approx 2N\Phi$（与DDP相同）</li>
</ul>
</li>
<li>
<p>Server进行allreduce的过程中，其他所有worker都在等待</p>
<ul>
<li>
<p>针对这一点，某些框架提出了一些异步的解决方法，但其实都是一个效果与性能的tradeoff，比如<a href="https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf">参数服务器</a>中3.4节提到了 Asynchronous Tasks and Dependency（其实我没太看懂），这篇博客<a href="https://zhuanlan.zhihu.com/p/617133971">图解大模型训练之：数据并行上篇(DP, DDP与ZeRO)</a>中对这一部分进行了说明，文中叫做“<strong>梯度异步更新</strong>”，比如在延迟为1的异步更新中，下面重新画一下示意图（这里是针对某一个worker进行示意）。其实感觉就是梯度累计的意思（这篇<a href="https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/">博客</a>中也简要提到了梯度累计，搜索&quot;gradient accumulation&quot;）</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-10-14:47:12.png" alt="梯度异步更新"  /></p>
</li>
</ul>
</li>
</ul>
<h2 id="分布式数据并行ddp">分布式数据并行DDP</h2>
<p>使用场景：每张卡上都有一份完整的模型，常用于多机多卡</p>
<p>具体方法：</p>
<ul>
<li>
<p>将一个batch分成若干份mini-batch，每个worker上进行一份mini-batch的计算（前向和反向），得到一份模型的梯度</p>
</li>
<li>
<p>使用Ring-AllReduce，使得每个Worker都得到一份完整的reduced梯度（聚合后的梯度）</p>
<blockquote>
<p>有几个关键词需要注意：一份完整的/一块不完整的，reduced/un-reduced，完整是指tensor没有切开，reduced是指多个tensor经过allreduce聚合</p>
</blockquote>
<ul>
<li>一开始每个Worker只有自己的一份完整的un-reduced梯度，而且将梯度分成N块</li>
<li>Reduce-Scatter：每次每个Worker向相邻的下一个Worker发送一块不完整的un-reduced梯度（大小是$\frac{\Phi}{N}$，进行一次reduce），一共N-1次，通讯量$(N-1)\times \frac{\Phi}{N}$，此时每个Worker都有一块不完整的reduced梯度</li>
<li>All-Gather：每次每个Worker向相邻的下一个Worker发送一块不完整的reduced梯度（大小是$\frac{\Phi}{N}$，直接进行替换），一共N-1次，通讯量$(N-1)\times \frac{\Phi}{N}$，此时每个Worker都有一份完整的reduced梯度</li>
<li>因此通信量为：$2\times(N-1)\times \frac{\Phi}{N} \approx 2\Phi$</li>
</ul>
</li>
</ul>
<p>效果：</p>
<ul>
<li>摆脱了参数服务器的编程框架，各个Worker地位相同，解决了参数服务器方式中通讯负载不均的问题</li>
<li>每次iteration，单卡总通讯量为$2\times(N-1)\times \frac{\Phi}{N} \approx 2\Phi$，系统总通讯量为$2\times(N-1)\times \frac{\Phi}{N} \times N \approx 2N\Phi$。DDP与DP通讯量相同，但是DDP通讯负载均衡</li>
</ul>
<p>实现细节：梯度分桶（Gradient Bucketing）</p>
<ul>
<li>
<p>原理：（不局限于大模型）</p>
<blockquote>
<p><a href="https://arxiv.org/pdf/2006.15704">论文</a>中的原话是：instead of launching a dedicated AllReduce immediately when each gradient tensor becomes available, DDP can achieve higher throughput and lower latency if it waits for a short period of time and buckets multiple gradients into one AllReduce operation. This would be especially helpful for models with many small parameters. However, DDP should not communicate all gradients in one single AllReduce, otherwise, no communication can start before the computation is over.</p>
</blockquote>
<ul>
<li>
<p>集合通信在小tensor上性能较差，在大tensor上性能较好，因此尽可能将模型的梯度lazy allreduce，将多个小梯度打包然后再allreduce。下图中表示对于一个60M的fp32 torc.Tensor，横轴是将该tensor切成不同大小的tensor进行allreduce，纵轴表示通信时间。</p>
  <img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20240910203004478.png" alt="image-20240910203004478" style="zoom:33%;" />
</li>
<li>
<p>对于一个大梯度，也不要只使用一个allreduce来通信。这主要考虑到计算和通信的overlap，将梯度的计算和梯度的allreduce通信进行重叠</p>
</li>
</ul>
</li>
<li>
<p>所以梯度分桶的做法将模型的Model的参数逆序插入每个Bucket中，当一个Bucket的参数的梯度都已经更新时，开启Allreduce，向另一个节点的对应Bucket传递梯度，这样也同时实现了异步AllReduce。更详细的过程可以阅读这篇博客：<a href="https://fazzie-key.cool/2022/01/23/ddp/">Pytorch Distributed Data Parallal</a></p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-10-20:55:10.png" alt="image-20240910205505079" style="zoom: 67%;" />
</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/493092647">第3篇 - 分布式训练常用的集合通信及其通信原语</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/617133971">图解大模型训练之：数据并行上篇(DP, DDP与ZeRO)</a></li>
<li><a href="https://pytorch.org/docs/stable/notes/ddp.html">PyTorch DDP desigin doc</a></li>
</ul>
<h2 id="zero内存优化">ZeRO内存优化</h2>
<p>背景：DP或DDP中，每个GPU中都有一份完整的模型，模型变大后，不仅仅是模型参数量更占显存了，同时训练过程中的优化器状态、梯度等也相应变大了，而显存是有限的，因此要尽可能节省显存。ZeRO就是对优化器状态、梯度这部分显存的优化。</p>
<p>核心思想：通信换空间。优化器状态、优化器中混合精度训练时使用的fp32参数 在前向、反向时不使用，只有在参数更新时才使用，即计算过程中内存出现了冗余。因此将这些内容放在放在不同的GPU上，使用时再经过通讯获取到完成的一份。</p>
<blockquote>
<p>关于模型在训练时，显存中到底存了哪些东西，可以参考<a href="https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/#gpu%E4%B8%8A%E9%83%BD%E5%AD%98%E4%BA%86%E5%93%AA%E4%BA%9B%E4%B8%9C%E8%A5%BF">GPU上都存了哪些东西</a>；混合精度训练，可以参考<a href="https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83">混合精度训练</a></p>
</blockquote>
<p>具体方法：ZeRO-DP、ZeRO-R，ZeRO-Offload是三种正交的显存优化方法</p>
<ul>
<li>
<p>ZeRO-DP：是针对model states的优化，ZeRO-1、ZeRO-2、ZeRO-3是三种不同程度的优化</p>
<ul>
<li>
<p>ZeRO-1（$P_{os}$）：将optimizer states和fp32参数切分，每次迭代中，</p>
<ul>
<li>
<p>将batch分成N个mini-batch，每个GPU输入一个mini-batch，每个GPU有一份完整的fp16的参数，经过前向反向，可以得到一份完整的un-reduced梯度</p>
</li>
<li>
<p>每个GPU上的完整的un-reduced梯度进行一次All-Reduce，每个GPU上都得到了一份完整的reduced梯度，单卡通信$2\times(N-1)\times \frac{\Phi}{N} \approx 2\Phi$</p>
</li>
<li>
<p>每个GPU上只有一块不完整的optimizer states，因此只能更新对应的一块不完整的参数</p>
</li>
<li>
<p>每个GPU上不完整的参数进行一次All-Gather，此时每个GPU上都得到了一次迭代后、更新后的完整的参数，单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</p>
<blockquote>
<p>但是上述方法不是最优的（单卡通信量为$3\Phi$），更优的方法（单卡通信量为$2\Phi$）为：</p>
<ul>
<li>将batch分成N个mini-batch，每个GPU输入一个mini-batch，每个GPU有一份完整的fp16的参数，因此可以得到一份完整的un-reduced梯度（同上）</li>
<li>每个GPU上的完整的un-reduced梯度进行一次Reduce-Scatter，每个GPU上都有一块不完整的reduced梯度，单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</li>
<li>每个GPU上有且只有对应的一块不完整的optimizer states，因此正好更新一块不完整的参数，得到updated参数</li>
<li>每个GPU上不完整的updated参数进行一次All-Gather，此时每个GPU上都得到了一次迭代后、更新后的完整的参数，单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li>
<p>ZeRO-2（$P_{os}+P_g$）：将optimizer state和fp32参数、梯度切分，（过程同上面那种更优的方法）</p>
<ul>
<li>将batch分成N个mini-batch，每个GPU输入一个mini-batch，每个GPU有一份完整的fp16的参数，因此可以得到一份完整的un-reduced梯度</li>
<li>每个GPU上的完整的un-reduced梯度进行一次Reduce-Scatter，每个GPU上都有一块不完整的reduced梯度，单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</li>
<li>每个GPU上有且只有对应的一块不完整的optimizer states，因此正好更新一块不完整的参数，得到updated参数</li>
<li>每个GPU上不完整的updated参数进行一次All-Gather，此时每个GPU上都得到了一次迭代后、更新后的完整的参数，单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</li>
</ul>
</li>
<li>
<p>ZeRO-3（$P_{os} + P_g + P_p$）：将optimizer state和fp32参数、梯度、fp16参数切分</p>
<ul>
<li>将batch分成N个mini-batch，每个GPU输入一个mini-batch，此时每个GPU只有一份不完整的fp16参数
<ul>
<li>forward过程中，对不完整的fp16参数进行一次All-Gather，每个GPU得到了一份完整的fp16参数，可以进行forward。forward完成后，丢弃掉刚才All-Gather得到的fp16参数。单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</li>
<li>backward过程中，再次同样对不完整的fp16参数进行一次All-Gather，每个GPU得到了一份完整的fp16参数，可以进行backward，得到一份完整的un-reduced梯度，单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</li>
</ul>
</li>
<li>每个GPU上的完整的un-reduced梯度进行一次Reduce-Scatter，每个GPU上都有一块不完整的reduced梯度，并且丢弃掉剩余的梯度，单卡通信$(N-1)\times \frac{\Phi}{N} \approx \Phi$</li>
<li>此时每个GPU上有一块不完整的optmizer states和一块不完整的reduced梯度，正好更新对应的一块不完整的参数，得到一块不完整的updated参数，而且此时不需要对不完整的update参数进行All-Gather操作（因为本来就是被切开的）</li>
</ul>
</li>
</ul>
</li>
<li>
<p>ZeRO-R：针对residual state的优化</p>
<ul>
<li>$P_a$：针对中间激活值，激活值只起到一个加速计算梯度的作用，这里将激活值也进行切分，每个GPU上保存一份，在反向计算梯度需要完整的激活值时再经过通讯获取完整的激活值</li>
<li>$C_B$：针对一些操作或算子需要开辟buffer或者临时数组，这里预先开辟较大的内存buffer，并在后续保持固定</li>
<li>$M_D$：针对频繁内存申请和释放可能导致的内存碎片，这里将内存大致分为两个部分，long lived参数放在一个部分，将另一些经常构造析构的中间激活值等放在另一个部分</li>
</ul>
</li>
<li>
<p>ZeRO-Offload：</p>
<ul>
<li>forward和backward计算量大，因此将fp16的参数、激活值放在显存中</li>
<li>更新参数的计算量小，因此将optimizer state（和fp32参数）、梯度放在内存中</li>
</ul>
</li>
</ul>
<p>效果：</p>
<ul>
<li>通信方面：ZeRO-1和ZeRO-2相较于DP或者DDP没有增加通信量，ZeRO-3仅仅增加了0.5倍的通信量。</li>
<li>显存方面：极大降低了（单卡）显存占用。图中K=12，是对应fp32 param（4）+fp32 momentum（4）+fp32 variance（4）；前面两个2分别是fp16 param（2）和fp16 grad（2）</li>
</ul>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-21:43:13.png" alt="image-20240910161304348"  /></p>
<p>其他说明：</p>
<ol>
<li>为什么ZeRO是数据并行？明明把优化器状态、梯度甚至参数切开了
<ul>
<li>模型并行是只使用自己那部分参数进行计算，将中间结果（激活值）进行通讯（比如TP）</li>
<li>ZeRO-DP将优化器状态、参数、梯度等切分放到多个GPU上，ZeRO-R将输入和中间激活值切分开放到多个GPU上，在实际运算时，每张卡上输入的mini-batch不相同，首先需要经过通讯，拿来完整的输入和完整的参数进行前向计算，拿来完整的中间激活和完整的权重反向传播计算梯度，因此ZeRO只是内存优化后的数据并行</li>
</ul>
</li>
</ol>
<p>参考：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/618865052">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/663517415">ZeRO: Zero Redundancy Optimizer，一篇就够了</a></li>
<li><a href="https://zerolovesea.github.io/2024/05/12/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%9A%E4%BA%86%E8%A7%A3Deepspeed%E4%B8%AD%E7%9A%84ZeRO1-2-3/">分布式训练：了解Deepspeed中的ZeRO1/2/3</a></li>
</ul>
<h2 id="fsdpfullyshardeddataparallel">FSDP(FullyShardedDataParallel)</h2>
<p>FSDP是DeepSpeed ZeRO-DP的进一步发展，主要实现了与PyTorch的co-design，改进主要体现在计算和通信的粒度，FSDP中前反向的计算都是以FSDP unit为粒度执行的。</p>
<p>(need further understanding)</p>
<p>推荐好文：</p>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/694288870">PyTorch FSDP 设计解读</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/649837295">FSDP 完全分片数据并行</a></p>
</li>
</ul>
<h1 id="流水线并行">流水线并行</h1>
<p>流水线并行大致可以这样进行分类：</p>
<ul>
<li>同步流水线
<ul>
<li>F-then-B：GPipe</li>
<li>1F1B
<ul>
<li>non-itlv：PipeDream-flush</li>
<li>itlv：
<ul>
<li>parallel-shape：Megatron-2</li>
<li>v-shape：Zero-Bubble-V</li>
</ul>
</li>
</ul>
</li>
<li>1F2B：Zero-Bubble</li>
</ul>
</li>
<li>异步流水线：
<ul>
<li>PipeDream</li>
<li>PipeDream-2BW</li>
</ul>
</li>
</ul>
<p>使用流水线并行有两点需要注意：</p>
<ol>
<li>
<p>流水线并行的通信量不大，因为只涉及到不同stage输入的通信（后面一维张量并行中会比较PP、DP、TP的通信量）。这里符号沿用<a href="https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/">LLM时代的transformer参数量、计算量、激活值的分析</a>，并设置一些新的符号表示：设micro-batch数量为m，每个micro-batch的大小为$b_m$（因此有$m \times b_m = b$，将模型划分为$N_s+1$个stage（或者叫做cell，chunk，在interleave场景下是virutal stage的数量），流水并行度即文章开头提到的$p$。（下面以transformer为例进行分析）</p>
<ul>
<li>
<p>一个micro-batch在一个stage的通信量是$w \times 2\times b_m sh$（2表示FWD过程中要将输出激活值通信+BWD过程中要将对输入激活值的梯度进行通信），m个micro-batch在切分成$N_s+1$个stage时，总的需要通信的数据大小为$m \times 2wb_msh \times N_s=2wbshN_s$，每个Device的通信量为$\frac{2wbshN_s}{p}$</p>
</li>
<li>
<p>我们来举几个例子，来量化感受一下（假设使用缓和精度训练，w=2，1F1B，non-itlv）</p>
<table>
<thead>
<tr>
<th>model</th>
<th>b</th>
<th>s</th>
<th>h</th>
<th>单层单次通信量$wbsh$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 175B</td>
<td>1</td>
<td>2048</td>
<td>12288</td>
<td>96MB</td>
</tr>
<tr>
<td>Llama-2 70B</td>
<td>1</td>
<td>2048</td>
<td>8192</td>
<td>128MB</td>
</tr>
<tr>
<td>Llama-3 405B</td>
<td>1</td>
<td>4096</td>
<td>12288</td>
<td>192MB</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>
<p>PP不推荐ZeRO-2/ZeRO-3同时使用，原因在于增加了很多通信，但是缺节省不了多少下显存。</p>
<p>下图中是PP+ZeRO-2的示意图，dp_size = 2，每个DP内部又划分为p = 4的PP。每个DP吃掉的mini-batch都是不同的，PP又将每个DP吃掉的mini-batch再次切分为micro-batch，即图中DP1对应micro-batch1~4，DP2对应micro-batch5~8。注意，由于使用ZeRO-2（或ZeRO-3），会对反向得到的梯度进行切分，DP1反向得到完整的un-reduced梯度，DP2反向得到完整的un-reduced梯度，然后进行Reduce-Scatter通信（比如GPU4上mbs1的梯度要与GPU8上mbs5的梯度进行Reduce-Scatter通信），DP1得到不完整的reduced梯度（即前半部分参数对应的梯度），DP2得到不完整的reduced梯度（即后半部分参数对应的梯度）。除此之外，由于PP使用梯度累计，所以最后UPD先进行梯度累积，然后使用不完整的redued梯度更新完权重后，还要进行All-Gather通信拿到完整的权重（因为ZeRO-2没有切权重）。如果是PP+ZeRO-3，则要在每个mbs的前向过程之前进行一次All-Gather，在mbs的反向过程之前进行一次All-Gather，反向过程之后进行一次Reduce-Scatter，最后的All-Gather可以省去。</p>
 <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:52:50.png" alt="ZeRO2+PP" style="zoom:80%;" />
<p>针对大模型而言，PP一般是要使用的，此时再使用ZeRO就是想减少单卡的内存占用。一方面，我们来看一下PP+ZeRO-2对减少内存占用的一个效果，PP切分模型后，每个stage上的参数量基本上是整个模型参数量的$\frac{1}{N_s+1}$，ZeRO-2又切分了梯度，相比于ZeRO-1内存占用减少了$\frac{1}{N_s+1}(1-\frac{1}{N})w\Phi$（如果进一步使用TP并行进行切分，则会进一步减小内存占用，这个内存减少的量也会进一步减小），因此，使用PP后再使用ZeRO-2可能节省不了多少显存。另一方面，从通信量来分析，由于ZeRO-2的梯度切分，反向得到梯度后要在DP group间通信，PP将mini-batch切分为micro-batch，又使得梯度Reduce-Scatter变得很频繁（相当于DP group变多），每一次Reduce-Scatter通信量都是两倍的对应梯度大小再乘以数值精度，通信量直接暴涨（虽然有可能做到计算和通信overlap，我也不太确定这一点，但是毕竟性能肯定还是有损失的）。PP+ZeRO-3也是同理，显存占用可能再少一点，但是带来的通信量增加是很大的。因此，在PP基础上使用ZeRO-2（或者ZeRO-3）节省不了多少显存，还会增加很多通信量，对性能的提升可能很有限甚至负提升。可以参考知乎问题：<a href="https://www.zhihu.com/question/652836990">大模型训练时ZeRO-2、ZeRO-3能否和Pipeline并行相结合？</a></p>
<p>但是可以PP+ZeRO-1（比如下图），由于梯度（和权重）都是完整的，先进行完整的un-reduced梯度的梯度累积，然后进行Reduce-Scatter通信，得到不完整的reduced梯度，ZeRO-1只会切分optimizer_states（和混合精度中使用到的fp32权重），正好更新那一部分权重，最后使用All-Gather通信拿到全部的权重。</p>
 <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:52:46.png" alt="ZeRO1+PP" style="zoom:80%;" />
</li>
</ol>
<p>参考：</p>
<ul>
<li><a href="https://juejin.cn/post/7262274383287484476#heading-15">大模型分布式训练并行技术（三）-流水线并行</a></li>
<li>好文推荐：<a href="https://zhuanlan.zhihu.com/p/707784244">大模型效率工程（五）：一文读懂流水线并行训练升级之路 —— From Naive to V-shape Zero Bubble</a></li>
</ul>
<h2 id="naive流水线并行f-then-b">naive流水线并行：F-then-B</h2>
<p>如果模型太大（参数量太大），单卡装不下时，将模型横着切，每个部分（可能是多层，称为一个chunk）放在一个GPU上：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-21:43:10.png" alt="image-20240910213333987" style="zoom:67%;" />
<p>模型可以训练起来，但是一个缺点，就是GPU利用率不高，bubble很多，某一时刻只有一个GPU在进行计算。</p>
<h2 id="gpipemicro-batch">GPipe：micro-batch</h2>
<p><a href="https://arxiv.org/abs/1811.06965">GPipe</a>主要是针对naive流水线并行的改进，主要是提升GPU利用率，减少bubble占比。</p>
<p>具体方法是，同样将模型横切，划分为多个cell（每个cell包含若干个连续的层，原来叫做chunk，后面PipeDream又叫做stage，一个意思），每个cell放在一块GPU上。将输入mini-batch再划分为micro-batch，将多个micro-batch依次送入到模型中，形成流水线，这种策略也叫F-then-B。简单来说，模型切法没变，batch切开。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-21:43:07.png" alt="image-20240910213853238" style="zoom:67%;" />
<p>第一个下标表示GPU编号（或者cell编号），第二个下标表示micro-batch编号，横轴表示时间，纵轴表示不同GPU。比如$F_{0,1}$
表示第1个micro-batch作为cell0（对应GPU0）的输入。假设划分为M个micro-batch，则bubble占比为$\frac{N-1}{M+N-1}$。</p>
<p>有一个细节需要注意，将mini-batch切成micro-batch，会对Batch norm的计算有影响，因为原来BN是对整个batch的相同channel进行norm，现在整个batch被拆开成好几个micro-batch，每个micro-batch都能单独计算一个BN，但是同时还维护一个全局的、针对所有micro-batch的移动平均和方差，以供最后推理阶段使用。但是LLM或者NLP中使用BN好像不多，且micro-batch对LN没有影响（本来LN就是对每个token内部算均值方差）。</p>
<p>GPipe论文中还提出了一个优化显存的方法：重计算（re-materialization，或者re-compute，也叫做active checkpoint，activation checkpoint），重计算不属于流水线并行（也不属于各种并行），这里简单介绍一下。在反向传播计算梯度的时候，需要上一层传来的梯度、该层的参数，还可能需要该层的输入激活值，如果在前向过程中这个激活值没有保留的话，就需要从之前的某个起点开始重新走一遍前向过程得到这个激活值，流水线并行恰巧比较方便确定这个起点，就是每个cell的输入。重计算降低的是峰值显存。</p>
<p>GPipe有两个缺点：</p>
<ul>
<li>
<p>只有将模型划分得比较均匀时，流水线并行才能得到较为理想的效果；但是有的模型不太好均匀划分</p>
</li>
<li>
<p>每个GPU，需要缓存每个micro-batch前向过程中的输入和所有的中间激活，一个解决方法是开重计算，开重计算后，每个GPU只需要缓存每个cell的输入即可，属于是时间换空间了。</p>
<ul>
<li>为了降低空泡比，就要增大micro-batch的数量，假设原来mini-batch大小不变，中间激活大小也没变，但是每个micro-batch大小就变小了，太小会导致计算效率变低。可以参考<a href="https://zhuanlan.zhihu.com/p/707784244">大模型效率工程（五）：一文读懂流水线并行训练升级之路 —— From Naive to V-shape Zero Bubble</a>中3.2节</li>
</ul>
</li>
<li>
<p>但是也会增加峰值显存。</p>
</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/613196255">图解大模型训练之：流水线并行（Pipeline Parallelism），以Gpipe为例</a></li>
</ul>
<h2 id="pipedream1f1b">PipeDream：1F1B</h2>
<p>PipeDream是一种异步流水线的方案，同时维护多个版本的权重，每次反向传播都会更新权重，<a href="https://arxiv.org/abs/1806.03377">PipeDream论文</a>中提到的weight Stashing和vertical Sync（论文中说vertical Sync可以忽略），个人理解不是很透彻（水平方向上可以使用weight stash保证每个micro-batch更新的是对应版本的权重，但是竖直方向上Device4每次都更新权重，Device1上将梯度更新平均到了4个版本的weight上，最后训练出来，权重到底怎么算？），这里先暂且略过，待后续更新。</p>
<blockquote>
<p><a href="https://arxiv.org/abs/1806.03377">PipeDream论文</a>中提出了1F1B流水线并行策略，但它是异步的；<a href="https://arxiv.org/abs/2006.09503">Memory-Efficient Pipeline-Parallel DNN Training</a>中提出了PipeDream-Flush（这是同步版本的1F1B流水线）和PipeDream-2BW（这也是异步版本的1F1B流水线，但它减少了对weight stash的维护）</p>
</blockquote>
<p>这里主要来描述一下同步流水线1F1B流水线并行策略，一个micro-batch的前向计算可以和另一个micro-batch的反向计算交叉进行，从而可以及时释放不必要的中间激活。因为是同步的，所以此时没有了weight stash。图中标号表示不同的micro-batch；蓝色块表示前向过程，每个同步阶段、每个stage都是基于相同的参数、吃掉不同的micro-batch；绿色块表示反向过程，每个stage基于相同的参数、使用与对应前向过程的中间激活（比如GPU0上吃掉micro-batch0，前向和反向中间有间隔，需要保证对应），得到梯度（实际上应该是累计的梯度）；红色块表示使用梯度更新参数。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-21:45:35.png" alt="1F1B"  /></p>
<p>第一行三个图实际上是相同的，区别只是在于一开始前面几个micro-batch输入顺序有稍许不同（只要能保证上下游的依赖关系，可以随便调整）。这幅图除了示意说明1F1B，还说明了1F1B与F-then-B的耗时是相同的，bubble也是相同的。1F1B相较于F-then-B的优点，在于减少了中间激活，节省显存。GPipe前向过程中需要将每个micro-batch计算的中间激活值保留下来，而在1F1B中，分别需要保存4、3、2、1份中间激活，stage0中需要保存micro-batch 1~4计算的中间激活值，stage1中需要保存micro-batch 1~3计算的中间激活值，stage2中需要保存micro-batch 1~2计算的中间激活值，stage3中只需要保存一份micro-batch计算的中间激活值，因为随后的对应的反向计算中会使用该中间激活，使用完可以丢弃（或者由于LLM的规整性，可以复用），以后不会用到这些中间激活了。节省显存之后，就可以使用更多的micro-batch数量，从而达到减少bubble的目的。</p>
<p>有一点需要注意，GPipe中，增大micro-batch数量不会降低显存占用，但是在1F1B中是可以的，更准确来说是，增大micro-batch数量，Device1的峰值显存在1F1B中会更少，相较于GPipe。比如原来micro-batch数量是4，现在micro-batch增大到8，那么GPipe中还是F-then-B的方法，但是1F1B中可以调整流水线编排：（上面GPipe的图中省略了micro-batch的标号，下面1F1B的编排中Device1最多只有4份中间激活）</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-12-00:24:15.png" alt="image-20240912002410045"  /></p>
<p>针对PipeDream和GPipe，而且也属于是1F1B的一个改进是<a href="https://arxiv.org/abs/2006.09503">PipeDream-2BW</a>，可以看作是PipeDream和GPipe的折中和结合，PipeDream的异步+Gpipe的梯度累计更新参数+double-buffer。具体而言，就是流水线总体上是异步的没有flush（类似PipeDream），但是现在只维护了两个weight buffer；每输入m个micro-batch使用相同的权重，然后进行一次梯度累计和梯度更新（类似于GPipe，不像PipeDream是每输入一个micro-batch都要梯度更新）；对权重使用double-buffer，每隔m个micro-batch，更新旧的weight，后面接着的m个micro-batch切换weight。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-21:42:59.png" alt="image-20240911193934004"  /></p>
<h2 id="megatron-2httpsarxivorgabs210404473interleaveitlv"><a href="https://arxiv.org/abs/2104.04473">Megatron-2</a>：interleave(itlv)</h2>
<p>将模型分为多个chuck（若干个连续的层构成一个chuck，或者叫做virtual stage），每张卡上放2个以上的chuck，比如下面图示中，比如Device1上有两个chunk（0、1层组成一个chunk，8、9层组成一个chunk），micro-batch1分别经过device1的chunk1（0，1层）、device2的chunk1（2，3层）、device3的chunk1（4，5层）、device4的chunk1（6，7层）后，再返回，经过device1的chunk2（8，9层）、device2的chunk2（10，11层）、device3的chunk2（12，13层）、device4的chunk2（14，15层）。深色表示该Device上的第一个chunk，浅色表示第二个chunk。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-21:45:28.png" alt="image-20240911201453598"  /></p>
<p>该方案要求micro-batch数量是Device数量的整数倍，因此才能最大程度的减少bubble。假设原来一个stage切成了v个virtual stage（或者chunk），一个mini-batch切成了m个micro-batch，气泡占比从原来的$\frac{(p-1)(Time_{FWD}+Time_{BWD})}{m(Time_{FWD}+Time_{BWD})}=\frac{p-1}{m}$降低到$\frac{(p-1)(Time_{FWD} / v+Time_{BWD} / v)}{m (Time_{FWD}+Time_{BWD})}=\frac{1}{v} \times \frac{p-1}{m}$，但是代价是PP之间的通信也变为原来的v倍。</p>
<h2 id="zero-bubble1f2b">Zero-Bubble：1F2B</h2>
<p>原来流水线基本都是FWD和BWD相重叠，<a href="https://openreview.net/pdf?id=tuzTN0eIO5">Zeor-Bubble</a>中将BWD的过程进一步细化，进行拆分：</p>
<ul>
<li>
<p>反向过程中要计算对权重的梯度，这个过程记为W</p>
</li>
<li>
<p>反向过程中要计算对输入的梯度，这个过程记为B</p>
<blockquote>
<p>一般来说，参数梯度计算耗时W &lt; 前向过程计算耗时F &lt; 激活值梯度计算耗时B</p>
</blockquote>
</li>
</ul>
<p>原来B过程和W过程都融合在一起，只有二者都完成，才能进行下一个stage的反向。但是中间存在一个不必要的依赖：第i-1层的B过程隐式的依赖于第i层的W过程，或者说，原来只有$B_i$和$W_i$都完成了，才能进行$B_{i-1}$和$W_{i-1}$的过程，而实际上，只要$B_i$完成了，就能进行$B_{i-1}$和$W_{i-1}$的过程。甚至最极端的情况是，先完成所有的B过程，然后再进行W过程，来更新参数。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-21:42:53.png" alt="295FAF21598E98D3380B10AF5C5B2BA8" style="zoom: 20%;" />
<p>来自相同micro-batch的F过程和B过程仍然要保持先F后B的dependency，W过程只要保证在上一个micro-batch的B过程之后就可以，比较灵活，因此可以进行手工调整以减少气泡，这是两个手工设计的流水线方案：</p>
<ul>
<li>ZB-H1：相较于1F1B，ZB-H1气泡更少，峰值显存相同，主要是因为B过程提前，而且尾部的气泡可以被W填充
<ul>
<li>B过程和W过程计算时需要的激活值的显存分别记为$M_B=bs(34h+5as)$和$M_W=32bsh$，第i个Device：
<ul>
<li>ZB-H1占用显存$(p-i+1)M_B+(i-1)M_W$，其他Device的显存占用不会比Device1显存少太多</li>
<li>1F1B占用显存$(p-i+1)M_B$，其他Device的显存占用明显少于Device1</li>
</ul>
</li>
</ul>
</li>
<li>ZB-H2：ZB-H2的气泡更少，但是峰值显存会变多
<ul>
<li>第i个Device，ZB-H2占用显存$(2p-2i+1)M_B+2(i-1)M_W$，相比于ZB-H1显存几乎翻倍</li>
<li>论文中附录F中说到，用显存换ZB-H2是值得的，在相同显存的情况下，ZB-H2的micro-batch size可以是1F1B的一半，由于大模型训练中几乎不存在设备利用率不饱和的情况，所以减小micro-batch size，以换取更少的bubble，这么看是值得的，况且显存可以使用ZeRO等方式进行优化。</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-11-22:38:31.png" alt="image-20240911215653985" style="zoom: 67%;" />
<p>参考：</p>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/681363624">AI Infra论文阅读之将流水线并行气泡几乎降到零（附基于Meagtron-LM的ZB-H1开源代码实现解读）</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/685838198">模型并行训练：零气泡流水线并行 ZERO BUBBLE PIPELINE PARALLELISM</a></p>
</li>
</ul>
<h2 id="v-shape">V-shape</h2>
<p>首先说明，V-shape是在1F1B itlv的情况下的一个流水线模式。1F1B itlv（vpp）是左边的模式，第二次通过流水线的时候是顺序的，比如3个Device，模型6层（分成6个chunk，每层两个chunk），Device1上放1、4层，Device2上放2、5层，Device3上放3，6层。V-shape中，第二次通过流水线是倒序的，Device1上放1、6层，Device2上放2、5层，Device3上放3，4层。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-12-10:46:31.png" alt="image-20240912004140261"  /></p>
<p><a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2401.10241">Zero Bubble Pipeline Parallelism</a> （Arxiv版本）在第6节介绍了 V-shape Zero Bubble Pipeline Parallelism，名为ZB-V，<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2405.15362">Pipeline Parallelism with Controllable Memory</a> 对 V-shape 进行了系统性介绍，称 V-shape Zero Bubble PP 为 V-ZB。V-shape最主要的作用就是减少峰值显存，峰值显存降低到与1F1B相同（V-shape和Parallel-shape的通信还是相同的），下面是对比：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-12-18:17:10.png" alt="v-shape" style="zoom:50%;" />
<h2 id="hanayo">Hanayo</h2>
<p><a href="https://zhuanlan.zhihu.com/p/694110614">【分布式训练技术分享九】聊聊高效流水并行Hanayo: Harnessing Wave-like Pipeline Parallelism</a></p>
<h1 id="张量并行">张量并行</h1>
<p>流水线并行是将模型按层横着切分，张量并行是将模型按列切分，每张卡上放一块不完整的权重，以减少内存占用。</p>
<p>首先来看如何对$Y=XW$的矩阵乘法进行TP切分，这是模型张量并行的基础。</p>
<ul>
<li>
<p>RowParallel：对W按行切分，此时X要按列切分，比如切成两份，X1、W1、Y1放在GPU1上，X2、W2、Y2放在GPU2上，Y在每个GPU上都有一份</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-12-18:16:24.jpg" alt="v2-c7e117053154a79a281d4e9c313f0898_r (1)" style="zoom: 20%;" />
<p>深度学习中，不仅要有Y=XW的前向计算，还要进行反向计算，求得对权重W和对输入X的梯度</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-12-18:16:28.jpg" alt="v2-f23a9da7560340314fe42aff3a90709f_r" style="zoom: 25%;" />
<ul>
<li>f：
<ul>
<li>forward：split（或者说only keep one part），原来X在每张卡上都有一份，现在GPU1上keep X1，GPU2上keep X2</li>
<li>backward：all-gather，将GPU1上的X1的梯度和GPU2上X2的梯度进行all-gather通信，每张卡上都能拿到完整的X的梯度</li>
</ul>
</li>
<li>g：
<ul>
<li>forward：all-reduce，将GPU1上的Y1和GPU2上的Y2进行all-reduce通信，每张卡上都有reduced的Y1+Y2</li>
<li>backward：identity，或者说将Y的梯度broadcast给GPU1和GPU2</li>
</ul>
</li>
</ul>
</li>
<li>
<p>ColumParallel：对W按列切分，此时X要按行切分，比如切成两份，W1、Y1在GPU1上，W2、Y2在GPU2上，X、Y在每张卡上都有一份</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-12-18:16:32.jpg" alt="v2-b97f58d7420ec2cce8561158fe55681c_r" style="zoom: 20%;" />
<p>同样还要可以计算对权重W和对输入X的梯度</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-12-18:16:36.jpg" alt="v2-4540e7bcea664717f17e0ad217f4fc23_r" style="zoom: 25%;" />
<ul>
<li>f：
<ul>
<li>forward：identity，或者说将X复制到每张卡上完整一份</li>
<li>backward：all-reduce，将GPU1上的X的梯度和GPU2上的X的梯度进行all-reduce通信，每张卡上都有reduced的X梯度</li>
</ul>
</li>
<li>g：
<ul>
<li>forward：all-gather，将GPU1上的Y1和GPU2上的Y2进行all-gather通信，每张卡上都有一份完整的Y</li>
<li>backward：split（或者说only keep one part），原来Y的梯度在每张卡上都有一份，现在GPU1上keep Y1的梯度，GPU2上keep Y2的梯度</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="一维张量并行megatron-lmhttpsarxivorgabs190908053">一维张量并行：<a href="https://arxiv.org/abs/1909.08053">Megatron-LM</a></h2>
<h3 id="具体过程">具体过程</h3>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-13-01:28:43.png" alt="TP"  /></p>
<p>总的通信量为：$w(vh+4bsh \times l + 2bs + N)$</p>
<h3 id="简要分析">简要分析</h3>
<ol>
<li>
<p>DP、PP、TP通信量比较：（以整个transformer模型为例，假设w=2）</p>
<ul>
<li>
<p>DP：输入batch，经过前向反向后得到参数的梯度，然后对梯度进行all-reduce，transformer 的参数量为$2Vh + (4h+4+3I)hl$，因此DP的通信量为：$2w\Phi = 4wVh + 2w(4h+4+3I)hl$，该通信量只与模型相关</p>
</li>
<li>
<p>PP：m个micro-batch（即一个mini-batch）在一个stage（一个stage可能包含多个transformer block）的通信量是$m \times 2 \times w(b_msh)=2wbsh$（non-itlv），如果整个模型切成了$N_s+1$个stage，那么通信量为$2wbshN_s$，如果是itlv（假设原来一个chunk切分为2个virtual stage）则通信量翻倍，可以看出该通信量与输入token数量、模型的隐藏层维度、模型切分的stage数量相关</p>
</li>
<li>
<p>TP：$w(vh+4bsh \times l + 2bs + N) \approx w(vh + 4bshl) \approx 2wbsh \times 2l$，可以看出该通信量与输入token数量、模型大小相关，而且$2l$一般大于$N_s$，所以现在就可以分析出通信量TP&gt;PP</p>
</li>
<li>
<p>然后进行一个case study，来直观感受一下。因为我们在训练时总想在显存允许的范围内增大batch size、增大序列长度，一般来说TP通信量还是要比DP通信量多的（可能会有例外），但是可能没有数量级上的差距。但是TP需要频繁的进行通信，所以TP更适合在单机多卡间并行。</p>
<table>
<thead>
<tr>
<th>model</th>
<th>h</th>
<th>I</th>
<th>b</th>
<th>s</th>
<th>$N_s$</th>
<th>DP通信量$\approx 4\Phi$</th>
<th>PP通信量$=4bshN_s$</th>
<th>TP通信量$\approx 8bshl$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>8</td>
<td>2048</td>
<td>16</td>
<td>651.92GB</td>
<td>12GB</td>
<td>144GB</td>
</tr>
<tr>
<td>Llama-2 7B(6.7B)</td>
<td>4096</td>
<td>32</td>
<td>8</td>
<td>4096</td>
<td>4</td>
<td>24.96GB</td>
<td>1GB</td>
<td>32GB</td>
</tr>
<tr>
<td>Llama-2 13B</td>
<td>5120</td>
<td>40</td>
<td>8</td>
<td>4096</td>
<td>8</td>
<td>48.43GB</td>
<td>2.5GB</td>
<td>50GB</td>
</tr>
<tr>
<td>Llama-2 70B</td>
<td>8192</td>
<td>80</td>
<td>8</td>
<td>4096</td>
<td>8</td>
<td>260GB</td>
<td>8GB</td>
<td>160GB</td>
</tr>
<tr>
<td>Llama-3 405B</td>
<td>16384</td>
<td>126</td>
<td>8</td>
<td>4096</td>
<td>16</td>
<td>391.15GB</td>
<td>32GB</td>
<td>504GB</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>
<p>辨析一下TP和ZeRO-3：<a href="https://github.com/microsoft/DeepSpeed/discussions/1911">Github issus</a></p>
<ul>
<li>相同点：都是将权重（和梯度）竖着切开，放在不同卡上</li>
<li>不同点：
<ul>
<li>输入方面：TP可以使用不完整的输入进行计算，ZeRO-3必须使用完整的输入进行计算</li>
<li>通信方面：TP通信是为了对激活值进行reduce，ZeRO-3通信是为了拿到完整的权重和梯度</li>
<li>总结就是，TP使用本地不完整的输入、本地不完整的权重进行计算，通信是为了对结果进行all-reduce；而ZeRO-3使用本地完整的输入、全局完整的权重（由通信拿来的）进行计算，通信是为了拿来全局完整的权重</li>
</ul>
</li>
<li>TP与ZeR-3是兼容的，但是TP+ZeRO-3（和TP+ZeRO-2）实际上就退化成了TP+ZeRO-1（TP已经切分了权重和梯度，ZeRO只是切optimizer_states和fp32权重）</li>
</ul>
</li>
<li>
<p>效果</p>
<table>
<thead>
<tr>
<th></th>
<th>计算量</th>
<th>参数量</th>
<th>激活值</th>
<th>通信带宽</th>
<th>通信时延</th>
</tr>
</thead>
<tbody>
<tr>
<td>without TP</td>
<td>$(24h+4s)bshl+2bshV$</td>
<td>$2Vh+(12h^2+13h)l$</td>
<td>$2bsh+[34bsh+5bas^2]l$</td>
<td>$O(1)$</td>
<td>$O(1)$</td>
</tr>
<tr>
<td>with TP</td>
<td>$\frac{(24h+4s)bshl}{t}+2bshV$</td>
<td>$\frac{2Vh}{t}+(4h+\frac{12h^2+9h}{t})l$</td>
<td>$2bsh+[10bsh+\frac{24bsh+5bas^2}{t}]l$</td>
<td>$O(2(t-1)t)$</td>
<td>$O(2(t-1))$</td>
</tr>
</tbody>
</table>
</li>
</ol>
<p>参考和推荐好文：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/622212228">图解大模型训练之：张量模型并行(TP)，Megatron-LM</a></li>
</ul>
<h2 id="多维张量并行colossal-ai">多维张量并行：Colossal-AI</h2>
<p>个人对多维张量并行了解不深入，也没有使用过，看一些解读，感觉就像是对矩阵乘法各种拆分和在多处理器上并行，这里简单进行一下记录。</p>
<p>参考：<a href="https://juejin.cn/post/7269698032655728640#heading-8">大模型分布式训练并行技术（四）-张量并行</a></p>
<h3 id="2d-tensor-parallelhttpsarxivorgabs210405343"><a href="https://arxiv.org/abs/2104.05343">2D Tensor Parallel</a></h3>
<p>背景：Megatron-LM的张量并行中，每个GPU上都要保留完整的激活，激活值没有被切分，如下图（绿色表示激活，蓝色表示权重）：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-13-01:28:36.png" alt="image-20240913000333504" style="zoom: 33%;" />
<p>解决方法：将激活和权重划分成二维网格，一个限制是输入激活和权重需要是方阵，需要有$q\times q$个处理器，每个处理器上保留网格中的一块权重和激活值（而非完整的激活值），因为Transformer中很多都是矩阵乘，所以中间可以使用SUMMA并行矩阵进行并行计算。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-13-01:28:32.png" alt="image-20240913000444093" style="zoom:50%;" />
<p>效果：每张GPU上，中间激活值变为1D TP的$\frac{1}{q^2}$，因此可以使用更大的batch size，计算量和参数量变为1D TP的$\frac{1}{q}$，但是代价是通信带宽和通信时延变大了（变为原来的3倍）</p>
<h3 id="25d-tensor-parallelhttpsarxivorgabs210514500v1"><a href="https://arxiv.org/abs/2105.14500v1">2.5D Tensor Parallel</a></h3>
<p>2D TP虽然减小了激活值、计算量和参数量，但是增加了通信带宽和通信时延。对于Y=XA，2D-TP中将X和A划分为二维网格（对应$q\times q$个处理器）；2.5D-TP中将X划分为三维网格，将A划分为二维网格，然后再使用SUMMA算法计算并行矩阵乘（需要$d\times q\times q$个处理器）。在d=1时，2.5D-TP退化为2D-TP，d=q时，2.5D-TP变成了3D-TP。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-14:08:01.png" alt="image-20240915175910712" style="zoom:33%;" />
<p>上图中，将输入X划分为d个二维网格，图中排布成了三维的结构（但是感觉排布成多个二维网格stack的形式更好？）。效果是，每张GPU上，中间激活值和计算量变为2D-TP的$\frac{1}{d}$，参数量没变，通信带宽减小了，通信时延没变。</p>
<h3 id="3d-tensor-paralllelhttpsarxivorgabs210514450"><a href="https://arxiv.org/abs/2105.14450">3D Tensor Paralllel</a></h3>
<p>类似的，对于Y=XA，3D-TP中将A和X划分为三维网格（对应$q \times q \times q$个处理器），计算量、参数量、中间激活都变成了2D-TP的$\frac{1}{q}$，通信带宽变为2D-TP的$\frac{1}{q^2}$，通信延时与2D-TP相同。</p>
<h1 id="序列并行">序列并行</h1>
<p>原来Transformer的Attention部分的计算复杂度是输入序列长度的二次方，因此一般输入序列长度不会太长。序列并行没有在算法角度上改变Attention的计算，Attention的输入仍然是是[b, s, h]的形状，序列并行在s维度上切分，将每一个输入分块放在一个GPU上，在计算过程中进行通信。经过s维度的切分，这样每个GPU上Attention的输入就变成了[b, s/N, h]，因此输入序列可以很长。</p>
<p>more reading：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/707204903">图解序列并行云台28将（上篇）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/707435411">图解序列并行云台28将（云长单刀赴会）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/707499928">图解序列并行云台28将（下篇）</a></li>
</ul>
<h2 id="colossal-sphttpsarxivorgabs210513120"><a href="https://arxiv.org/abs/2105.13120">Colossal-SP</a></h2>
<p>原来Attention的计算为：$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$，Colossal-SP中将Attention变成了分布式环境下的Ring Self-Attention，具体过程为（下面是单头的示意图，每个Q、K、V大小为[b, s/n, h/a]）：</p>
<ul>
<li>
<p>（一开始的输入已经在s维度进行了切分，每张卡的输入为[b, s/n, h]）</p>
</li>
<li>
<p>在计算$QK^T$的过程中，通过Ring的方式传递K，因此每个GPU上分块的Q可以看见所有的K</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-14:07:54.png" alt="RSA" style="zoom:50%;" />
</li>
<li>
<p>在计算$attn_score \times V$的过程中，通过Ring的方式传递V，类似上面的过程</p>
</li>
</ul>
<p>需要注意的有以下几点：</p>
<ol>
<li>Colossal-SP针对的是encoder-only架构的的SP并行，针对decoder-only架构，因为attn_score后还要加上一个mask，比如上图GPU1在step4阶段需要mask掉 attn_score[1, 2:8]和 attn_score[2, 3:8]，mask矩阵较大；而GPU4只需要mask掉attn_score[7,8]即可，被mask掉的部分，$Q_iK_j^T$即使算了也是白算，所以这些部分可能不会实际进行计算，后面直接加上一份负无穷大的数，这样就导致不同GPU计算负载不均衡，前面mask多的部分计算少，后面mask少的部分计算多。</li>
</ol>
<blockquote>
<p>在<a href="https://arxiv.org/abs/2105.13120">该论文</a>中，除了提出了Colossal-SP，还针对FFN部分进行了分析。</p>
<ul>
<li>原来Megatron-LM的TP并行中，输入的是完整的中间激活[b, s, h]，对Linear的权重进行了切分。</li>
<li>当序列s比较长的时候，可能输入的中间激活占用显存比权重更大。因此，论文中对输入的中间激活也在s维度进行切分，每张卡上FFN部分的输入的中间激活形状为[b, s/n, h]，此时需要每张卡上保留完成的Linear权重</li>
</ul>
<p>到底哪个更省显存，需要根据模型结构、序列长度等进行计算，比如GPT类的FFN和Llama类的FFN结构就稍有区别，但总的来说就是将FFN部分的激活占用的显存和参数占用的显存加在一起，比一下，也不复杂。论文中举了一个GPT FFN的例子，结论是当$bs \gt 32h$时，序列并行更省显存（这里符号表示与论文不一样，参考文章开头的描述）</p>
</blockquote>
<h2 id="megatron-sphttpsarxivorgabs220505198"><a href="https://arxiv.org/abs/2205.05198">Megatron-SP</a></h2>
<p>Colossal-SP与Megatron-SP都是将[b, s, h]的输入在s维度进行切分，但是其实是解决的不同的问题。Colossal-SP通过SP切分，解决了在分布式环境下训练超长序列的问题，解决方式是s维度切分+Ring通信。Megatron-SP通过SP切分，解决了每张卡上Transformer Block中LN和Dropout都要保留一份完整的[b, s, h]的输入的问题，解决方式是s维度切分+修改通信方式，具体过程为：（上图是Megatron-TP，下图是Megatron-SP）</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-14:08:15.png" alt="image-20240915204902150" style="zoom: 67%;" />
<ul>
<li>
<p>将LN后面的通信从$f$替换为$g$</p>
<ul>
<li>Megatron-TP中，LN之前是完整的[b, s, h]激活，经过LN之后仍然是完整的[b, s, h]激活，正好输入到Self-Attention中。反向过程中，每张卡都有对输入激活的完整的un-reduced梯度，需要先all-reduce平均后，才能继续向前进行反向传播</li>
<li>Megatron-SP中，LN之前是沿s维度切分的[b, s/n, h]激活，经过LN之后，需要先经过all-gather通信，每张卡拿到完整的[b, s, h]激活，才能输入到Self-Attention中。反向过程中，每张卡都有完整的un-reduced梯度，需要先经过reduce-scatter，每张卡拿到对不完整的输入激活对应的不完整的reduced梯度，才能继续向前反向传播</li>
</ul>
</li>
<li>
<p>将Dropout前面的通信从$\bar{f}$替换为$\bar{g}$</p>
<ul>
<li>Megatron-TP中，Dropout之前是每张卡上MHA部分的计算的输出激活，前向过程中需要先对不同卡上该输入激活进行一个all-reduce平均，然后再Dropout。反向过程中，梯度在FFN部分的$f$进行过一次all-reduce，再经过LN和Dropout后，此时还没有必要使得对MHA输入的梯度都相同，所以直接$\bar{f}$的反向是no op</li>
<li>Megatron-SP中，经过TP切分之后的MHA和FFN，每张卡都有一份完整的、un-reduced输入激活，进行一次reduce-scatter正好可以使得中间激活恢复到每张卡上只有一份不完整的reduced激活。反向过程中，MHA和FFN部分的反向需要使用到对输出的完整的梯度，此时每张卡上只有一块不完整的梯度，所以要进行一次all-gather拿到对输出激活的完整的梯度。</li>
</ul>
<blockquote>
<p>$f$：FWD=no_operation, BWD=all-reduce</p>
<p>$\bar{f}$：FWD=all-reduce, BWD=no_operation</p>
<p>$g$：FWD=all-gather, BWD=reduce-scatter</p>
<p>$\bar{g}$：FWD=reduce-scatter, BWD=all-gather</p>
</blockquote>
</li>
</ul>
<p>需要说明的有两点：</p>
<ol>
<li>Megatron-SP需要配合Megatron-TP使用，Megatron-TP在MHA和FFN之前仍然需要使用完整的输入激活，但是Colossal-SP可以使用序列切分之后的不完整的输入进行Self-Attention的计算（Ring Self-Attention）</li>
<li>Megatron-SP的通信量与Megatron-TP的参数量和通信量相同，中间激活减少，计算量也减少</li>
</ol>
<blockquote>
<p><a href="https://arxiv.org/abs/2205.05198">该论文</a>中还提到一点优化，叫做selective re-compute，就是对于那些占据大量中间激活、但是计算量小的部分，这些部分可以开启重计算（之前全部重计算叫做full re-compute）。比如Transformer block中，$QK^T$矩阵乘法，softmax，softmax dropout，attn_score和V的矩阵乘法，这是一个连续的部分，占用激活多，但是计算量不大，这部分可以在训练过程中进行重计算，比如实际训练中，论文中提到GPT-3的例子，这样开启selective re-compute后，节省了70%的显存，但是只增加了2.7%的FLOPs，看起来很划算。</p>
</blockquote>
<h2 id="ulysses">Ulysses</h2>
<p>Ulysses可以认为是对前面Colossal-SP的改进，Colossal-SP中在序列s的维度进行了切分，在计算Attention的时候，采用了Ring Self-Attention方法，假如有N个GPU，那么需要进行N-1次的Ring，每次单卡通信量为$[b, \frac{s}{N}, \frac{h}{a}] \times a$个头，因此单卡每次前向需要的通信量为：$2(N-1) \times (b \times \frac{s}{N} \times h) = 2bsh \times \frac{N-1}{N}$。Ulysses对这个通信量进行了优化，Attention部分具体过程为：</p>
<ul>
<li>（一开始的输入已经在s维度进行了切分，每张卡的输入为[b, s/n, h]）</li>
<li>每张卡上先乘$W_Q, W_K, W_V$得到Q，K，V
<ul>
<li>极其注意此时$W_Q, W_K, W_V$的维度是[h, h]，而Colossal-SP和Megatron-SP中$W_Q, W_K, W_V$的维度是[h, h/a]</li>
<li>也就是说，对于[b, s/n, h]的输入，Ulysses在每张卡上都保留了所有head的Q、K、V（Q、K、V的维度是[b, s/n,  h]），而Colossal-SP和Megatron-SP只在每张卡上保留了该卡上对应head的Q、K、V（Q、K、V的维度是[b, s/n,  h/a]）</li>
</ul>
</li>
<li>此时每张卡上Q、K、V的维度是[b, s/n,  h]，对Q、K、V在h维度上切分，一共a个头，正好切成a份（这个部分没有在图虫表示），后续每张卡上都会进行$\frac{a}{N}$个头Attention的计算（比如下图中a=8，N=4，每张卡上进行2个头的Attention的计算）。这个操作也是很好理解，因为本来QKV就应该划分成多头，每个头计算一部分。</li>
<li>进行All-to-All通信。刚才在每张卡上都对QKV按头进行了切分，现在将每张卡上QKV对应head1、head2的部分（颜色最浅）发送到GPU1，将每张卡上QKV对应head3、head4的部分发送到GPU2，将每张卡上QKV对应head5、head6的部分发送到GPU3，将每张卡上QKV对应head7、head8的部分（颜色最深）发送到GPU4。</li>
<li>每张卡上进行多头注意力的计算，就像Megatron-TP那样</li>
<li>算完了多头注意力，然后再All-to-All通信，将计算结果物归原主</li>
</ul>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-14:08:23.png" alt="Ulysses"  /></p>
<p>分析一下单卡通信量，每个Q、K、V的大小是[b, s/n, h]，需要发送给其他GPU的部分是其中的$\frac{N-1}{N}$，通信量=$3 \times (b \times \frac{s}{N} \times h) \times \frac{N-1}{N} = 3bh \times \frac{s}{N} \times \frac{N-1}{N} = 2bsh \times \frac{N-1}{N} \times \frac{3}{2N}$，从复杂度上看是Colossal-SP通信量的$\frac{1}{N}$。而且注意到一点，如果序列长度翻倍，那么只要GPU数量也翻倍，则通信量不变。</p>
<p>还有两点需要说明：</p>
<ol>
<li>
<p>Ulysses可以和ZeRO-3一起使用，因为Ulysses中通信的都是中间激活值，每张卡上Wq、Wk、Wv、Wo及其梯度和optimizer_states可以切分放在不同的DP上，进一步节省显存</p>
</li>
<li>
<p>Ulysses的一个缺点是，每张卡上负责$\frac{a}{N}$个head的计算，每个head可以使用FA进行单卡优化，但是如果模型使用MQA、GQA等方式，头数a本来就不大，也就限制了N的大小</p>
</li>
</ol>
<p>参考：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/689067888">大模型训练之序列并行双雄：DeepSpeed Ulysses &amp; Ring-Attention</a></li>
</ul>
<h2 id="ring-attention">Ring-Attention</h2>
<p>Ring-Attention的提出还是在<a href="https://arxiv.org/abs/2105.13120">Colossal-SP这篇论文</a>中，此时Ring-Attention还是使用two-pass的方式，先Ring K，再Ring V。在<a href="https://arxiv.org/abs/2310.01889">后续的论文</a>中，基于FA online-softmax的思路（或者直接调用flash-attention的接口），成功将two-pass的方式优化成one-pass的方式，一次Ring就可以计算最后的输出$O$。还有一个优化点是针对Ring-Attention由于casual mask导致的计算负载不均衡问题。一个优秀的开源实现是<a href="https://zhuanlan.zhihu.com/p/683714620">ring attention + flash attention：超长上下文之路</a>，这里基本按照该实现的思路，简单描述一下过程。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-19-23:07:33.png" alt="RSA(updated)"  /></p>
<ul>
<li>
<p>首先还是针对[b, s, h]的输入，在s维度上进行切分，每个GPU上分别有一个$V_i, Q_i, K_i$分块，然后使用flash-attention进行计算，得到一个局部的$O_i$，以及flash-attention函数特有的一个返回值$lse_i$</p>
<blockquote>
<p>关于flash-attention和这个返回值$lse$，可以查看<a href="https://qinganzhang.github.io/posts/flash_attention%E7%AE%80%E8%A6%81%E7%AC%94%E8%AE%B0/">flash_attention简要笔记</a>中的记录，简单来说，$lse=log[\sum_j e^{S_{ij}-rowmax(S_{ij})}]$，其中$S_{ij}=Q_i\times K_j^T$</p>
</blockquote>
</li>
<li>
<p>对于每个头，每个step都会有：</p>
<ul>
<li>
<p>每个GPU上有一个$Q_i, K_i, V_i$分块，基于flash-attention算出的局部的输出$O_i$（记为$block_O_i$），和一个$lse_i$（记为$block_lse_i$或者$new_lse_i$）</p>
</li>
<li>
<p>（除了第一个step）还有全局的、未修正的输出$O_i$和$lse_i$</p>
</li>
<li>
<p>此时对全局的、未修正的输出$O_i$和$lse_i$进行修正，具体修正公式见上图的step2（第一个下标表示rank id，第二个下标(if have)表示step），这里以GPU1为例，此时进行$Q_1, K_2, V_2$的flash-attention计算，其中$S_{12}=Q_1K_2^T$</p>
<blockquote>
<p>实际代码中没有采用这种修正方式，进行了一些优化，具体介绍见：<a href="https://github.com/zhuzilin/ring-flash-attention/pull/34">improve readability and potential numerical stability of <code>out</code> and <code>lse</code> in <code>_update_out_and_lse</code> by refactoring their computational expressions. #34</a></p>
</blockquote>
</li>
<li>
<p>（除了最后一个step）在每个step的最后，会进行Ring KV的通信</p>
</li>
</ul>
</li>
</ul>
<p>还有针对由于mask导致的计算负载不均衡的问题，采用stripped ring-attention或者zigzag ring-attention的方法，该实现中为了调用flash-attention的接口，使用zigzag ring-attention的方法，这里解释一下（或者可以见<a href="https://arxiv.org/abs/2405.07719">USP: A Unified Sequence Parallelism Approach for Long Context Generative AI</a>中的load balance partition）：</p>
<ul>
<li>
<p>如果使用stripped ring-attention，比如GPU0上的Q分块是$Q_0, Q_4, Q_8, Q_{12}$，K分块是$K_0, K_4, K_8, K_{12}$，然后在Ring的过程中，还可能与GPU2的K分块$K_2, K_6, K_{10}, K_{14}$算attention，比如$Q_0, Q_4, Q_8, Q_{12}$与$K_2, K_6, K_{10}, K_{14}$算attention，$S_{ij}=Q_iK_j^T$需要进行mask的部分是：</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-18-01:06:24.jpg" alt="tset2"  /></p>
<p>注意到Q和K都是分块，每个分块有若干行向量，标准attention的mask是上三角矩阵，但是上面示意的mask部分是台阶形状的（比如$Q_0$的最后一个向量和$K_2$的第一个向量需要进行mask，但是转到Q的下一行，即$Q_4$，$Q_4$又不需要与$K_2$进行mask，所以mask一下就断层了），由于这个原因，就没法调用标准的flash-attention</p>
</li>
<li>
<p>如果使用zigzag ring-attention，比如GPU0上的Q分块是$Q_0, Q_1, Q_{14}, Q_{15}$，K分块是$K_0, K_1, K_{14}, K_{15}$，然后在Ring的过程中，还可能与GPU2的K分块$K_4, K_5, K_{10}, K_{11}$算attention，比如$Q_0, Q_1, Q_{14}, Q_{15}$与$K_4, K_5, K_{10}, K_{11}$算attention，$S_{ij}=Q_iK_j^T$需要进行mask的部分是：</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-18-01:06:26.jpg" alt="tset3"  /></p>
<p>所以<a href="https://zhuanlan.zhihu.com/p/683714620">博客</a>中说是一个完整的方形，因此可以直接调用flash-attention的接口了</p>
</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/689067888">大模型训练之序列并行双雄：DeepSpeed Ulysses &amp; Ring-Attention</a>中有对Ulysses和Ring-Attention的比较，，参考这篇博客，自己重新进行了理解：</p>
<ul>
<li>通信量
<ul>
<li>Ulysses单卡通信量为$2bsh \times \frac{N-1}{N} \times \frac{3}{2N}$</li>
<li>Ring-Attention每个step会Ring KV，KV大小为$bsh$，需要Ring N-1次，每张卡通信量都是相同的，所以单卡通信量为$2bsh\times \frac{N-1}{N}$</li>
<li>可以看出Ulysses的通信量大约为Ring-Attention的$\frac{1}{N}$</li>
</ul>
</li>
<li>通信方式：Ulysses需要All2All通信，更加复杂</li>
<li>内存使用：近似</li>
<li>其他特点：
<ul>
<li>模型结构的适配：Ring-Attention对头数没有要求，而Ulysses会受到头数的限制</li>
<li>输入长度的适配：Ulysses对输入长度没有要求，而Ring-Attention需要进行负载均衡</li>
</ul>
</li>
</ul>
<p>more reading:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/703669087">我爱DeepSpeed-Ulysses：重新审视大模型序列并行技术</a></li>
</ul>
<h2 id="uspunified-sequence-parallelismhttpsarxivorgabs240507719"><a href="https://arxiv.org/abs/2405.07719">USP(Unified Sequence Parallelism)</a></h2>
<p>总的来说就是将Ulysses-SP和Flash-Ring-Attention结合结合起来，具体过程是：（假设一个node有2张卡，一个2个node，总的序列长度为8，头数为2）</p>
<ul>
<li>step0，进行Ulysses的操作，在每个节点内进行All2All通信（每个节点内部构成一个通信组，比如GPU0、GPU1构成一个通信组，GPU2和GPU3构成一个通信组，通信组内部进行All2All的通信），此时每个节点得到一份节点内部完整序列的、按头切分的QKV，</li>
<li>然后进行Flash-Ring-Attention的操作，接下来每个step中：
<ul>
<li>基于局部的Q、K、V，进行flash-attention的计算，如果当前不是Ring-Attention的第一个step，则对输出O要进行修正</li>
<li>如果不是Ring-Attention的最后一个step，要进行K、V的Ring通信。在图中，对位于不同GPU上的、相同head的、不同部分序列的K、V进行Ring通信，比如GPU0和GPU2、GPU1和GPU3分别构成通信组，Ring通信在通信组内进行P2P通信</li>
</ul>
</li>
<li>最后（step3）再进行Ulysses的操作，将输出All2All通信，对应的token物归原主</li>
</ul>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-19-23:07:24.png" alt="USP"  /></p>
<p>more reading：</p>
<ul>
<li>
<p>通过将USP用在Megatron-LM中，给出了USP加入之后4D混合并行最佳实践方案：<a href="https://zhuanlan.zhihu.com/p/698031151">序列并行做大模型训练，你需要知道的六件事</a></p>
</li>
<li>
<p>给出了一个USP潜在适合应用的场景，<a href="https://zhida.zhihu.com/search?q=%E5%BA%8F%E5%88%97%E9%95%BF%E5%BA%A6&amp;zhida_source=entity&amp;is_preview=1">序列长度</a>变化的SFT训练任务：<a href="https://zhuanlan.zhihu.com/p/700491837">LLM变长序列分布式训练迷思</a></p>
</li>
<li>
<p>USP应用在Megatron-LM中，如何做通信计算重叠来追求更极致通信优化：<a href="https://zhuanlan.zhihu.com/p/700639611">大模型训练优化：论反向传播中序列并行和张量并行的通信计算重叠方法</a></p>
</li>
</ul>
<h1 id="context-parallel">Context Parallel</h1>
<p>Context Parallel很像（甚至就是）Ring-Attention，这里从历史发展的角度，进行一个简单的辨析：</p>
<ul>
<li>
<p>首先说一下Sequence Parallel。一开始（2021.05）有two-pass的<a href="https://arxiv.org/abs/2105.13120">Colossal-SP</a>；然后后来（2023.10）改进成了one-pass的<a href="https://arxiv.org/abs/2310.01889">Ring-Attention</a>，此时的one-pass是作者基于jax框架重新实现的、对输出O的调整逻辑（比如手动更新denominator和max_score，分别对应flash-attention原始论文中的$l$和$m$）；再后来（2024.02）<a href="https://www.zhihu.com/people/zhu-xiao-lin-22-96">朱小霖</a>实现了一版<a href="https://github.com/zhuzilin/ring-flash-attention">开源的Ring-Attention</a>，特点是仍然保持了one-pass的Ring-Attention，但是中间通过调用现有的flash-attention接口（会返回lse，用这个可以对输出O进行修正）避免了one-pass FA部分内部逻辑的改动（因此这个开源实现叫做ring-flash-attention），而且使用zigzag的方式实现了计算负载均衡。</p>
</li>
<li>
<p>然后说到Context Parallel，Context Parallel主要是针对Megatron-LM的。</p>
<ul>
<li>
<p>一开始（22.05）Megatron-LM也提出了序列并行，这里把它叫做<a href="https://arxiv.org/abs/2205.05198">Megatron-SP</a>，Megatron-SP和Colossal-SP都是针对序列维度进行切分，但是采用的不同的优化方法，Megatron-SP主要是在LN和Dropout的前面对输入在序列维度上进行切分，self-attention的前面经过All-Gather拿到了全部序列。</p>
</li>
<li>
<p>后来，Megatron-LM也采取了Ring-Attention的思想，原来attention之前不是要【前向All-Gather/反向Reduce-Scatter】吗，现在将这个【前向All-Gather/反向Reduce-Scatter】转换为P2P通信（比较困惑，后续需要结合代码看一下）。Megatron-LM中说CP相对于Ring-Attention的优势是（这里指的Ring-Attention，指的是<a href="https://arxiv.org/abs/2310.01889">Ring Attention with Blockwise Transformers for Near-Infinite Context</a>，此处的实现，作者基于jax框架，将flash-attention重新时间并融合到Ring-Attention中）：</p>
<ol>
<li>
<p>充分利用最新开源的cuDNN flash attention kernel</p>
<blockquote>
<p>tip: OSS 表示 Open Source Software</p>
</blockquote>
</li>
<li>
<p>移除由于上三角mask导致的不必要计算，从而实现负载均衡</p>
</li>
</ol>
<blockquote>
<p>Megatron中对CP的介绍部分的原话是：</p>
<p>Context Parallelism (&ldquo;CP&rdquo;) is a parallelization scheme on the dimension of sequence length. Unlike prior SP (sequence parallelism) which only splits the sequence of Dropout and LayerNorm activations, CP partitions the network inputs and all activations along sequence dimension. With CP, all modules except attention (e.g., Linear, LayerNorm, etc.) can work as usual without any changes, because they do not have inter-token operations. As for attention, the Q (query) of each token needs to compute with the KV (key and value) of all tokens in the same sequence. <strong>Hence, CP requires additional all-gather across GPUs to collect the full sequence of KV</strong>. （计算Attention之前，要进行一次all-gather通信）Correspondingly, reduce-scatter should be applied to the activation gradients of KV in backward propagation. <strong>To reduce activation memory footprint, each GPU only stores the KV of a sequence chunk in forward and gathers KV again in backward.</strong>（为了省显存，前向all-gather拿到全部的KV后，算完attention后只保存一块KV，反向时类似重计算一样重新all-gather一次） KV communication happens between a GPU and its counterparts in other TP groups. <strong>The all-gather and reduce-scatter are transformed to point-to-point communications in ring topology under the hood</strong>.（all-gather和reduce-scatter） Exchanging KV also can leverage MQA/GQA to reduce communication volumes, as they only have one or few attention heads for KV.</p>
<p>而且附了一张图：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-20-21:07:43.png" alt="image-20240920205531293" style="zoom: 67%;" />
<p>意思是在TP2CP2的情况下，GPU0和GPU1、GPU2和GPU3分别构成两个TP group，GPU0和GPU2、GPU1和GPU3分别构成两个CP group，按序列切分的输入经过第一个AG/RS，在一个TP group上得到了一个half的输入，然后得到QKV。接下来进行CP的通信操作，这里在CP group内部进行all-gather通信，后面的过程基本与原来相同。</p>
<p>（橙色的通信都是TP的，深蓝色的通信是CP的）</p>
</blockquote>
</li>
<li>
<p>在具体实现上，Megaron-LM中设置好CP的相关通信组cp_group之类的，然后将该cp_group传入到Megatron-LM调用的TransformerEngine中；在TransformerEngine中，如果使用flash-attention，调用的是<a href="https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/pytorch/attention.py#L3941"><code>attn_forward_func_with_cp</code></a>，内部根据cp通信方式的不同，又进一步调用了以下实现：</p>
<ul>
<li>
<p>如果<code>cp_comm_type=='p2p'</code>，则调用AttnFuncWithCPAndKVP2P</p>
<ul>
<li>内部通信调用的是<code>torch.distributed.P2POp</code></li>
</ul>
</li>
<li>
<p>如果<code>cp_comm_type=='all_gather'</code>，则调用AttnFuncWithCPAndKVAllGather</p>
<ul>
<li>内部通信调用的是<code>torch.distributed.all_gather_into_tensor</code></li>
</ul>
<blockquote>
<p>在<a href="https://arxiv.org/pdf/2407.21783">llama3</a> 3.3.2节 Context parallelism for long sequences 中，首先给出了负载均衡的办法，类似于上面朱小霖开源Ring-Attention中的zigzag方式，然后原来Ring-Attention中用的是P2P通信（可以通信计算重叠），现在改成先All-Gather拿来K和V，原因有二：</p>
<ol>
<li>it is easier and more flexible to support different types of attention masks in all-gather based CP attention, such as the document mask</li>
<li>由于使用了GQA，通信的K和V都比原来小了很多，通信时延也变小了</li>
</ol>
</blockquote>
</li>
<li>
<p>如果<code>cp_comm_type=='a2a'</code>，则调用AttnFuncWithCPAndKVPA2A</p>
<blockquote>
<p>类似Ulysses-SP，没有Ring的操作</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>more reading：</p>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/698447429">Context Parallelism的原理与代码浅析</a></p>
</li>
<li>
<p><a href="https://www.zhihu.com/question/637961859">Megatron-LM 中 Context Parallel 的工作原理是什么？</a></p>
</li>
<li>
<p><a href="https://www.mltalks.com/posts/1017283893/#more">Megatron-LM源码系列(八)： Context Parallel并行</a></p>
</li>
</ul>
<h1 id="专家并行expert-parallel">专家并行Expert Parallel</h1>
<p>进入到具体模型结构之前，首先要明确一个概念：MoE是在多个GPU上共享的，或者说，非MoE的部分像数据并行一样在多个GPU上有一份模型的replica（一份复制），但是MoE的部分是这多个GPU共享的，即每个GPU上放几个专家（注意这个不是<strong>专家并行EP</strong>，EP指的是一个专家放在多个GPU上），这些专家共同组成一个MoE结构。MoE在算法方面的改进更多一些，本文也会简单介绍，但是更多侧重还在于工程角度和具体过程。</p>
<p>MoE基本范式是：（图中红色表示输入输出，紫色表示中间变量，黑色表示通信和操作）</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:37:01.png" alt="MoE(router)"  /></p>
<ol>
<li>gating：[b, s, h]的输入先经过gating，输出是[b, s, e]，表示每个token分配到每个专家的概率，其中e指的是专家数量（实际上就一个linear+softmax的过程）。然后取topk专家，输出是[b, s, k]，表示每个token根据上面计算的概率选择了概率最大的k个专家，[b, s, k]中记录的是选择的专家的index</li>
<li>dispatch（上图中上半部分）：将[b, s, h]的输入tokens根据[b, s, k]的专家index，路由到特定的专家
<ul>
<li>需要注意的一点是，原来输入是batched，首先经过一次all2all通信，将token发送到对应专家所在的GPU上；如果一个GPU上有多个专家，那么还要进行一个本地的重排，使得发送给一个专家的tokens batch到一起</li>
</ul>
</li>
<li>专家计算</li>
<li>undispatch（上图中下半部分）：dispatch的逆过程</li>
</ol>
<h2 id="gshardhttpsarxivorgabs200616668"><a href="https://arxiv.org/abs/2006.16668">Gshard</a></h2>
<p>这是首次将MoE引入到Transformer的工作，将Encoder中的那一个FFN替换为一个MoE，相当于就是在FFN前面加了一个gating来进行路由选择（top2），而且每间隔一层来进行MoE的替换（比如一层MoE，一层FFN，这样交替循环）</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:36:54.png" alt="image-20240921005223186" style="zoom: 80%;" />
<h2 id="switch-transformerhttpsarxivorgabs210103961"><a href="https://arxiv.org/abs/2101.03961">Switch Transformer</a></h2>
<p>Switch Transformer主要针对Gshard进行了三点改进：</p>
<ol>
<li>
<p>简化路由：从原来top2简化到top1，也能保证模型的质量</p>
 <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:36:50.png" alt="image-20240921024726843" style="zoom:50%;" />
</li>
<li>
<p>高效路由：</p>
<ul>
<li>
<p>专家容量（expert capacity）和容量因子（capacity factor）：由于Switch Transformer是基于Mesh-TensorFlow实现的，该框架要求每个专家输入的Tensor shape是固定的，即需要提前分配好大小，但是由于动态路由，不到运行时也不知道每个专家有多少输入token。因此，干脆固定某个大小算了，这个固定的大小就叫做专家容量（expert capacity），具体来说就是$expert_capacity=capacity_factor \times  \frac{tokens_per_batch}{number_of_experts}$，其中这个分数表示一个mini-batch平均分到每个专家的token数量，容量因子就是来扩大这个平均值的</p>
<ul>
<li>
<p>如果容量因子太大，则提前分配的专家容量越大，其中需要padding的token越多，增加了无效计算</p>
</li>
<li>
<p>如果容量因子太小，如果某个token分配到某个专家，但是该专家的输入缓冲区已经满了，只能将该token丢弃，然后最后残差连接加回去，比如说像下面这个图（图中采取top2，但是Switch Transformer采取top1，这里只是为了说明drop token的例子），token7分配到expert2和expert3，但是这两个专家的输入缓冲已经满了，只能丢弃，然后残差跳过MoE连接到后续非MoE部分，或者说相当于MoE部分对该token的输出是0</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:36:44.jpeg" alt="img" style="zoom: 33%;" />
</li>
</ul>
</li>
<li>
<p>Load balance loss：由于token进行动态路由，可能有的专家要处理很多token，有的专家处理很少token，为了负载均衡，提高模型训练和推理效果和性能，最好做到token在不同专家之间分配大致相同，添加的$P_i$一项只是为了保证loss可以求导（前面那些部分无法求导）</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:36:34.png" alt="image-20240921024025937" style="zoom: 50%;" />
</li>
</ul>
</li>
<li>
<p>还提出了四条有助于提升预训练和微调稳定性的技巧，由于本文更多关注于工程角度和具体流程，这里省略</p>
</li>
</ol>
<h2 id="fastmoehttpsarxivorgabs210313262"><a href="https://arxiv.org/abs/2103.13262">FastMoE</a></h2>
<p>该论文的创新点：</p>
<ul>
<li>之前分布式MoE主要是基于TPU和Mesh-Tensorflow框架，FastMoE基于GPU和PyTorch框架，实现了EP并行</li>
<li>工程优化：
<ul>
<li>将专家模块进行抽象，使得任意网络可以作为专家</li>
<li>将一个GPU上多个专家的计算过程整合为一个batched_gemm</li>
<li>在all2all通信token之前，先all2all通信一下token的数量和大小，由此来动态分配空间</li>
</ul>
</li>
</ul>
<h2 id="deepspeed-moehttpsarxivorgpdf220105596"><a href="https://arxiv.org/pdf/2201.05596">DeepSpeed-MoE</a></h2>
<p>该论文对Switch Transformer有两点改进：</p>
<ol>
<li>
<p>提出了新的MoE结构（叫做PR-MoE，金字塔MoE），变化有两处：</p>
<ul>
<li>
<p>观察到在模型靠后的层使用MoE比模型前面层使用MoE的算法效果更好，所以随着模型层数增加，后面的层使用更多的专家，这样同时也减小吗MoE的显存</p>
</li>
<li>
<p>观察到top2 gating比top1 gating算法效果好，因此固定一个expert，然后剩下再进行top1 gating，这样路由过程与Switch Transformer相同，但是达到了top2 gating的效果</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:36:25.png" alt="image-20240921025315828" style="zoom:50%;" />
</li>
</ul>
</li>
<li>
<p>从推理角度改进了模型的并行方法，推理场景主要特征是batch_size比较小，所以可能不适合训练场景</p>
</li>
</ol>
<h2 id="tutelhttpsarxivorgabs220603382"><a href="https://arxiv.org/abs/2206.03382">TUTEL</a></h2>
<p>静态的策略不能满足MoE的动态特性（即专家的输入缓冲是固定大小的，但是训练过程中专家处理token数量是不固定的，有可能drop token损失精度，有可能padding浪费计算资源）</p>
<p>Tutel允许在每次训练迭代中，设置不同的expert_capacity，所以Tutel可以保证不drop tokens，但是没法避免zero-padding。</p>
<p>参考：</p>
<p><a href="https://zhuanlan.zhihu.com/p/653518289">MoE训练论文解读之Tutel: 动态切换并行策略实现动态路由</a></p>
<h2 id="megablockshttpsarxivorgabs221115841"><a href="https://arxiv.org/abs/2211.15841">MegaBlocks</a></h2>
<p>该论文针对一个GPU上多个专家、而且没有专家容量限制（即没有drop token，没有padding，来多少token就计算多少token）的MoE场景。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-00:36:20.png" alt="Megablocks" style="zoom: 50%;" />
<p>原来一个GPU上多个专家、有专家容量限制时，像左图，token3被drop，专家2的input buffer需要padding，此时输入大小都相等，可以直接调用cutlass中的batched gemm操作。现在一个GPU上多个专家、没有专家容量限制时，像右图，每个专家的input buffer大小不同，</p>
<p>cutlass中的grouped gemm操作需要输入维度大小相同，Megablocks中提出了Variable Sized Grouped GEMM操作</p>
<p>但是该论文的一个缺陷是，该论文要求专家数量&gt;GPU数量，但是在实际场景中往往GPU数量是更多的</p>
<h1 id="评测指标">评测指标</h1>
<p>MFU+HFU共同衡量了某一模型实现对某种芯片计算性能的利用情况：</p>
<ul>
<li>MFU（Model FLOPs Utilization）：模型算力利用率，指模型一次前反向计算消耗的矩阵算力与机器算力的比值
<ul>
<li>实际计算中，$MFU=\frac{每token模型的FLOPs \times 每秒的token数量}{机器峰值FLOPs}$</li>
</ul>
</li>
<li>HFU（Hardware FLOPs Utlization）：硬件算力利用率，指考虑重计算后，模型一次前反向计算消耗的矩阵算力与机器算力的比值</li>
</ul>
<p>还有一点需要说明，MoE的MFU一般相对较低</p>
<h1 id="更新">更新</h1>
<p>发现了更加优质的系列好文：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/J4QEFy_CZL61gYGCKtPhlg">大规模分布式 AI 模型训练系列——数据并行</a></li>
<li><a href="https://mp.weixin.qq.com/s/1syPf8XNQfgk7mClMDSqhw">大规模分布式 AI 模型训练系列——流水线并行</a></li>
<li><a href="https://mp.weixin.qq.com/s/V8SQWA9O8i5fPJF3etbxPA">大规模分布式 AI 模型训练系列——张量并行</a></li>
<li><a href="https://mp.weixin.qq.com/s/4tB1UCHdYOG9pOq7wxiNKA">大规模分布式 AI 模型训练系列——序列并行</a></li>
<li><a href="https://mp.weixin.qq.com/s/GJHsrF1rml9XAmkLDnj2xw">大规模分布式 AI 模型训练系列——专家并行</a></li>
</ul>
<p>llama3 pipeline：https://mp.weixin.qq.com/s/1syPf8XNQfgk7mClMDSqhw</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Encoder Decoder和decoder Only架构训练和推理浅析</title>
      <link>https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/</link>
      <pubDate>Sat, 07 Sep 2024 14:45:54 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/</guid>
      <description>之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐to</description>
      <content:encoded><![CDATA[<p>之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐token生成。欸，那之前传统机器翻译不也是这样吗，输出一个中文句子，然后再自回归地逐token生成英文句子，有点混乱了。下面我们来重新梳理一下，LLM的发展过程、典型模型架构和训练过程。</p>
<h1 id="开山之作attention-is-all-you-need">开山之作：Attention is all you need</h1>
<h2 id="模型架构和训练过程">模型架构和训练过程</h2>
<p>模型架构自不必多说，就像下面最左边和最右边经典的模型架构图所示，是典型的encoder-decoder架构。Transformer一开始，解决的是一个seq2seq的任务（下面以中译英机器翻译任务为例），模型结构中包含了encoder和decoder（将此时的decoder称为naive-decoder），下面图中是encoder-decoder的Transformer的训练过程，左边是encoder部分，右边是decoder部分。在训练阶段，将中文句子经过encoder生成编码矩阵C，将shifted right的ground truth（开头加了BOS的英文句子）作为decoder的输入，整体模型的监督信号是对应的英文句子。因此，训练是并行的，一个iteration就可以将一个中英pair训练完（假设batch size=1），训练任务就是next token predicate。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:01:57.png" alt="encoder-decoder架构的训练过程（以机器翻译为例）" style="zoom:120%;" />
<h2 id="推理过程">推理过程</h2>
<p>训练完成后，实际使用中开始进行推理。首先encoder部分要输入一个中文句子，还是经过encoder生成编码矩阵C。然后是decoder部分，一开始输入一个BOS，通过自回归的方式逐token生成，渐变的颜色就表示token生成的不同次序。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:02:08.png" alt="encoder-decoder架构的推理过程（以机器翻译为例）" style="zoom:80%;" />
<p>这里有一个问题需要明确一下，每次decoder部分的输入，是一个token呢，还是前面的所有token呢？比如要预测“a”这个token，那么此时decoder的输入是“have”这个token呢，还是BOS+“I”+“have”这三个token都输入呢？其实都可以，</p>
<ul>
<li>
<p>比如每次decoder部分的输入是前面所有的token（比如BOS+“I”+“have”这三个token都输入），那么Masked MHA、MHA部分的Q、K、V都有多行（比如现在是三行），最终decoder出来的这个矩阵，我们只需要最后一个向量（下面红框的这个向量），进行linear+softmax+采样，得到next token</p>
  <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:39:09.png" alt="image-20240905153902066" style="zoom:50%;" />
</li>
<li>
<p>也可以每次decoder部分的输入是一个token（比如预测“a”这个token，只输入“have”这个token），那么此时就需要将之前token的K、V向量缓存下来（即KV-Cache），一个Q向量和concat(当前的KV向量，之前缓存下来的KV向量)算注意力，Masked MHA、MHA部分的attn-score只有一行，然后最终decoder出来的矩阵也只是一个向量，直接进行linear+softmax+采样，得到next token</p>
</li>
</ul>
<p>采用kv-cache的方式，可以在推理过程中减少很多不必要的运算，而且从显存占用来看，kv-cache也是划算的。如果不使用kv-cache，那么每次都要输入之前生成的所有token，Q、K、V都是完整的矩阵；如果使用kv-cache，K、V显存占用与之前相同，Q只是一个行向量而原来只是一个多行的矩阵，使用kv-cache的显存占用相对于不使用kv-cache的峰值显存占用是更少的。虽然说“相对峰值显存更少”，但是需要留意的是，kv-cache还是很占显存的，尤其是大batch_size和长序列的情况下，后面产生了MQA、GQA等优化，这是后话了。</p>
<p>这样看来，似乎在很早以前就有kv-cache的概念，但是似乎在LLM中才真正被普遍应用起来，我想有这么几个原因（not for sure, hope your discussion）：</p>
<ol>
<li>之前可能更关注模型架构的改进，之前encoder-decoder架构、encoder-only架构、decoder-only架构没有一个特别突出的模型，直到GPT-3（decoder-only）的出现，掀起了LLM的时代，现在的LLM基本上都是decoder-only的架构</li>
<li>在推理场景中，之前的模型上下文长度（或者叫最长序列长度）比较小，计算强度没那么大（但是比如语音流式识别等场景中也会用到kv-cache，不是很确定），LLM时代的transformer上下文长度普遍较大，而且往往是chat场景，对延迟有要求，因此要想法设法减少计算</li>
</ol>
<h1 id="encoder-only的代表bert">encoder-only的代表：Bert</h1>
<p>Bert是典型的encoder-only架构，其训练和推理过程很相似，训练怎么训，往往推理就是直接一个前向的过程。现在LLM基本都是生成式任务，encoder-only的架构总是感觉很别扭，decoder-only架构就很自然。可以关注知乎问题：<a href="https://www.zhihu.com/question/588325646">为什么现在的LLM都是Decoder only的架构？</a></p>
<h1 id="llm时代">LLM时代</h1>
<h2 id="模型架构和训练过程-1">模型架构和训练过程</h2>
<p>在模型架构方面，GPT-3和Llama只是Decoder-only架构的两个典型代表，基本保持了Vanilla-Transformer-decoder的结构，但是中间很多地方做了改动，比如去掉了与encoder的cross-attn，在masked-MHA的输入QK前面在加上旋转位置编码RoPE，将LayerNomr调整为post-norm结构，MLP部分可能会进行一些调整等，如果仅仅是走一遍训练和推理的流程，那么不会有影响。</p>
<p>从LLM训练过程来看，预训练阶段与之前的语言模型基本一致（如下图，这里不讨论微调、RLHF等过程，更侧重工程和流程方面），都是next token predicate任务，没有了encoder部分，模型结构看起来更加简洁。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-06-23:26:59.png" alt="decoder-only架构的训练过程（以语言模型为例）"  /></p>
<h2 id="推理过程-1">推理过程</h2>
<p>Decoder-only架构的推理过程和训练过程基本保持了相同的形式，推理过程就是先输入prompt，然后自回归的逐token生成。这里提一下prompt，LLM可以认为是一种特殊的语言模型，语言模型通俗来说就是续写，但是LLM包含了in-context learning的能力，prompt可以认为是一个指令，一个问题，使得“续写”的内容能够反映prompt的意图。</p>
<p>这里我们可以尝试分析一下文章开头提出的疑惑：</p>
<ul>
<li>在encoder-decoder架构的机器翻译任务中，
<ul>
<li>训练过程：中文句子输入到encoder，decoder部分的训练就是语言模型</li>
<li>推理过程：中文句子输入到encoder，BOS输入到decoder，然后自回归的逐token进行生成</li>
</ul>
</li>
<li>在decoder-only架构的LLM中，
<ul>
<li>预训练过程：没有encoder，就是训练语言模型</li>
<li>推理过程：prompt输入到decoder（prefill阶段），然后自回归的逐token进行生成（decode阶段）</li>
</ul>
</li>
</ul>
<p>LLM推理过程分为prefill阶段和decode阶段，不仅仅是从推理过程上看起来可以分成两个阶段，更重要的是，这两个阶段的特点不同，性质不同，为了尽可能推理加速，才有必要分成两个阶段。</p>
<ul>
<li>prefill阶段：输入prompt，生成第一个token。由于prompt往往较长，或者实际使用中将多个prompt打成一个batch（小batch），以提高模型吞吐，所以这个阶段计算量较大。衡量该阶段的一个指标是首字延迟（TTFT，Time To First Token）。还有一个需要注意的是，为了减小decode阶段的计算量，prefill阶段在计算prompt的注意力机制的时候，会将K、V矩阵缓存下来，空间换时间，即kv-cache。</li>
<li>decode阶段：后续自回归的逐token进行生成的过程，直到生成一个终止符（或者达到长度限制）。该阶段中，每次自回归过程中，输入一个token，然后生成q、k、v向量，k、v向量更新到kv-cache中，然后q向量和矩阵的计算（gemv），计算量较小，访存逐渐称为bottleneck。为了提高计算强度，往往会将多个请求decoder阶段的计算组成一个大batch。衡量该阶段的一个指标是TPOT（Time Per Output Token，生成每个token的耗时）。</li>
</ul>
<p>上面TTFT和TPOT指标是针对streaming generate场景，如果是non-streaming generate场景，则还是使用经典的延迟（Latency，生成一个完整输出的耗时）、吞吐（Throughput，每秒可以生成几个完整的输出）作为指标。</p>
<p>reference and more reading：</p>
<p><a href="https://zhuanlan.zhihu.com/p/685706549">一些已成为LLM 推理引擎中事实标准的方法</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/696850285">大模型高效推理 I 推理技术框架总结</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/683359705">LLM推理到底需要什么样的芯片？（1）</a>  <a href="https://zhuanlan.zhihu.com/p/683908169">LLM推理到底需要什么样的芯片？（2)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/704408423">大模型推理原理&amp;流程详解</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>LLM时代的transformer参数量、计算量、激活值的分析</title>
      <link>https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/</link>
      <pubDate>Sat, 07 Sep 2024 14:43:19 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/</guid>
      <description>导读：本文可以看作是对分析transformer模型的参数量、计算量、中间激活、KV cache的详细说明 定性分析 GPU上都存了哪些东西 首先我</description>
      <content:encoded><![CDATA[<p>导读：本文可以看作是对<a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a>的详细说明</p>
<h1 id="定性分析">定性分析</h1>
<h2 id="gpu上都存了哪些东西">GPU上都存了哪些东西</h2>
<p>首先我们来从全局整体的角度看一看，在训练阶段GPU显存上都有哪些内容：</p>
<ul>
<li>Model States：模型训练过程中必须存储的states
<ul>
<li>params（下面有时也叫做weights）：模型参数，记参数量为$\Phi$</li>
<li>grads：模型梯度，梯度数量同参数量$\Phi$</li>
<li>optimizer states：Adam优化器中的momentum和variance，数量分别是$\Phi$，共$2\Phi$</li>
</ul>
</li>
<li>Residual States：模型训练过程中，中间临时的、动态产生的states
<ul>
<li>activation：中间激活值，这个部分可能在训练过程中占据很大一部分显存，下面会详细分析。但是激活值不是必须存储的，可以使用重计算（recompute，也叫做activation checkpoint），在反向算梯度的时候，再重新算一遍，当然计算增加了，时间换空间，实际使用中可以部分选择性的进行重计算。</li>
<li>temporary buffers：临时存储，比如cuda、nccl等临时申请的显存。</li>
<li>unusable fragment memory：内存碎片导致的内存浪费，比如在开启重计算（或者叫做activation checkpointing）的情况下，中间激活不持久保留，这部分显存不断申请、释放，中间可能带来大量的内存碎片</li>
</ul>
</li>
</ul>
<p>推理阶段就相对简单一些，最主要的是Model States中的params和Residual States中的activation。</p>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/618865052">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></p>
<h2 id="混合精度训练">混合精度训练</h2>
<p>上面只是列出了训练过程中，显存中存放的内容和保存的数值数量，但是实际训练过程中，为了节省显存，以及考虑到训练过程中间某些过程对精度不是特别敏感，所以中间有些部分会使用fp32，有些部分会使用fp16/bf16。下面以Megatron为例，简单分析混合精度训练的一个大致流程。</p>
<p>首先我们来看一下不使用混合精度训练的场景，数值精度全使用fp32，作为一个分析的baseline。具体过程是：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:18:21.png" alt="fp32精度训练" style="zoom: 40%;" />
<p>占用显存为：$4\Phi$（fp32 weights）+$4\Phi$（fp32 momentum）+$4\Phi$（fp32 variance）+$4\Phi$（fp32 grad）+fp32 activation（可能很大）=$16\Phi$ Bytes + fp32 activation（4代表fp32的4Bytes，2代表fp16/bf16的2Bytes）</p>
<p>如果使用fp16的混合精度训练（bf16应该也可以，但是实际Megatron有点不同，下面会提到），具体过程是：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:15.png" alt="fp16混合精度训练" style="zoom:50%;" />
<p>占用显存为：$4\Phi$（fp32 weights）+$4\Phi$（fp32 momentum）+$4\Phi$（fp32 variance）+$2\Phi$（fp16 grad）+$2\Phi$（fp16 scaled grad）+$4\Phi$（fp32 unscaled and cliped grad）+fp16 activation（可能很大）=$20\Phi$ Bytes + fp16 activation</p>
<p>需要说明的有两点：</p>
<ol>
<li>当fp16 scaled grad转为为fp32 unscaled and cliped grad后，fp16 scaled grad就没用了，但是此时Megatron中仍然保留着一份fp16 scaled grad，所以显存占用中这两部分都会计算在内，这也符合Megatron offical readme中的描述：</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:22.png" alt="image-20240907213340085" style="zoom:80%;" />
<ol start="2">
<li>
<p>注意到上面流程中多了一个scale/unscale的操作，这叫做“loss scaling”</p>
<p>​	在使用混合精度训练时，如果直接使用fp16的grad来更新fp16的梯度，一是会产生舍入误差（比如梯度很小，权重更新后，由于精度不够，累加上的lr * grad被舍入，权重没变，一句话来说就是<strong>大数吃小数</strong>），二是会产生梯度下溢（比如梯度过小，fp16范围不够，导致很小的梯度下溢成为0，而这样的小梯度占比很大，一句话来说就是<strong>下溢成0</strong>）。对于舍入误差，可以在更新权重时，将fp16的梯度转换为fp32，再更新fp32的权重，从而避免精度问题。对于梯度下溢，需要使用loss scale。</p>
<p>​	loss scale就是FWD计算出loss后，对loss放大若干倍，由于求导的链式法则，放大的若干倍同样会传导到fp16梯度，这样fp16梯度就不会产生梯度下溢。在更新权重时，将fp16的梯度转换为fp32，同时进行unscale。</p>
</li>
</ol>
<p>刚才说到bf16有一点点特殊，我们看相应的代码：（Megatron中的arguments.py）</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:49:42.png" alt="image-20240907214939077" style="zoom: 80%;" />
<p>注意到如果使用bf16，那么会强行设置accumulate_allreduce_grads_in_fp32=True，这与上面Megatron offical readme截图（Distributed Optimizer）表格中的第二行【bf16 param, fp32 grads】相对应。具体过程应该是（not for sure, hope for discuss）：</p>
<blockquote>
<p>accumulate_allreduce_grads_in_fp32：If true, do the gradient accumulation and communication in fp32. <a href="https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/distributed.html">from here</a></p>
<p>gradient accumulation：在若干次iteration中，每次都会反向得到一份梯度，将这若干次iteration得到的梯度进行累加、求平均，在最后一次iteration才更新权重。gradient accumulation与data parallel是等价的，gradient accumulation在时间维度上训练多个mini-batch，而data parallel在相同时间内将不同mini-batch放在不同的机器上训练，结果都是一样的。</p>
<p>参考：</p>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/595716023">聊聊梯度累加(Gradient Accumulation)</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/650710443">梯度累积算法</a></p>
</li>
<li>
<p><a href="https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation">Hugging Face:Performing gradient accumulation with 🤗 Accelerate </a></p>
</li>
</ul>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:37.png" alt="bf16混合精度训练" style="zoom:50%;" />
<p>这里找到一个为什么要将bf16与accumulate_allreduce_grads_in_fp32绑定的<a href="https://github.com/NVIDIA/Megatron-LM/issues/372">issue</a>，里面提到“We found this to lead to more stable training before, but you could also try to perform the all-reduce in <code>bf16</code> (it might hurt convergence but will be faster).”</p>
<p>参考：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/618865052">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/662700424">图解大模型训练系列之：Megatron源码解读3，分布式混合精度训练</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training">NVIDIA Docs Hub：Train With Mixed Precision</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/441591808">全网最全-混合精度训练原理</a></li>
</ul>
<h1 id="量化分析">量化分析</h1>
<h2 id="transformer结构详解">transformer结构详解</h2>
<p>LLM中的transformer一般是decoder-only结构，所以下面的transformer block主要是decoder，但是与Vanilla Transformer中的decoder不同的是，这里没有了cross-attn，因此结构看起来反而有点像encoder（但不是，因为有casual mask）。</p>
<p>下面图中的Transformer，没有上kv-cache、GQA等优化，这部分后面会分析。其中，参数量$\Phi$表示有多少个参数；中间激活值$A$的单位是Bytes，主要参考的是<a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a></p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-15:58:31.png" alt="transformer详细分析" style="zoom:150%;" />
<p>在<a href="https://arxiv.org/pdf/2205.05198">Reducing Activation Recomputation in Large Transformer Models</a> 4.1节中也对transformer激活值进行了一个分析，但是该论文中，self-attention block部分softmax之前没有加mask，上图中添加了mask，具体在Attention部分stage SA_3，其中mask由于是整个transformer共享的，所以就省略了，$QK^T$的乘积被mask原地修改，所以$wbas^2$也省略了，这样激活值与原论文中仍然是一样的。</p>
<h2 id="kv-cache对参数量计算量激活值的影响">KV cache对参数量、计算量、激活值的影响</h2>
<p>关于KV Cache的来龙去脉，<a href="https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/">Encoder Decoder和decoder Only架构训练和推理浅析</a>中简单捋了一下。简单来说，kv cache在推理过程中使用，而且模型只能是decoder-only架构。由于自回归的方式逐token生成，self-attention部分必须使用casual mask，因此Q矩阵部分只需要计算最新token的q向量即可，K、V矩阵部分只需要拼接新token的k、v向量即可：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-14:04:56.png" alt="kv_cache" style="zoom: 50%;" />
<p>上面又重新回顾了一下kv cache。首先kv cache不会对参数量有影响，kv cache主要是用来减少不必要的计算的，显存因此也可能有相应的减少，上面只是一个示意图，中间省略了一些部分，详细的量化分析见下图，需要说明的有两点：</p>
<ol>
<li>kv cache使用场景是推理场景，LLM推理分为prefill阶段和decode阶段，prefill阶段创建kv-cache，decode阶段更新kv-cache。在输入prompt的这个prefill阶段中，with kv-cache和without kv-cache的计算量是相同的（显存占用由于分配kv-cache，可能with kv-cache会更多一点）。计算量的减少主要体现在decode阶段，因此下面的分析主要是针对单次decode阶段的，因此固定$s==1$</li>
<li>下图中说的“相对于原来“指的是without kv-cache时，每次都输入之前所有的token，计算完整的attention-score方阵，因而此时的序列长度$s=s_n \le s_m$。在最终分析时，取最大值$s=s_m$进行比较，对应decode阶段的最后一个token的生成过程，有的博客可能会将输入序列长度（prompt长度）和输出序列长度分开，这里合起来了，注意区别。</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-16:10:01.png" alt="transformer详细分析（kv cache）" style="zoom:150%;" />
<table>
<thead>
<tr>
<th></th>
<th>原来（without kv-cache）</th>
<th>现在（with kv-cache）</th>
<th>变化</th>
</tr>
</thead>
<tbody>
<tr>
<td>参数量</td>
<td>$2Vh+(12h^2+13h)l$</td>
<td>$2Vh+(12h^2+13h)l$</td>
<td>不变</td>
</tr>
<tr>
<td>中间激活</td>
<td>$2bsh+(34bs_mh+5bas_m^2)l$</td>
<td>$2bsh+(30bh+4bs_mh+5bas_m)l$</td>
<td>减少了$(30bh(s_m-1)+5bas_m(s_m-1))l$，原来中间激活是最长序列长度$s_m$的二次方，现在随着$s_m$线性增长</td>
</tr>
<tr>
<td>计算量</td>
<td>$(24h+4s_m)bs_mhl+2bs_mhV$</td>
<td>$(24h+4s_m)bhl+2bhV$</td>
<td>减少了$(24h+4s_m)bhl(s_m-1)+2bhV(s_m-1)$，原来计算量是最长序列长度$s_m$的二次方，现在随着$s_m$线性增长</td>
</tr>
</tbody>
</table>
<p>code:  from <a href="https://zhuanlan.zhihu.com/p/667763542">【手撕LLM-KVCache】显存刺客的前世今生&ndash;文末含代码</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># author: xiaodongguaAIGC</span>
</span></span><span class="line"><span class="cl"><span class="c1"># KV-Cache + Generation + decoder </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">LlamaModel</span><span class="p">,</span> <span class="n">LlamaConfig</span><span class="p">,</span> <span class="n">LlamaForCausalLM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">D</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># single-head-dim</span>
</span></span><span class="line"><span class="cl"><span class="n">V</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># vocab_size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xiaodonggua_kv_cache</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">V</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">V</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>     
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Wk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>     
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Wv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">V</span><span class="p">)</span> <span class="c1"># LM_head</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># initial</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Q</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="bp">self</span><span class="o">.</span><span class="n">Wk</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="bp">self</span><span class="o">.</span><span class="n">Wv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input_Q:&#34;</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input_K:&#34;</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;input_V:&#34;</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Easy KV_Cache</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span> <span class="c1"># first time</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">=</span> <span class="n">K</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span> <span class="o">=</span> <span class="n">V</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span><span class="p">,</span> <span class="n">V</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span>
</span></span><span class="line"><span class="cl">            <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_V</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;cache_K:&#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;cache_V:&#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_K</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># ignore proj/MLP/scaled/mask/multi-head when calculate Attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span><span class="n">Q</span><span class="nd">@K.transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="nd">@V</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># output</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">xiaodonggua_kv_cache</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="c1"># 创建数据、不使用tokenizer</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Generation </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> step input_shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">：&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">next_token</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>reference and more reading：</p>
<p><a href="https://blog.csdn.net/weixin_65514978/article/details/141399339">【大模型理论篇】Transformer KV Cache原理深入浅出</a></p>
<p><a href="https://juejin.cn/post/7362789570217885759#heading-3">大模型推理优化技术-KV Cache</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/686183300">一文读懂KVCache</a></p>
<h2 id="mqa和gqa对显存占用的影响">MQA和GQA对显存占用的影响</h2>
<p>在实际推理场景中，kv-cache已经是默认的选项。但是kv-cache是很占显存的，占用显存为$2 w_{kv} b s_m (a h_a) l$（其中$h=a * h_a$），后面会有case study分析。针对kv cache的各种优化层出不穷，下面的参考中有几篇博客总结了一下对kv cache的各种优化，简单来说，从上面的显存分析入手，有以下几种优化方法：</p>
<ul>
<li>针对attention 窗口（或者叫做context，上下文，或者当作最长序列长度$s_m$）$s_m$的优化，比如window attention，sparse attention，StreamingLLM</li>
<li>针对注意力头$a$的优化，比如MQA，GQA共享kv-cache（sharing）</li>
<li>针对层数$l$的优化，比如YOCO层间共享kv-cache（sharing）</li>
<li>针对精度$w_{kv}$的优化，比如kv-cache采用int8量化</li>
<li>针对内存分配的优化，减少内存碎片等，比如PagedAttention</li>
<li>其他优化。。。</li>
</ul>
<p>其中MQA/GQA在LLM中广泛使用，比如Llama2中就使用到了GQA。下面简单分析一下。</p>
<p>GQA方法很简单，原来MHA中每个q向量对应一个k向量和v向量，进行attention计算；现在好几个q向量对应（或者说共享）一个k向量和v向量，这“好几个q向量”构成一组，一共有g组，每组就有$\frac{a}{g}$个q向量。如果g=1，那么就是MQA，a个q向量构成一组，共享一个k、v向量；如果g=a，那么就是MHA，每个q向量构成一组，对应一个k、v向量。实际场景中，往往g=8，比如推理场景中单卡放不下，正好单机八卡，每张卡对应一组q向量。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-16:40:24.png" alt="image-20240908164016647" style="zoom: 67%;" />
<p>虽然MQA/GQA是针对推理过程中kv-cache的优化，但是在训练中也能用，也能省显存。下面对GQA在推理场景中的使用（with kv_cache）进行一个量化分析。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-17:24:56.png" alt="image-20240908172449500" style="zoom:150%;" />
<p>因为GQA只影响self-attention计算部分，因此其他部分省略，下面的表格也是只分析这个变化的部分。可以看出，由于kv-cache在长序列的情况下会占用很多显存，GQA针对中间激活的优化与序列长度相关，实际上GQA对中间激活的优化就是将kv-cache变为原来的$\frac{g}{a}$倍。</p>
<table>
<thead>
<tr>
<th></th>
<th>原来（MHA）-现在（GQA）</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>参数量</td>
<td>$\left [3(h^2+h) \right ]l - \left [ (\frac{2g}{a}+1)(h^2+h) \right ]l=2(1-\frac{g}{a})(h^2+h)l$</td>
<td></td>
</tr>
<tr>
<td>中间激活</td>
<td>$\left [ wbsh+2w_{kv}bs_mh \right]l - \left [ wbsh + 2w_{kv}bs_mh \times\frac{g}{a} \right ]l = 2w_{kv}bs_mhl(1-\frac{g}{a})$</td>
<td>尤其当长序列（$bs_m$较大），大模型（$hl$较大）时，前面系数较大，整体激活减少比较可观</td>
</tr>
<tr>
<td>计算量</td>
<td>$\left [ 6bsh^2 \right ]l - \left [ 2bsh^2 (\frac{2g}{a}+1) \right ] l = 4bsh^2l(1-\frac{g}{a}) \overset{s=1}{=} 4bh^2l(1-\frac{g}{a}) $</td>
<td></td>
</tr>
</tbody>
</table>
<p>在训练场景中，同样给出量化分析。需要说明的是，上述分析是在推理场景+kv_cache+GQA的情况下进行的分析，下面公式是针对的是训练场景+GQA。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-17:47:01.png" alt="transformer训练场景分析（GQA）"  /></p>
<p>code： from <a href="https://zhuanlan.zhihu.com/p/717838262">MHA，MQA，GQA注意力</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GroupedQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span> <span class="o">=</span> <span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># attention weights</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># n == num_heads or num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, n, head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">num_groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">//</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len, head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">merge_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param x: (batch_size, num_heads, seq_len, head_dim)
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># (batch_size, seq_len, num_heads, head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># ( batch_size, seq_len, embed_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">causal_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 分割注意力头</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意力计算</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># causal mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">attn_weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">causal_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq_len</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">mask_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 归一化</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 合并注意力头</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merge_heads</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">attn_output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>参考：</p>
<p><a href="https://zhuanlan.zhihu.com/p/685853516">大模型百倍推理加速之KV cache篇</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/659770503">LLM（二十）：漫谈 KV Cache 优化方法，深度理解 StreamingLLM</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/697311739">[KV Cache优化]🔥MQA/GQA/YOCO/CLA/MLKV笔记: 层内和层间KV Cache共享</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/708120479">大模型推理加速：KV Cache 和 GQA</a></p>
<h2 id="case-study">case study</h2>
<p>我们以GPT和Llama为例，进行case study。</p>
<h3 id="关于参数量的分析">关于参数量的分析</h3>
<h4 id="gpt-3">GPT-3</h4>
<p>GPT-3模型结构就大致上面【transformer结构详解】中的结构，但是多了一个可学习的position embedding，包含$n_{ctx} * h$个参数，其中$n_{ctx}=2048$，rectified这一列是加上这些参数后的参数量。</p>
<table>
<thead>
<tr>
<th>params</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>V <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">from GPT-2</a></th>
<th>calculated params=$Vh+(12h^2+13h)l$</th>
<th>rectified</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 Small: 125M</td>
<td>768</td>
<td>12</td>
<td>64</td>
<td>0.5M</td>
<td>50257</td>
<td>123651840  $\approx$ 123.7M</td>
<td>125224704  $\approx$ 125.2M</td>
</tr>
<tr>
<td>GPT-3 Medium: 350M</td>
<td>1024</td>
<td>24</td>
<td>64</td>
<td>0.5M</td>
<td>50257</td>
<td>353772544 $\approx$353.8M</td>
<td>355869696 $\approx$ 355.9M</td>
</tr>
<tr>
<td>GPT-3 Large: 760M</td>
<td>1536</td>
<td>24</td>
<td>96</td>
<td>0.5M</td>
<td>50257</td>
<td>757151232 $\approx$ 757.1M</td>
<td>760296960 $\approx$ 760.3M</td>
</tr>
<tr>
<td>GPT-3 2.7B</td>
<td>2560</td>
<td>32</td>
<td>80</td>
<td>1M</td>
<td>50257</td>
<td>2646305280 $\approx$ 2.64B</td>
<td>2651548160 $\approx$ 2.65B</td>
</tr>
<tr>
<td>GPT-3 6.7B</td>
<td>4096</td>
<td>32</td>
<td>128</td>
<td>2M</td>
<td>50257</td>
<td>6650007552 $\approx$ 6.65B</td>
<td>6658396160 $\approx$ 6.67B</td>
</tr>
<tr>
<td>GPT-3 13B</td>
<td>5140</td>
<td>40</td>
<td>128</td>
<td>2M</td>
<td>50257</td>
<td>12942401780 $\approx$ 12.94B</td>
<td>12952928500 $\approx$ 12.95B</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>3.2M</td>
<td>50257</td>
<td>174579068928 $\approx$ 174.58B</td>
<td>174604234752 $\approx$ 174.60B</td>
</tr>
</tbody>
</table>
<blockquote>
<p>说明：</p>
<ol>
<li>GPT-3词表大小V在论文中没找到，所以用的GPT-2的词表大小，这里论文中是提到的</li>
</ol>
<p>more relative reading：</p>
<ul>
<li><a href="https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters">How does GPT-3 spend its 175B parameters?</a></li>
</ul>
</blockquote>
<h4 id="llama-1-llama--open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971">Llama 1: <a href="https://arxiv.org/pdf/2302.13971">LLaMa:  Open and Efficient Foundation Language Models</a></h4>
<p>模型结构：<a href="https://huggingface.co/docs/transformers/model_doc/llama">from hugging face transformers LLaMA</a></p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-22:35:04.png" alt="llama1" style="zoom: 40%;" />
<p>论文中说，该模型与Vanilla Transformer有三处区别：</p>
<ol>
<li>
<p>Pre-normalization and RMSNorm</p>
 <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-04-22:22:22.png" alt="image-20240904222219836" style="zoom: 50%;" />
<p>​	原始Transformer中使用post-norm居多，后来使用pre-norm居多，而且往往在FFN之前也加一个norm。尤其在大模型中，可能在通过LN之后MHA之前，Q和K还要加上旋转位置编码。</p>
<blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/474988236">【重新了解Transformer模型系列_1】PostNorm/PreNorm的差别</a></p>
</blockquote>
</li>
<li>
<p>SwiGLU activation function</p>
<p>SwiGLU激活函数不太像传统的ReLU等激活函数那样简单，比如ReLU都不带参数，而SwiGLU乍一看上去不明觉厉，实际上将SwiGLU理解成对传统FFM的替换，感觉更合适一些。直接看公式有点懵，看图更容易理解，下面是FFM和SwiGLU的对比</p>
 <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-23:03:55.png" alt="SwiGLU" style="zoom: 50%;" />
<p>SwiGLU写成公式就是$SwiGLU(x) = \left [ SiGU \left( gate_proj(x) \right) \odot up_proj(x)  \right] \times down_proj(x)$，其中可能有点困惑的是这个$\frac{8h}{3}$是怎么来的，实际上就是为了左右这两个结构的参数量相等：$2 \times h \times 4h \equiv 2 \times h \times \frac{8h}{3} + \frac{8h}{3} \times h$</p>
</li>
<li>
<p>Rotary Embedding</p>
</li>
</ol>
<p>下面是模型配置，验证一下前面推出来的参数量相关的公式能否对上：</p>
<table>
<thead>
<tr>
<th>params</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>calculated params=$2Vh+(12h^2+13h)l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>6.7B</td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_7b.yaml">11008</a></td>
<td>6706298880  $\approx$ 6.71B</td>
</tr>
<tr>
<td>13.0B</td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_13b.yaml">13824</a></td>
<td>12913254400  $\approx$ 12.91B</td>
</tr>
<tr>
<td>32.5B</td>
<td>6656</td>
<td>60</td>
<td>52</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_30b.yaml">17920</a></td>
<td>32328857600 $\approx$ 32.33B</td>
</tr>
<tr>
<td>65.2B</td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_65b.yaml">22016</a></td>
<td>64957317120 $\approx$ 64.96B</td>
</tr>
</tbody>
</table>
<p>每次总是差一点，但是差的不多，差在了哪里呢？MLP部分，理论上intermediate_size=$\frac{8h}{3}$，但是实际上可能会比这个值大一些，往往向上取到256、512、1024等的倍数，对矩阵乘法性能更好，因此来修正一下参数量、计算量、激活值的量化分析：</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-16:15:19.png" alt="transformer详细分析(llama)"  /></p>
<p>重新计算一下，这次参数量就很接近了</p>
<table>
<thead>
<tr>
<th>params</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>calculated params=$2Vh+(4h+4+3I)hl$</th>
</tr>
</thead>
<tbody>
<tr>
<td>6.7B</td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_7b.yaml">11008</a></td>
<td>6738673664  $\approx$ 6.74B</td>
</tr>
<tr>
<td>13.0B</td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_13b.yaml">13824</a></td>
<td>13016268800  $\approx$ 13.02B</td>
</tr>
<tr>
<td>32.5B</td>
<td>6656</td>
<td>60</td>
<td>52</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_30b.yaml">17920</a></td>
<td>32529735680 $\approx$ 32.53B</td>
</tr>
<tr>
<td>65.2B</td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>4M</td>
<td>32K</td>
<td><a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_65b.yaml">22016</a></td>
<td>65286963200 $\approx$ 65.29B</td>
</tr>
</tbody>
</table>
<h4 id="llama-2-llama-2-open-foundation-and-fine-tuned-chat-modelshttpsarxivorgpdf230709288">Llama 2: <a href="https://arxiv.org/pdf/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></h4>
<p>Llama2在模型结构方面与Llama1相差不大，只是将MHA替换为GQA，将attention的context length从2k提升到4k。下面是Llama2的模型配置</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>MHA or GQA</th>
<th>calculated params=$2Vh+(12h^2+13h)l$</th>
<th>calculated params=$2Vh+(4h+4+3I)hl$</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B, <a href="https://gitee.com/hf-models/Llama-2-7b-hf/blob/main/config.json">config</a></td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>4M</td>
<td>32K</td>
<td>11008</td>
<td>MHA</td>
<td>6706298880  $\approx$ 6.71B</td>
<td>6738673664  $\approx$ 6.74B</td>
</tr>
<tr>
<td>13B, <a href="https://gitee.com/hf-models/Llama-2-13b-hf/blob/main/config.json">config</a></td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>4M</td>
<td>32K</td>
<td>13824</td>
<td>MHA</td>
<td>12913254400  $\approx$ 12.91B</td>
<td>13016268800  $\approx$ 13.02B</td>
</tr>
</tbody>
</table>
<p>至于70B的<a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a>（h=8192, l=80, a=64, b=4M, V=32K, intermediate_size=28672, g=8）使用了group=8的GQA，只有attention部分的参数量会发生一些变化，调整公式后，分别计算一下：</p>
<ul>
<li>calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$ = 5556092928 $\approx$ 55.56B，相差较大</li>
<li>llama calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$ = 68977950720 $\approx$ 68.98B，比较接近了</li>
</ul>
<p>因此，对于transformer而言，</p>
<ul>
<li>如果MLP是传统FFN那样的结构，calculated params=$2Vh+(12h^2+13h)l$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$</li>
</ul>
</li>
<li>如果MLP是SwiGLU那样的结构，calculated params=$2Vh+(4h+4+3I)hl$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</li>
</ul>
</li>
</ul>
<p>但是总的来说，transformer的复杂度还是$O(h^2l)$级别的</p>
<blockquote>
<p>more relative reading：</p>
<p><a href="https://medium.com/@saratbhargava/mastering-llama-math-part-1-a-step-by-step-guide-to-counting-parameters-in-llama-2-b3d73bc3ae31">“Mastering Llama Math (Part-1): A Step-by-Step Guide to Counting Parameters in Llama-2”</a></p>
<p><a href="https://bitddd.blog.csdn.net/article/details/132161203">LLM - Transformer &amp;&amp; LLaMA2 结构分析与 LoRA 详解</a></p>
</blockquote>
<h4 id="llama-3-the-llama-3-herd-of-modelshttpsarxivorgpdf240721783">Llama 3: <a href="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd of Models</a></h4>
<p>Llama3的改进相对于Llama2和Llama1，主要体现在使用了更高质量的数据和更大规模的训练，模型结构基本没变。下面是模型配置，</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>V</th>
<th>intermediate_size</th>
<th>GQA group</th>
<th>calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>8B, <a href="https://gitee.com/hf-models/llava-llama-3-8b-hf/blob/main/config.json">config</a></td>
<td>32</td>
<td>4096</td>
<td>32</td>
<td>4M-&gt;8M-&gt;16M</td>
<td>128K</td>
<td>14336</td>
<td>8</td>
<td>8028422144 $\approx$ 8.03B</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Meta-Llama-3-70B/blob/main/config.json">config</a></td>
<td>80</td>
<td>8192</td>
<td>64</td>
<td>4M-&gt;8M-&gt;16M</td>
<td>128K</td>
<td>28672</td>
<td>8</td>
<td>70550814720 $\approx$ 70.55B</td>
</tr>
<tr>
<td>405B</td>
<td>126</td>
<td>16384</td>
<td>128</td>
<td>4M-&gt;8M-&gt;16M</td>
<td>128K</td>
<td>53248</td>
<td>8</td>
<td>405849112576 $\approx$ 405.85B</td>
</tr>
</tbody>
</table>
<p>参考：</p>
<p><a href="https://blog.csdn.net/weixin_54338498/article/details/135269411">LLaMa-1/2/3 原理+源码——拆解 (KV-Cache, RoPE, RMSNorm, GQA, SwiGLU)</a></p>
<h3 id="关于激活的分析">关于激活的分析</h3>
<p>前面总说中间激活可能很占显存，我们来分析几个case。</p>
<p>GPT-3</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>s</th>
<th>V <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">from GPT-2</a></th>
<th>activation $\approx (34bsh+5bas^2)l$</th>
<th>activation （with GQA）$\approx \left [  (28+\frac{4g}{a})bsh+5bas^2\right]l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 Small: 125M</td>
<td>768</td>
<td>12</td>
<td>64</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>15972.0MB  $\approx 67.0 \times 2\Phi$</td>
<td>15873.0MB $\approx 66.58 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Medium: 350M</td>
<td>1024</td>
<td>24</td>
<td>64</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>32352.0MB $\approx 48.5 \times 2\Phi$</td>
<td>32088.0 $\approx 48.1 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Large: 760M</td>
<td>1536</td>
<td>24</td>
<td>96</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>48528.0 MB  $\approx 33.5 \times 2\Phi$</td>
<td>48120.0MB $\approx 33.2 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 2.7B</td>
<td>2560</td>
<td>32</td>
<td>80</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>55.3GB $\approx 11.0 \times 2\Phi$ wrong</td>
<td>54.4GB $\approx 10.82 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 6.7B</td>
<td>4096</td>
<td>32</td>
<td>128</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>88.5GB  $\approx 7.10 \times 2\Phi$</td>
<td>87.1GB $\approx 6.98 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 13B</td>
<td>5140</td>
<td>40</td>
<td>128</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>113.3GB $\approx 4.68 \times 2\Phi$</td>
<td>111.1GB $\approx 4.59 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>1</td>
<td>2048</td>
<td>50257</td>
<td>316.5GB $\approx 0.97 \times 2\Phi$</td>
<td>303.6GB $\approx 0.93 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>8</td>
<td>2048</td>
<td>50257</td>
<td>2532.0GB $\approx 7.77 \times 2\Phi$</td>
<td>2428.5GB $\approx 7.45 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>12288</td>
<td>96</td>
<td>128</td>
<td>64</td>
<td>2048</td>
<td>50257</td>
<td>19.78TB $\approx 62.14 \times 2\Phi$</td>
<td>18.97TB $\approx 59.60 \times 2 \Phi$</td>
</tr>
</tbody>
</table>
<p>Llama-2：</p>
<table>
<thead>
<tr>
<th>config</th>
<th>h</th>
<th>l</th>
<th>a</th>
<th>b</th>
<th>s</th>
<th><a href="https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig">V</a></th>
<th>intermediate_size</th>
<th>GQA: group</th>
<th>activation （with GQA）$\approx \left [  (13+\frac{4g}{a})bsh+5bas^2 + 6bsI\right]l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B, <a href="https://gitee.com/hf-models/Llama-2-7b-hf/blob/main/config.json">config</a></td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>1</td>
<td>4096</td>
<td>32K</td>
<td>11008</td>
<td>32(MHA)</td>
<td>96.6GB $\approx 7.4 \times 2\Phi$</td>
</tr>
<tr>
<td>13B, <a href="https://gitee.com/hf-models/Llama-2-13b-hf/blob/main/config.json">config</a></td>
<td>5120</td>
<td>40</td>
<td>40</td>
<td>1</td>
<td>4096</td>
<td>32K</td>
<td>13824</td>
<td>40(MHA)</td>
<td>150.9GB $\approx 6.2 \times 2\Phi$</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a></td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>1</td>
<td>4096</td>
<td>32K</td>
<td>28672</td>
<td>8</td>
<td>486.25GB $\approx 3.7 \times 2\Phi$</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a></td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>8</td>
<td>4096</td>
<td>32K</td>
<td>28672</td>
<td>8</td>
<td>3890.0GB $\approx 29.8 \times 2\Phi$</td>
</tr>
<tr>
<td>70B, <a href="https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json">config</a></td>
<td>8192</td>
<td>80</td>
<td>64</td>
<td>64</td>
<td>4096</td>
<td>32K</td>
<td>28672</td>
<td>8</td>
<td>30.39TB $\approx 238.7 \times 2\Phi$</td>
</tr>
</tbody>
</table>
<blockquote>
<p>由于前面分析过，intermediate_size往往会略微大于$\frac{8h}{3}$，因此根据前面分析的llama结构，重新推导一下激活的计算公式，这里省略了。</p>
</blockquote>
<p>可以看出，当大batch、长序列的情况下，中间激活可以是模型参数所占显存的很多倍，即使使用了GQA。</p>
<p>上面都是在训练场景下的激活值分析，在推理阶段中，可以使用kv-cache减少模型计算量，同时中间激活也大幅度减少，kv-cache的大小为$2w_{kv}bs_mh$（单层），我们也来量化分析一下（假设$w_{kv}$=2，且s=1，推理context长度最后一个token的情况，即最坏情况）</p>
<table>
<thead>
<tr>
<th>config</th>
<th>b</th>
<th>$s_m$</th>
<th>h</th>
<th>a</th>
<th>l</th>
<th>kv_cache size=$2w_{kv}bs_mhl$</th>
<th>without kv-cache activation$\approx (34bs_mh+5bas_m^2)l$</th>
<th>with kv-cache activation $\approx (30bh+4bs_mh+5bas_m)l$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 Small: 125M</td>
<td>1</td>
<td>2048</td>
<td>768</td>
<td>64</td>
<td>12</td>
<td>72MB $\approx 0.30 \times 2\Phi$</td>
<td>15972.0MB $\approx 67.0 \times 2\Phi$</td>
<td>79.8MB $\approx 0.33 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Medium: 350M</td>
<td>1</td>
<td>2048</td>
<td>1024</td>
<td>64</td>
<td>24</td>
<td>192MB $\approx 0.29 \times 2\Phi$</td>
<td>32352.0MB $\approx 48.5 \times 2\Phi$</td>
<td>207.7MB $\approx 0.31 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 Large: 760M</td>
<td>1</td>
<td>2048</td>
<td>1536</td>
<td>96</td>
<td>24</td>
<td>288MB $\approx 0.20 \times 2\Phi$</td>
<td>48528.0MB $\approx 33.5 \times 2\Phi$</td>
<td>311.6MB $\approx 0.21 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 2.7B</td>
<td>1</td>
<td>2048</td>
<td>2560</td>
<td>80</td>
<td>32</td>
<td>640MB $\approx 0.12 \times 2\Phi$</td>
<td>55.3GB $\approx 11.0 \times 2\Phi$</td>
<td>667.3MB $\approx 0.13 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 6.7B</td>
<td>1</td>
<td>2048</td>
<td>4096</td>
<td>128</td>
<td>40</td>
<td>1280MB $\approx 0.1 \times 2\Phi$</td>
<td>110.6GB $\approx 8.9 \times 2 \Phi$</td>
<td>1334.7MB $\approx 0.1 \times 2 \Phi$</td>
</tr>
<tr>
<td>GPT-3 13B</td>
<td>1</td>
<td>2048</td>
<td>5140</td>
<td>128</td>
<td>96</td>
<td>3.76GB $\approx 0.15 \times 2\Phi$</td>
<td>272.0GB $\approx 11.2 \times 2\Phi$</td>
<td>3.89GB $\approx 0.16 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>1</td>
<td>2048</td>
<td>12288</td>
<td>128</td>
<td>96</td>
<td>9.0GB $\approx 0.02 \times 2\Phi$</td>
<td>316.5GB $\approx 0.97\times 2\Phi $</td>
<td>9.15GB $\approx 0.03 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>8</td>
<td>2048</td>
<td>12288</td>
<td>128</td>
<td>96</td>
<td>72.0GB $\approx 0.22 \times 2\Phi$</td>
<td>2532.0GB $\approx 7.77 \times 2\Phi$</td>
<td>73.2GB $\approx 0.22 \times 2\Phi$</td>
</tr>
<tr>
<td>GPT-3 175B</td>
<td>64</td>
<td>2048</td>
<td>12288</td>
<td>128</td>
<td>96</td>
<td>576.0GB $\approx 1.77 \times 2\Phi$</td>
<td>19.78TB $\approx 62.1 \times 2\Phi$</td>
<td>585.6GB $\approx 1.80 \times 2\Phi$</td>
</tr>
</tbody>
</table>
<p>可以看出在推理时，kv-cache大幅度减少了中间激活。而且使用了kv-cache以后，kv-cache在激活中占据了绝大部分的比例，kv-cache甚至可以超过模型所占内存。</p>
<h3 id="关于计算量的分析">关于计算量的分析</h3>
<p>量化分析模型的计算量，主要是为了预估模型训练时间。根据前面的分析，一个FWD+BWD的iteration训练过程中，计算量FLOPs=$6 \times \Phi \times 输入tokens数量$，因此可以大致估计训练时间=$\frac{6 \times \Phi \times 输入tokens数量}{GPU数量\times GPU算力(flops) \times MFU}$。</p>
<h2 id="其他说明">其他说明</h2>
<h6 id="1-layernorm的计算">1. LayerNorm的计算</h6>
<p>LayerNorm的计算过程见<a href="https://blog.csdn.net/weixin_39228381/article/details/107939602">pytorch LayerNorm参数详解，计算过程</a>，总结一下就是：</p>
<ol>
<li>比如输入是<code>[b,s,h]</code>，LN的<code>normalized_shape=[h]</code>，此时就是对每一个大小为<code>h</code>的向量分别进行归一化（一共<code>b*s</code>个）</li>
<li>然后如果LN的<code>elementwise_affine=True</code>，就需要对每个大小为<code>h</code>的向量elementwise的乘上$\gamma: [h]$，再elementwise的加上$\beta:[h]$，$\gamma$和$\beta$就是该LN层的两个可学习的参数。如果LN的<code>elementwise_affine=False</code>，则只会进行第一步的归一化，不会进行第二步的affine</li>
</ol>
<p>一个有趣的问题是，<a href="https://zhuanlan.zhihu.com/p/707778968">Transformer中的LayerNorm可以并行吗？</a></p>
<p>关键词： Welford online Algorithm，当一个集合新增加一个元素$x_N$的时候，可以通过前N-1个样本的corrected sum of squares($\sum_{i=1}^{N-1}(x_i-\bar{x})^2$)，计算出前N个样本的corrected sum of squares，从而只需要one pass就可以完成LN的计算（之前navie的方法是two pass）</p>
<h6 id="2-关于dropout的位置">2. 关于dropout的位置</h6>
<p>一共（可能）在有四个地方有dropout：</p>
<ol>
<li>在PositionalEmbedding中有一个dropout：<code>dropout(x + PositionEmbedding(x))</code>，不过好像LLM现在使用旋转位置编码RoPE多一些，在计算attention之前在Q和K上加上RoPE，一开始输入的embedding不加PositionalEmbedding了</li>
<li>在softmax计算得到的attention score之后有一个droput：$dropout( softmax(\frac{QK^T}{scale}+casual_mask) )$</li>
<li>在sublayer（Attention和MLP）计算完之后，各有一个dropout：<code>x+dropout(sublayer(norm(x)))</code></li>
</ol>
<h1 id="总结">总结</h1>
<p>transformer的参数量的复杂度是$O(h^2l)$级别的，粗略估计可以认为是$12h^2l$或者$(4h+3I)hl$，如果要详细分析，就要看一看每个部分的结构，是否使用了bias，使用的不同优化，比如：</p>
<ul>
<li>如果MLP是传统FFN那样的结构，calculated params=$2Vh+(12h^2+13h)l$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$</li>
</ul>
</li>
<li>如果MLP是SwiGLU那样的结构，calculated params=$2Vh+(4h+4+3I)hl$
<ul>
<li>如果attention部分使用了GQA，则calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</li>
</ul>
</li>
</ul>
<p>对transformer中间激活的分析要分训练场景和推理场景</p>
<ul>
<li>在训练场景中，中间激活可以是模型参数所占显存的很多倍，尤其在大batch、长序列的情况下。
<ul>
<li>中间激活值所占显存粗略估计可以认为是$(34bsh+5bas^2)l$或者$(17bsh+5bas^2+6bsI)l$，可以看出与输入token数量（batch和seq_len）、隐藏层维度、头数、intermediate_size、层数相关，因此相对参数量的分析稍微复杂一点。</li>
</ul>
</li>
<li>在推理场景中，prefill阶段基本同训练场景，decode阶段每次输入的序列长度为1，而且默认使用kv-cache。由于使用kv-cache，中间激活相对于训练时的中间激活大幅度减小，但是在大batch、长序列的情况下，kv-cache的显存占用仍然可能超过模型参数的显存占用。还有一点需要注意，推理场景中kv-cache在中间激活中占据了绝大部分。
<ul>
<li>中间激活值所占显存粗略估计可以认为是$(30bh+4bs_mh+5bas_m)l$或者$(13bh+4bs_mh+5bs_ma+6bI)l$</li>
</ul>
</li>
</ul>
<p>对transformer的计算量的分析比较简单，transformer中计算较为规整，计算量体现在若干个大块矩阵的乘法。一般量化分析计算量主要是为了预估模型训练时间，所以一般分析的不多（一般也没有机会训练大模型，如果训练普通规模的网络，尝试跑几个iteration就能估计）。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>flash_attention简要笔记</title>
      <link>https://qinganzhang.github.io/posts/flash_attention%E7%AE%80%E8%A6%81%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 07 Sep 2024 14:07:19 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/flash_attention%E7%AE%80%E8%A6%81%E7%AC%94%E8%AE%B0/</guid>
      <description>本文中有较多Latex数学公式，博客上有一些数学公式格式渲染不正确，可以查看flash_attention简要笔记 优化效果 原来，attent</description>
      <content:encoded><![CDATA[<p>本文中有较多Latex数学公式，博客上有一些数学公式格式渲染不正确，可以查看<a href="https://blog.csdn.net/weixin_44343319/article/details/142318098">flash_attention简要笔记</a></p>
<h1 id="优化效果">优化效果</h1>
<p>原来，attention部分的计算量和中间激活占用显存的复杂度都是$O(N^2)$</p>
<blockquote>
<p>计算量部分原来QK矩阵乘和attn_score@V矩阵乘的计算量，复杂度都是$O(N^2)$；中间激活因为中间有一个attn_score，所以复杂度也是$O(N^2)$</p>
</blockquote>
<p>现在，attention部分的中间激活占用显存的复杂度变为$O(N)$，计算量的复杂度没有变但是通过减少访存加快了计算速度，而且fa与原attention完全等价</p>
<h1 id="具体过程">具体过程</h1>
<p>flash-attention还是基于kernel融合的思想，将QK矩阵乘法、mask、softmax、dropout合并成一个kernel，这样不仅减少了中间变量对显存的占用，而且也减少了计算过程中的访存</p>
<p>一些符号表示：</p>
<ul>
<li>
<p>$S_{ij}=Q_i \times K_j^T$，Q分块和K分块的乘积，形状为$[B_r, B_c]$</p>
</li>
<li>
<p>$\widetilde{m}<em>{ij}=rowmax(S</em>{ij})$：对分块$S_{ij}$而言，得到其每行的最大值，形状为$[B_r, 1]$</p>
</li>
<li>
<p>$\widetilde{P}<em>{ij}=e^{S</em>{ij}-\widetilde{m}<em>{ij}}=e^{S</em>{ij}-rowmax(S_{ij})}$：每个分块$S_{ij}$减去其局部rowmax $\widetilde{m}_{ij}$，形状为$[B_r, B_c]$</p>
</li>
<li>
<p>$\widetilde{l}<em>{ij}=rowsum(\widetilde{P}</em>{ij})=rowsum(e^{S_{ij}-rowmax(S_{ij})})$：对$\widetilde{P}_{ij}$而言，按行求和，形状为$[B_r, 1]$</p>
</li>
<li>
<p>$m^{new}<em>i=max(\widetilde{m}</em>{i0}, \widetilde{m}<em>{i1}, &hellip; , \widetilde{m}</em>{ij})=rowmax(concat(S_{i0}, S_{i1}, &hellip; , S_{ij}))$：即$contcat(S_{i0}, S_{i1}, &hellip; , S_{ij})$这j+1个分块的每行的最大值，形状为$[Br, 1]$</p>
</li>
<li>
<p>$m_i$：$m_i^{new}$位于SRAM上，将$m_i^{new}$写回到HBM就是$m_i$，初始化$m=-\infty$</p>
</li>
<li>
<p>$l^{new}<em>i=e^{m_i-m_i^{new}}l_i + e^{\widetilde{m}</em>{ij}-m_i^{new}} \widetilde{l}<em>{ij}=rowsum[e^{S</em>{00}-max(\widetilde{m}<em>{00},&hellip;,\widetilde{m}</em>{0j})}] + &hellip; + rowsum[e^{S_{0j}-max(\widetilde{m}<em>{00},&hellip;,\widetilde{m}</em>{0j})}]$：</p>
</li>
<li>
<p>$l_i$：$l_i^{new}$位于SRAM上，将$l_i^{new}$写回到HBM就是$l_i$，初始化$l=0$</p>
</li>
</ul>
<p>如果不使用flash-attention，具体过程为：</p>
<ol>
<li>$S = Q K ^T $</li>
<li>$P = softmax(S+mask)$</li>
<li>$O = P V$</li>
</ol>
<p>如果使用flash-attention，前向过程为：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-16-23:13:58.png" alt="image-20240916230932000" style="zoom: 40%;" />
<p>大致过程为：</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-01:47:02.jpg" alt="FA" style="zoom:100%;" />
<ol>
<li>首先对QKV进行分块，K、V分块方法相同（V的分块图中没画出来），首先可以计算$S_{ij}=Q_i\times K_j^T$。因为对QKV进行了分块，所以每次SRAM上能保留$S_{ij}$和$\widetilde{P}_{ij}$（橙黄色表示存储在SRAM上；橙红色表示虽然也存储在SRAM上，但是这些部分每次outer loop会写回到HBM中）</li>
<li>如果有mask，此时对$S_{ij}$进行mask</li>
<li>使用一个局部变量$\widetilde{m}<em>{ij}$和一个全局变量$m$（或者说$m^{new}$，$m^{new}$的值在SRAM上，但是每次outer loop会写回到HBM中）来记录分块$S</em>{ij}$局部rowmax和中间遍历过的分块$S_{i:}$的历史rowmax</li>
<li>然后基于分块$S_{ij}$计算局部的safe softmax的分子部分，即$e^{S_{ij}-rowmax(S_{ij})}$，safe softmax的分子部分累加就是分母部分，这样，就得到了一个针对分块$S_{ij}$的、局部的safe softmax的分母$\widetilde{l}<em>{ij}$，和  一个  遍历过的历史分块$S</em>{i:}$的  safe softmax分子部分的   累加和$l^{new}$（注意断句，写公式有点晦涩难懂，用语言描述又不太好描述），局部的$\widetilde{l}<em>{ij}$就是用来更新全局的$l$（或者说$l^{new}$，$l^{new}$的值在SRAM上，但是每次outer loop会写回到HBM中），对$\widetilde{l}</em>{ij}$举一个例子：
<ul>
<li>当j=0，i=0时，$l_0^{new}=e^{m_0-m_0^{new}} l_0+e^{\widetilde{m}<em>{00}-m_0^{new}} \widetilde{l}</em>{00}=\widetilde{l}_{00}$</li>
<li>当j=1，i=0时，$l_0^{new} = rowsum(e^{S_{00}-max⁡(\widetilde{m}<em>{00}, \widetilde{m}</em>{01})})+rowsum(e^{S_{01}-max⁡(\widetilde{m}<em>{00}, \widetilde{m}</em>{01})})$</li>
</ul>
</li>
<li>然后对$\widetilde{P}_{ij}$进行dropout</li>
<li>然后相当于要进行$O+=\widetilde{P}_{ij} V_i$了，对于算法的第15行，可以使用分配律拆开看，其中有两个操作：
<ol>
<li>后半部分：对于当前的$\widetilde{P}<em>{ij} V_i$相乘，$\widetilde{P}</em>{ij}$中减去的是分块$S_{ij}$局部的rowmax，需要调整到  此时已经见过的、所有分块$S_{i:}$的rowmax，就是第15行后半部分中$e^{\widetilde{m}_{ij}-m_i^{new}}$的意思</li>
<li>前半部分：调整上一次的$O$，先乘旧的$l_i$恢复到safe softmax的分子部分，然后乘以$e^{m_i-m_i^{new}}$更新一下safe softmax分子部分中减去的全局rowmax，最后再除以当前的safe softmax的分母</li>
</ol>
</li>
</ol>
<p>（反向过程还是看别的博客吧）</p>
<h1 id="简要分析">简要分析</h1>
<p>首先分析一下fa的FLOPs（只分析大块的矩阵乘法，其他小的操作就不计算了）：</p>
<ul>
<li>一开始的$Q_i K^T_j$矩阵相乘，其中$Q_i$的形状为$[B_r, d]$，$K_j^t$的形状为$[d, B_c]$，此时FLOPs=$2d \times B_r \times B_c$</li>
<li>后面计算O的时候有一个$\widetilde{P}<em>{ij} V_i$矩阵相乘，其中$\widetilde{P}</em>{ij}$的形状为$[B_r, B_c]$，$V_i$的形状为$[B_c, d]$，此时FLOPs=$2B_c \times B_r \times d$一共进行了$\frac{N}{B_r} \times \frac{N}{B_c}$次上面的循环，所以FLOPs=$4N^2d$，如果d远小于N，则计算复杂度就变成了$O(N^2)$，计算复杂度相比于standard attention没有变化</li>
</ul>
<p>然后再分析一下显存占用（显存占用说的是HBM上的显存占用，假设计算精度为$w$ Bytes）</p>
<ul>
<li>HBM上需要维护一个全局的rowmax和expsum，占用显存为$w\times N$</li>
<li>然后还要存储一个最后的输出$O$，占用显存为$wNd$，但是这个部分是必须的</li>
<li>因此，显存占用的复杂度为$O(Nd)$（或者$O(N)$，如果不考虑$O$的话）。standard attention需要保存中间的$S, P$，显存占用复杂度为$O(N^2)$</li>
</ul>
<p>fa相对于standard attention一个优势，在于减小了计算过程中的访存量，最后来分析一下访存次数：</p>
<ul>
<li>standard attention
<ul>
<li>从HBM中读取Q，K（形状都是$[N, d]$），访存量=$wNd$，计算$S=QK^T$，然后向HBM中写回S（形状为$[N, N]$），访存量=$wN^2$</li>
<li>从HBM中读取S，访存量=$w N^2$，计算$P=softmax(S)$，向HBM中写回P，访存量=$w N^2$</li>
<li>从HBM中读取P（形状为$[N, N]$）、V（形状为$[N, d]$），访存量=$w N^2 + wNd$，计算$O=PV$，向HBM中写回O（形状为$[N, d]$），访存量=$wNd$</li>
<li>总的访存量=$w(3Nd+4N^2)$，如果d远小于N，则访存量的复杂度变成了$O(N^2)$</li>
</ul>
</li>
<li>flash attention（分析时将inner loop作为一个整体进行分析，就像上面示意图画的那样）
<ul>
<li>从HBM中读取分块$Q_i, i=0, &hellip;, T_r -1$，读取分块$K_j$，访存量=$w(Nd+B_c d)$；后面$S_{ij}, \widetilde{P}_{ij}$不需要写回HBM；$m, l$只是一个向量，数据量很少，忽略；再后面读取和写入分块$O_i, i = 0, &hellip;,T_r =1$，访存量=$w(2\times Nd)$</li>
<li>outer loop共有$\frac{N}{B_c}=T_c$次，总的访存量=$w\times \frac{N}{B_c} \times (Nd + B_cd + 2Nd)=w(Nd+\frac{3N^2d}{B_c})=w(T_c+1)Nd$</li>
<li>比如N=1024，d=64，B=64，standard_attention访存量-flash_attention访存量=$w(3Nd+4N^2-Nd-\frac{3N^2d}{B_c})=w(2Nd+(4-\frac{3d}{B_c})N^2)=w(2Nd+N^2)$，可以看出少了很多访存</li>
</ul>
</li>
</ul>
<h1 id="实际使用">实际使用</h1>
<h2 id="接口返回值">接口返回值</h2>
<p>flash-attention开源代码中，针对不同qkv、是否是varlen、是否需要kv_cache等不同需求封装了不同的<a href="https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/flash_attn_interface.py">接口</a>，这里说一下返回值。这些接口的返回值都相同，除了返回输出的$O$之外，如果设置了<code>return_attn_probs=True</code>，还会返回softmax_lse和S_dmask：</p>
<ul>
<li>softmax_lse（形状$[nheads, seqlen]$）：在计算$S=\frac{QK^T}{scale}$之后，会得到形状为$[bs, seqlen, seqlen]$的方阵S，在计算softmax的过程中，需要按行求和，得到一个列向量，然后再取log，写成表达式即为：$softmax_lse=log[\sum_je^{S_{ij}}]$，注意不是$softmax_lse=log[\sum_je^{S_{ij}-rowmax(S_{ij})}]$，参考issue：<a href="https://github.com/Dao-AILab/flash-attention/issues/404">What&rsquo;s the exactly formula of <code>softmax_lse</code>? #404</a></li>
<li>S_dmask（形状$[bs, nheads, seqlen, seqlen]$）：就是返回$P=softmax(\frac{QK^T}{scale}+mask)$的这个P矩阵</li>
</ul>
<h2 id="varlen-attention">varlen attention</h2>
<p>特别的，这里再说一下<code>flash_attn_varlen_func</code>等一些支持varlen的接口，其函数形参中还有<code>cu_seqlens_q</code>、<code>cu_seqlens_k</code>、<code>max_seqlen_q</code>、<code>max_seqlen_k</code>等特有的参数。这里介绍一些varlen是什么。</p>
<p>varlen即变长序列，产生的背景是”数据拼接“，即LLM使用的训练数据集中，长度较短的序列占大多数，这些短序列为了能够符合Transformer固定长度的输入，就要进行padding，序列越短，padding越多，而我们不太想要padding，padding只是无奈之举。此时，我们可以使用varlen特性，简单来说就是将多个短序列拼接成一个长序列，但是还是每个短序列自己内部计算注意力，短序列之间是隔离的，这样减少了padding，节省计算量和显存。</p>
<p>这里举个例子（<a href="https://xtuner.readthedocs.io/zh-cn/latest/acceleration/varlen_flash_attn.html">参考</a>），比如一些短序列长度分别是：70，300，180， &hellip;，260，120，1200，&hellip;等，attention固定输入长度是4096，此时我们将这些短序列拼接起来，使用varlen_attn后，就像右图所示，每个短序列自己内部计算attention，短序列之间不计算attention（否则就像左图这样，白白多了很多浪费的计算）</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-11:48:10.png" alt="XTuner" style="zoom: 40%;" />
<p>为了实现varlen特性，需要对接口有一些调整。比如不使用varlen的flash_attn接口中，传入的Q、K、V的形状一般为$[bs, seqlen, nheads, head_dim]$（K和V的nheads可以少于Q的nheads，此时就是GQA/MQA）。在使用varlen的flash_attn接口中，主要有两点变化：</p>
<ul>
<li>Q、K、V的形状一般为$[total_seq, nheads, head_dim]$，这里将多个batch拼接起来，拼起来的长度为$total_seq$</li>
<li>多了<code>cu_seqlens_q</code>、<code>cu_seqlens_k</code>、<code>max_seqlen_q</code>、<code>max_seqlen_k</code>等特有的参数
<ul>
<li><code>cu_seqlens_q</code>是对每个短序列的Q的长度的exclusive_scan，作用就是找到原来每个batch的起始点（offset），比如上面的例子，此时<code>cu_seqlens_q=[0, 70, 370, 550, ... ]</code>，如果<code>cu_seqlens_q</code>的形状为$[batch_size+1]$，则需要在最后拼接上序列Q的总长度</li>
<li><code>max_seqlen_q</code>好理解，就是短序列的Q的最长长度</li>
</ul>
</li>
</ul>
<p>在具体实现中，对每个序列的每个head分别launch kernel，来实现并行计算，这个过程中要通过<code>cu_seqlens_q</code>来确定对应Q的start_idx和end_idx。</p>
<p>参考：</p>
<p><a href="https://66ring.github.io/2024/05/31/universe/ml/flash_attn_varlen_batcing_api_usage/">Flash attention变长batching API使用</a></p>
<p><a href="https://github.com/Dao-AILab/flash-attention/issues/850">How did flash-attn compute attention for cu_seqlens #850</a></p>
<h1 id="参考">参考</h1>
<p><a href="https://zhuanlan.zhihu.com/p/669926191">图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑</a></p>
<p>优质好文：</p>
<p><a href="https://zhuanlan.zhihu.com/p/668888063">[Attention优化][2w字]🔥原理&amp;图解: 从Online-Softmax到FlashAttention V1/V2/V3</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>反向传播和自动微分简析</title>
      <link>https://qinganzhang.github.io/posts/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E7%AE%80%E6%9E%90/</link>
      <pubDate>Mon, 24 Jun 2024 11:43:44 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E7%AE%80%E6%9E%90/</guid>
      <description>[toc] 1.自动微分 1.1 初步封装 首先我们考虑函数的自动微分。函数是这样的：$y=f(x), l=L(y)$，其中x表示输入，y是中间变量，L相当于是损</description>
      <content:encoded><![CDATA[<p>[toc]</p>
<h1 id="1自动微分">1.自动微分</h1>
<h2 id="11-初步封装">1.1 初步封装</h2>
<p>首先我们考虑函数的自动微分。函数是这样的：$y=f(x), l=L(y)$，其中x表示输入，y是中间变量，L相当于是损失函数，l是损失函数计算出来的loss，大致模拟神经网络的结构。</p>
<p>从中可以抽象出两个部分，输入/输出的变量Variable，和中间计算的函数Function。</p>
<p>首先我们对Variable进行初步封装：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>	<span class="c1"># data是一个numpy的ndarray</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后对Function进行初步封装：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Function</span><span class="p">:</span>		<span class="c1"># 基类</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>	<span class="c1"># __call__方法将Variable类型的input中数据成员data取出来进行计算，计算完成后再返回Vaiabel</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>	<span class="c1"># 实际进行计算的函数</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="n">NotImplementError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="12-反向传播">1.2 反向传播</h2>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-05-14-11:22:11.png" alt="image-20240514112203666" style="zoom: 50%;" />
<p>观察前向传播和反向传播的流程图，可以看出Variable和Function中前向和反向中存在一定的对应关系，即Variable中需要同时保存数据成员和其激活值，Function中需要同时有forward和backward过程，因此可以拓展Variable和Function类：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Function</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="n">NotImplementError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>		<span class="c1"># gy是loss值l对y的导数值，backward中实现以下y对x的导数，返回相乘的结果</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="n">NotImplementError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="n">gx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">gy</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">F</span> <span class="o">=</span> <span class="n">Exp</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 但是这样只能手动实现反向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="13-自动反向传播">1.3 自动反向传播</h2>
<p>为了实现反向传播的自动化，一个思路是使用列表记录一下函数执行的顺序（但是对于有分支或多次使用同一个变量的计算图，需要使用Wengert列表（或者叫tape））。</p>
<p>另一个思路是将Variable分为两种类型，一种是用户给出的，另一种是由某个Function产生的，因此可以在Variable中记录其creator；在Function运行时实际执行的过程中，此时来记录局部的变量的creator信息，因为这个“连接”的信息是在forward前向传播的过程中记录，因此这种方式也称为Define-by-Run，所构造的计算图也称为动态计算图。</p>
<p>比如，对于例子$y=f(x)$而言，已知$\frac{dl}{dy}$，需要求出$\frac{dl}{dx}$。计算过程是：当前可以拿到输出Variable $y$，也可以产生这个Variable的Function $f=y.creator$，然后根据$f$获取其输入Variable $x=f.input$，就可以计算出x的梯度$x.grad=\frac{dl}{dy} \times \frac{dy}{dx} = f.backward(y.grad)$。这个计算过程是相同的，可以实现程序控制的自动反向传播，因此放在Variable类中，可以递归的来进行计算，也可以转换成迭代的方法进行计算</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span> <span class="c1"># data should be np.ndarray type </span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&#34;</span><span class="si">{}</span><span class="s2"> is not supported&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="n">func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">funcs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="n">funcs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">f</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">pop</span><span class="p">();</span> <span class="c1"># 因为现在针对的是函数的自动求导，所以funcs中总是只有一个元素</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">output</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">funcs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        # 递归版本
</span></span></span><span class="line"><span class="cl"><span class="s1">        f = self.creator
</span></span></span><span class="line"><span class="cl"><span class="s1">        if f is not None:
</span></span></span><span class="line"><span class="cl"><span class="s1">        	x = f.input
</span></span></span><span class="line"><span class="cl"><span class="s1">        	x.grad = f.backward(self.grad)
</span></span></span><span class="line"><span class="cl"><span class="s1">        	x.backward()
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">as_array</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Function</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">as_array</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="c1"># y经过forward之后未必是ndarray，比如x=np.array(1.0), y = x**2之后y是np.float64类型</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="o">.</span><span class="n">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="c1"># output保存creator信息</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">gy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">Exp</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="2计算图的自动微分">2.计算图的自动微分</h1>
<h2 id="21-多输入多输出的function实现">2.1 多输入、多输出的Function实现</h2>
<p>某些Function的输入可能有多个，比如Add，输出也可能有多个，比如separate，此时可以使用可变长参数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">as_array</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Function</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span> <span class="c1"># 针对多个输入的可变长参数</span>
</span></span><span class="line"><span class="cl">        <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">ys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">xs</span><span class="p">)</span>	<span class="c1"># 这里可以将xs整个列表传给forward，也可以将xs列表解包之后传给forward，分别对应的forward形参的不同形式，后一种形式更为直观和易用一些</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span> <span class="c1"># ys需要是tuple类型</span>
</span></span><span class="line"><span class="cl">            <span class="n">ys</span> <span class="o">=</span> <span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ys = self.forward(xs)</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Variable</span><span class="p">(</span><span class="n">as_array</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="o">.</span><span class="n">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">outputs</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span> <span class="c1"># 多个输入，一个输出</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">x1</span> 
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    def forward(self, xs): # xs是一个列表
</span></span></span><span class="line"><span class="cl"><span class="s1">        x0, x1 = xs
</span></span></span><span class="line"><span class="cl"><span class="s1">        return (x0 + x1, )
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>	<span class="c1"># 针对多个输入，要返回多个偏导数</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">Add</span><span class="p">()(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">gy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">square</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果某个Function的输入是多个，那么此时输出y就应该对每个输入求偏导数，Function中backward可能返回多个偏导数，此时Variable中的backward也应该进行一些修改：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not supported&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="n">func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">cleargrad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">funcs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="n">funcs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">f</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span> <span class="c1"># Bug：这里需要修改</span>
</span></span><span class="line"><span class="cl">            <span class="n">gys</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">outputs</span><span class="p">]</span> <span class="c1"># f的输出可能有多个</span>
</span></span><span class="line"><span class="cl">            <span class="n">gxs</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="o">*</span><span class="n">gys</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gxs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">gxs</span> <span class="o">=</span> <span class="p">(</span><span class="n">gxs</span><span class="p">,</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">gx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">gxs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">				<span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">gx</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="err">：</span>
</span></span><span class="line"><span class="cl">                	<span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">gx</span> <span class="c1"># x.grad += gx是inplace操作</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">funcs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="22-计算图的反向传播">2.2 计算图的反向传播</h2>
<p>为了实现反向传播，主要有两种思路。一种是对计算图进行一个拓扑排序，然后逆序进行反向传播。另一种是bfs的思路，记录一下当前Function和Variable是在哪一个level上，先取出后面level的进行反向传播。</p>
<p>这里采用第二种思路：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span><span class="lnt">90
</span><span class="lnt">91
</span><span class="lnt">92
</span><span class="lnt">93
</span><span class="lnt">94
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">heapq</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">weakref</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not supported&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="n">func</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">generation</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># Variable的generation是其creator的generation-1</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">cleargrad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">funcs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">heapq</span><span class="o">.</span><span class="n">heapify</span><span class="p">(</span><span class="n">funcs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">seen_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="n">funcs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">f</span> <span class="o">=</span> <span class="n">heapq</span><span class="o">.</span><span class="n">heappop</span><span class="p">(</span><span class="n">funcs</span><span class="p">)</span> <span class="c1"># 使用最大堆，每次弹出最大generation的creator</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># print(f.generation)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">input</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">inputs</span> <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">creator</span> <span class="ow">in</span> <span class="n">seen_set</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="n">heapq</span><span class="o">.</span><span class="n">heappush</span><span class="p">(</span><span class="n">funcs</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">seen_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gys</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">outputs</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">            <span class="n">gxs</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="o">*</span><span class="n">gys</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gxs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">gxs</span> <span class="o">=</span> <span class="p">(</span><span class="n">gxs</span><span class="p">,</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">gx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">gxs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">gx</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">gx</span> <span class="c1"># x.grad += gx是inplace操作        </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">as_array</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span>                    
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Function</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span> <span class="c1"># 针对多个输入的可变长参数</span>
</span></span><span class="line"><span class="cl">        <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">ys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">xs</span><span class="p">)</span>	<span class="c1"># 这里可以将xs整个列表传给forward，也可以将xs列表解包之后传给forward，分别对应的forward形参的不同形式，后一种形式更为直观和易用一些</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span> <span class="c1"># ys需要是tuple类型</span>
</span></span><span class="line"><span class="cl">            <span class="n">ys</span> <span class="o">=</span> <span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ys = self.forward(xs)</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Variable</span><span class="p">(</span><span class="n">as_array</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">generation</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">])</span> <span class="c1"># Function的generation是其多个输入中最大的generation</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="o">.</span><span class="n">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">        <span class="c1"># self.outputs = outputs</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">outputs</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span> <span class="c1"># 自定义排序，便于进行堆排序</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">func</span><span class="o">.</span><span class="n">generation</span> <span class="c1"># 因为heapq只能实现小根堆</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>    
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">gy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">Square</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span> <span class="c1"># 多个输入，一个输出</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">x1</span> 
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>	<span class="c1"># 针对多个输入，要返回多个偏导数</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">Add</span><span class="p">()(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span><span class="n">square</span><span class="p">(</span><span class="n">square</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span> <span class="n">square</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="23-一些优化措施">2.3 一些优化措施</h2>
<h3 id="针对循环引用的优化">针对循环引用的优化</h3>
<p>原来Function的outputs和Variable的creator相互引用，如果这两个对象都不使用了，此时没法通过引用技术来回收内存（可以通过垃圾回收机制GC释放内存，但是使用GC推迟内存释放会导致程序整体的内存使用量增加）。因此一个优化是打破这个循环引用，将Function的outputs设置为弱引用，弱引用是在不增加引用技术的情况下对另一个对象的引用。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-05-14-22:51:20.png" alt="image-20240514225118035"  /></p>
<p>具体修改是第61行和第33行</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">heapq</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">weakref</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not supported&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="n">func</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">generation</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># Variable的generation是其creator的generation-1</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">cleargrad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">retain_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">funcs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">creator</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">heapq</span><span class="o">.</span><span class="n">heapify</span><span class="p">(</span><span class="n">funcs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">seen_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="n">funcs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">f</span> <span class="o">=</span> <span class="n">heapq</span><span class="o">.</span><span class="n">heappop</span><span class="p">(</span><span class="n">funcs</span><span class="p">)</span> <span class="c1"># 使用最大堆，每次弹出最大generation的creator</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># print(f.generation)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">input</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">inputs</span> <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">creator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">creator</span> <span class="ow">in</span> <span class="n">seen_set</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="n">heapq</span><span class="o">.</span><span class="n">heappush</span><span class="p">(</span><span class="n">funcs</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">seen_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">creator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gys</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">()</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">outputs</span><span class="p">]</span> <span class="c1"># 如果output为弱引用，需要output()来获取其内容</span>
</span></span><span class="line"><span class="cl">            <span class="n">gxs</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="o">*</span><span class="n">gys</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gxs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">gxs</span> <span class="o">=</span> <span class="p">(</span><span class="n">gxs</span><span class="p">,</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">gx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">gxs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">gx</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">gx</span> <span class="c1"># x.grad += gx是inplace操作 </span>
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="ow">not</span> <span class="n">retain_grad</span><span class="p">:</span> <span class="c1"># 不保留中间变量的梯度</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">output</span><span class="p">()</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># 最终只有一开始的input的grad保留下来</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">as_array</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span>                    
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Function</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span> <span class="c1"># 针对多个输入的可变长参数</span>
</span></span><span class="line"><span class="cl">        <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">ys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">xs</span><span class="p">)</span>	<span class="c1"># 这里可以将xs整个列表传给forward，也可以将xs列表解包之后传给forward，分别对应的forward形参的不同形式，后一种形式更为直观和易用一些</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span> <span class="c1"># ys需要是tuple类型</span>
</span></span><span class="line"><span class="cl">            <span class="n">ys</span> <span class="o">=</span> <span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ys = self.forward(xs)</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Variable</span><span class="p">(</span><span class="n">as_array</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">generation</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">])</span> <span class="c1"># Function的generation是其多个输入中最大的generation</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="o">.</span><span class="n">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span> <span class="c1"># 将Function的outputs变为弱引用，防止循环引用</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># self.outputs = outputs</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">outputs</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span> <span class="c1"># 自定义排序，便于进行堆排序</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">func</span><span class="o">.</span><span class="n">generation</span> <span class="c1"># 因为heapq只能实现小根堆</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gy</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>    
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="减少内存占用的优化">减少内存占用的优化</h3>
<p>不保留不必要的导数</p>
<p>在深度学习和神经网络的反向传播中，更新的是参数的权重，我们需要求得参数的梯度，但是中间激活值的梯度信息可以用完即弃，不需要保存到内存中，比如下图中，只需要保存$\frac{dl}{da}$这个梯度信息（红色部分），其他的梯度信息都不需要保存到内存中。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-24-11:42:47.png" alt="image-20240514230625379" style="zoom:33%;" />
<h3 id="快速切换训练阶段和推理阶段">快速切换训练阶段和推理阶段</h3>
<p>一种方式是直接使用if语句进行判断enable_backprop的值；另一种更好的方法是使用with语句实现临时的状态切换。具体而言，使用contextlib模块下的装饰器<code>@contextlib.contextmanager</code>修饰一个函数（比如<code>using_config</code>），当进入到with块的作用域时，首先调用预处理的代码（修饰函数中yield之前是预处理的代码），当离开with块的作用域时，调用后处理的代码（yield之后是后处理的代码）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">contextlib</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">enable_backprop</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@contextlib.contextmanager</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">using_config</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">old_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">Config</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">setattr</span><span class="p">(</span><span class="n">Config</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">yield</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">setattr</span><span class="p">(</span><span class="n">Config</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">old_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">using_config</span><span class="p">(</span><span class="s1">&#39;enable_backprop&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># example: y = x^2, with only forward </span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="对variable的拓展和完善">对Variable的拓展和完善</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not supported&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">creator</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generation</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">set_creator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="o">...</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">cleargrad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="o">...</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">retain_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span> <span class="c1"># 使用@property装饰器进行修饰，shape方法可以作为属性（或者实例变量）被访问</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ndim</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   	
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="s2">&#34;Variable(None)&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">+</span><span class="s1">&#39; &#39;</span><span class="o">*</span><span class="mi">9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="s2">&#34;Variable(&#34;</span> <span class="o">+</span> <span class="n">p</span> <span class="o">+</span> <span class="s2">&#34;)&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>运算符重载</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">as_variable</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">obj</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Variable</span><span class="p">)</span> <span class="k">else</span> <span class="n">Variable</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    
    <item>
      <title>A survey of Efficient Transformer on Inference</title>
      <link>https://qinganzhang.github.io/posts/a_survey_of_efficient_transformer_on_inference/</link>
      <pubDate>Mon, 04 Mar 2024 20:57:14 +0800</pubDate>
      
      <guid>https://qinganzhang.github.io/posts/a_survey_of_efficient_transformer_on_inference/</guid>
      <description>Abstract Transformer模型架构在自然语言处理、计算机视觉、强化学习等领域表现出了强大的能力，已经成为当前深度学习很多模型的核心，当前发展迅</description>
      <content:encoded><![CDATA[<h2 id="abstract">Abstract</h2>
<p>Transformer模型架构在自然语言处理、计算机视觉、强化学习等领域表现出了强大的能力，已经成为当前深度学习很多模型的核心，当前发展迅速的大模型更加凸显出这一点。由于Transformer较高的复杂度，限制了其在很多场景中的应用。因此，为了提高模型的高效性，针对Transformer的改进层出不穷。本文从模型算法的角度出发，关注于模型推理的场景，从不同层次梳理当前提高模型效率的方法，包括设计复杂度更低的注意力机制、提出更加高效的网络设计、进行模型压缩和优化的方法，并针对每一种方法进一步做了分类和总结，并选取具有代表性的方法进行说明。本文最后探讨了Transformer未来可能的发展方向。</p>
<h1 id="1-introduction">1. Introduction</h1>
<p>近年来，深度学习发展迅速，尤其是以Transformer为核心的结构，构成了当前深度学习架构的核心，在计算机视觉、自然语言处理等领域，SOTA的模型均以Transformer架构为核心，而且当前诸如ChatGPT等大模型，核心同样是基于RLHF的Transformer，显示出了Transformer强大的能力。</p>
<p>但是，受限于Transformer相对于序列长度平方的计算复杂度，在图片、视频等需要长序列的场景下，相对于传统的CNN架构，Transformer仍不够有效，无法得到有效的应用。Transformer的平方复杂度来源于注意力机制，因此，许多研究关注于改进注意力机制，降低注意力机制的复杂度，提出新的注意力机制。除此之外，不同的Transformer架构被提出，这些架构在Vanilla Transformer架构上做出改进来提高计算和访存效率，这可以归结为efficient attention或efficient Transformer网络架构的设计。</p>
<p>除此之外，为了进一步降低Transformer模型的复杂度，提高模型的推理速度，efficient Transformer的网络架构还可以使用一些模型压缩的方法，比如剪枝、量化、蒸馏、神经架构搜索（NAS）等，这些方法可以在基本保持模型效果的同时，降低模型复杂度，减小模型大小，进一步加速模型的推理。</p>
<p>需要说明的是，efficiency是一个比较宽泛的用词，包括data-efficiency, model-efficiency（efficient architecture），training-efficiency，inference-efficiency。其中data-efficiency一般指充分利用、挖掘数据，从小规模数据中进行学习；model-efficiency侧重于降低模型的复杂度或是参数量；training-efficiency指使用更少的资源（或提高资源利用效率）、使用更少的时间来进行训练；inference-efficiency通常也被成为模型推理加速，它针对训练好的模型，尽可能提高模型的推理速度、吞吐量等。本综述中只涉及到model-efficiency，并介绍一些针对Transformer的模型压缩方法。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/613685663">Blog: Efficient Deep Learning 高效深度学习</a></p>
</blockquote>
<h1 id="2-model-efficiency">2. Model Efficiency</h1>
<p>Model efficiency主要侧重于提出新的架构，或者改善现有架构，从而降低模型复杂度或参数量。不同于训练场景只关注于模型的参数量，在推理场景中，模型在访存、计算等方面同样需要高效，2.1节说明了在推理场景中模型所关注的几种不同的efficiency。为了能够量化的来比较模型在推理时的efficiency，2.2节总结了一些评估模型推理性能的指标。</p>
<h2 id="21-kinds-of-efficiency">2.1 Kinds of Efficiency</h2>
<p>模型的高效是一个相对的概念，但是有几个发展方向是确定的，比如高效的模型一般具有一下几个特征：模型中存在较多的计算密集型算子而非访存密集型算子（有助于充分发挥硬件性能），模型计算复杂度尽量低（可以应用于更加广泛的场景），模型参数量尽量少（可以减少存储空间和内存的占用），受限于模型的结构、应用的场景，在应用中需要先对这几个方向进行分析，然后才能够做进一步的分析和优化。</p>
<h3 id="211-memory-efficiency">2.1.1 Memory Efficiency</h3>
<p>访问内存的开销是影响模型推理速度的一个关键因素。Transformer中许多操作，比如频繁的reshape，element-wise相加，归一化等操作，这些操作或算子是访存密集型的，即大部分时间花费在访存上，而计算耗时占比很小，此时模型推理速度主要受到内存带宽限制。减少模型推理过程在访存上的时间开销，就是提高memory efficiency。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/414430541">Blog: 深度学习模型大小与模型推理速度的探讨</a></p>
</blockquote>
<h3 id="212-computation-efficiency">2.1.2. Computation Efficiency</h3>
<p>模型的computation efficiency往往指的是模型的算法复杂度低。特别的，针对Transformer而言，当序列长度序列较小时，此时模型的计算开销主要集中在FFN模块，计算复杂度近似地线性于序列长度。但是在很多使用Transformer的场景中，比如图片、视频等场景中，输入序列长度较大。此时，模型的计算开销会集中于自注意力层，产生相对于序列长度平方的复杂度，限制了Transformer在很多场景中的应用。</p>
<blockquote>
<p><a href="https://0809zheng.github.io/2021/07/12/efficienttransformer.html">Blog: Efficient Transformers</a></p>
<p><a href="https://arxiv.org/pdf/1706.03762.pdf">Paper: Attention is all you need</a> 中FFN与Attention复杂度对比</p>
</blockquote>
<h3 id="213-parameter-efficiency">2.1.3 Parameter Efficiency</h3>
<p>Parameter Efficiency主要指的是模型的轻量化和较少的参数量。使用参数量较少的模型，可以减少模型在磁盘上存储的空间和模型加载后内存的占用。需要注意的是，随着大模型的发展，受限于大模型训练的成本，大模型的微调技术PEFT（Parameter-efficient fine-tuning）发展迅速。PEFT旨在最小化微调参数的数量和计算复杂度，以减少大模型微调的成本，来提高模型在新任务上的性能。这里所说的Parameter Efficiency更加类似于模型轻量化的概念。</p>
<h2 id="22-metrics">2.2 Metrics</h2>
<p>设计神经网络架构的主要考虑因素之一就是效果和成本的权衡。一般情况下，一个模型的参数量越多，计算量越大，模型的容量越大，该模型的效果就越好。但是，不同模型在不同硬件平台上的推理效果往往无法直接比较。因此，在比较模型推理性能时，经常会使用一些指标，从不同角度对模型的推理性能进行比较。</p>
<h3 id="221-计算量">2.2.1 计算量</h3>
<p>计算量是评价模型efficiency最常用的指标，包括很多文献进行对比时，常常会将计算量和参数量作为最重要的比较依据。计算量是模型所需的计算次数，模型的整体计算量等于模型中每个算子的计算量之和。衡量计算量主要有两个指标：</p>
<ul>
<li>
<p>FLOPs（Floating Point Operations，浮点计算次数）：计算量一般用OPs（Operations，计算次数）来表示，由于最常用的格式为float32，因此也常被写作为FLOPs。</p>
</li>
<li>
<p>MACs（Multiply-Accumulate Operations，乘加累计操作数）：1个MACs包括一个乘法操作与一个加法操作，大约相当于2FLOPs。在很多硬件上，Multiply-Accumulate可以使用单独一个指令完成，而且很多对tensor的操作也是Multiply-Accumulate操作。FLOPs通常用于模型的理论上计算量的分析，MACs更加贴近真实的计算量。</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation">Multiply–accumulate_operation from wiki</a></p>
</blockquote>
</li>
</ul>
<h3 id="222-参数量">2.2.2 参数量</h3>
<p>参数量是模型中参数的总和，直接反应了模型在磁盘中存储的大小。虽然参数量并不直接影响推理性能，但是参数量一方面会影响内存占用，另一方面会影响程序初始化时间。而且，在某些场景下，参数量是很重要的指标。比如在嵌入式或移动端场景下，磁盘空间极其有限，此时往往会对模型的参数量有比较严格的限制。在这种情况下，除了在设计时减少参数量，还可以通过压缩模型权重的方式进一步降低打包后模型的大小，但是这样会带来解压缩开销，会在一定程度上增加程序初始化的时间。</p>
<h3 id="223-访存量">2.2.3 访存量</h3>
<p>访存量往往是最容易被忽略的指标，但它对推理性能有着极大的影响。访存量是指模型推理时所需访问内存的数据量，反应了模型对存储带宽的要求。访存量有时也称作MAC（Memory Access Cost）或者MOPs（Memory Operations），一般用Bytes（或KM/MB/GB）来表示，即模型需要读取/写入多少Bytes的内存数据。和计算量一样，模型整体访存量等于模型各个算子的访存量之和。</p>
<h3 id="224-运行速度">2.2.4 运行速度</h3>
<p>运行速度是衡量模型efficiency最有效的指标，但是需要基于相同的硬件平台进行对比，而且，即使使用相同的硬件平台，使用不同的软件环境、使用流水线的效率等因素也对最终的推理速度有极大的影响，所以往往在实践中难以直接进行比较。运行速度主要有两种形式进行反应：</p>
<ul>
<li>吞吐量（Throughput）：在单位时间内处理的样本个数，相当于可以并行处理的任务量，充分利用流水线可以极大提高模型推理的吞吐量。</li>
<li>延迟（Latency）：通常指单个样本或单个batch处理完成的时间，相当于串行处理一个任务所需要的时间。相对于吞吐量，流水线无法减少延迟。因此，对于需要实时推理的模型而言，需要考虑延迟而非提高吞吐量。</li>
</ul>
<blockquote>
<p>[Paper: THE EFFICIENCY MISNOMER]</p>
</blockquote>
<p>需要注意的是，使用单个指标对模型进行评估往往会导致不全面的结论，甚至评价指标无法真实地比较模型在硬件上的推理速度。比如在下图中，相较于其他网络，在保持类似精确度的情况下，EfficientNet具有相对较小的计算量（GFLOPs）和参数量（Million Parameters），但是模型的推理速度并没有相对于其他模型很明显的提升，甚至有时其他模型推理速度更快一些。虽然如此，但是固定某些指标进行比较，仍是一个相对公平的方法。而且通过分析模型的推理瓶颈，可以针对性的提升模型的某些指标，从而加速推理。</p>
<img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-03-04-21:13:15.png" alt="image-20230620112102735" style="zoom:80%;" />
<blockquote>
<p><a href="">Paper: THE EFFICIENCY MISNOMER</a> Figure5</p>
</blockquote>
<h2 id="23-find-the-bottleneck">2.3 Find the Bottleneck</h2>
<p>不同的模型具有不同的特征，即使同一个模型的不同部分也有不同的特征，比如某些部分是计算密集性的，有些部分是访存密集型的，这里选取Bert和GPT-2两个典型的模型进行分析。</p>
<p>为了综合衡量计算密集型与访存密集型，通常使用算数强度（arithmetic intensity，也称计算密度，计算强度，计算访存比等）来表示。算数强度表示从内存加载的每个字节可以进行的浮点运算的数量，反映了程序相对于访存而言计算的密集程度，可以通过计算量FLOPs除以访存量来计算得到。RoofLine模型是基于算数强度，来评估程序在硬件上能达到性能上界的模型，即给定一个硬件资源的限制（算力、内存带宽），模型在该硬件上可以达到的最大计算速度。</p>
<p>当模型的计算密度较小时，访存相对较多，计算相对较少，模型性能主要受到内存带宽限制，此时模型是访存密集型的。反之如果模型的计算密度较大，访存相对较少，计算相对较多，模型性能主要受到硬件算力的限制，此时模型是计算密集型的。一般而言，模型的计算密度越大，越有可能提升硬件的计算效率，充分发挥硬件性能。对于访存密集型算子，推理时间跟访存量呈线性关系，而对于计算密集型算子，推理时间跟计算量呈线性关系。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/411522457">Blog: 深度学习模型大小与模型推理速度的探讨</a></p>
<p>[Paper: Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures]</p>
</blockquote>
<figure>
    <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-03-04-21:13:38.png" width=400/>
    <img src="https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-03-04-21:13:50.png" width=400/>
</figure>
<p>BERT是Encoder-only的模型，而GPT-2是Decoder-only的模型，如图a所示，这个区别导致两类模型的计算密度差异很大，而两种不同大小的BERT模型的计算密度差异反而不是很大。究其原因，是由于Decoder模型中，每次都是逐个token输入并解码，导致实际矩阵乘法退化为矩阵与向量的乘法，数据重用有限，使其更容易受到内存带宽的限制。因此，如图b所示，当使用高算力的硬件进行推理性能测试时，以BERT-Base的推理时间为基准，尽管相对于BERT-Base，GPT-2具有更少的计算量，但是由于访存量的激增，导致计算密度变低，最终在实际推理时，推理延时远远慢于BERT-Base。因此，针对模型进行优化时，需要综合不同的指标，分析模型的特点，找到模型的瓶颈，从而进行针对性的优化，才能对最终的推理性能有较大提升。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/619596323">Blog: LLM Inference CookBook</a></p>
<p>[Paper: Full Stack Optimization of Transformer Inference: a Survey] Figure 6 9</p>
</blockquote>
<h1 id="3-efficient-transformer">3. Efficient Transformer</h1>
<p>虽然当前很多SOTA模型都是基于Transformer，而且很多大模型也都是以Transformer为基础，但是由于Transformer相对于输入序列的平方的复杂度，使得在很多需要长序列的场景中，比如处理图片、视频时受到很大的限制，因此很多方法被提出来改善模型的复杂度，比如降低模型的时间复杂度，减少模型的参数量，设计更适合于硬件的模型来减少访存等。本章节从三个不同角度来讨论使得模型在设计上更加高效的方法。</p>
<h2 id="31-efficient-attention">3.1 Efficient Attention</h2>
<p>注意力机制作为Transformer的核心，它使得模型可以捕捉全局信息，进行长距离建模。但是注意力机制最核心的操作是进行矩阵相乘，由于词向量维数一般固定且不是很大，可以认为是常数，因此时间复杂度可以认为是输入序列长度的平方。本节讨论一些方法，侧重于改善注意力机制的时间复杂度，并根据核心思想进行分类和总结。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/263031249">Blog: Transformers大家族——Efficient Transformers: A Survey</a></p>
<p><a href="https://blog.csdn.net/weixin_44808865/article/details/119173304">Blog: 「ArXiv2020」【Efficient Transformers: A Survey】论文笔记（更新中）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/602394470">Blog: Efficient Transformers: A Survey</a></p>
<p><a href="https://blog.csdn.net/triplemeng/article/details/113821740">Blog: 进击的Transformer &mdash; 一文介绍Efficient Transformers</a></p>
</blockquote>
<ul>
<li>
<p>Fixed Patterns</p>
<p>将注意力机制从全局变为局部，限制注意力机制的范围，从而降低复杂度。根据限制的范围和形式，可以分为blockwise pattern， strided pattern，compressed pattern。</p>
<p>Blockwise pattern将输入序列切成多个block，只在每个block内部进行注意力机制的计算，显著降低了计算复杂度，比如Blockwise Attention、Local Attention等。但是这样简单的切割会导致序列不连贯，缺乏block之间的信息交互，注意力机制能力有限。虽然很简单，但是确实后续很多改进的基础。</p>
<p>Strided pattern采用滑动窗口的形式，每个token与周围相邻的几个token计算注意力，即按固定间隔进行注意力机制的计算。比如，Sparse Transformer使用类似strided形式的滑动窗口，LongFormer使用类似dilated形式的滑动窗口。相较于Blockwise pattern，考虑到自然语言很多情况下都是局部相关性较高，因此在一个窗口范围内计算注意力可能不会丢失太多信息。</p>
<p>Compressed pattern则是先通过卷积、池化等CNN操作进行下采样，从而有效减小序列长度，将输入序列转换到固定的模式，降低计算注意力机制的复杂度。</p>
<blockquote>
<p>Blockwise attn: <a href="https://arxiv.org/abs/1911.02972">Blockwise Self-Attention for Long Document Understanding</a></p>
<p>Local attn: <a href="https://arxiv.org/abs/1802.05751">Image Transformer</a></p>
<p>Sparse Trans: <a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a></p>
<p>LongFormer: <a href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer</a></p>
</blockquote>
</li>
<li>
<p>Combination of Patterns</p>
<p>对于输入的token，可以在不同维度、不同区域上组合使用不同的注意力机制，从而学习到更好的特征。比如，Sparse Transformer将一半的注意力头使用strided pattern，另一半注意力头使用local pattern。类似的，在Axial Tranformer中不是像多数注意力模块一样先将多维输入展平，而是每次沿着特征图的单个维度计算自注意力，然后组合多个维度的特征图以得到覆盖全局感受野的特征图。</p>
<blockquote>
<p>Axial Trans: <a href="https://arxiv.org/abs/1912.12180">Axial Attention in Multidimensional Transformers</a></p>
</blockquote>
</li>
<li>
<p>Learnable Patterns</p>
<p>Learnable pattern是对fixed pattern的拓展，fixed pattern是提前规定好一些区域，在这些区域中进行注意力，而learnable pattern则是引入可学习参数，让模型自己找到计算注意力的区域，即以数据驱动的方式指导模型的学习过程。比如Reformer引入基于哈希的相似度度量方法来将输入进行切割，Routing Transformer对token向量进行k-means聚类，从而将整体序列分割为多个子序列。因此，从最后注意力计算的角度看，Learnable pattern与fixed pattern是一致的，都是通过将整体序列进行切分，只在子序列中计算注意力，不同的只是子序列的划分方式是提前确定的还是模型学习得到的。</p>
<blockquote>
<p>Reformer: <a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a></p>
<p>Routing Trans: <a href="https://arxiv.org/abs/2003.05997">Efficient Content-Based Sparse Attention with Routing Transformers</a></p>
</blockquote>
</li>
<li>
<p>Neural Memory</p>
<p>Neural memory类似于compressed pattern中先压缩再计算注意力的想法，Set Transformer中第一次使用了这种方法。具体而言，就是初始化k个untrainable向量（k&laquo;n），n个token embedding和这k个trainable向量计算注意力，压缩得到k个向量，然后k个向量再和n个向量计算注意力还原得到n个向量，达到抽取输入序列特征的目的。这k个untrainable向量就可以理解为memory，用于处理临时上下文信息。</p>
<blockquote>
<p>Set Trans: <a href="https://arxiv.org/abs/1810.00825">Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</a></p>
</blockquote>
</li>
<li>
<p>Low-Rank</p>
<p>Low-rank通过矩阵压缩或矩阵近似来降低计算注意力的复杂度。假设$N$是序列长度，$d$是向量维度，$k$是矩阵压缩的超参数。在Linformer中观察到，经过softmax计算之后得到的$N \times N$的attention score矩阵是不满秩的，这意味着不需要一个完整的attention score矩阵，可以使用一个$N \times k$的矩阵来近似$N \times N$的attention score矩阵，同时需要将$N \times d$的key和value向量映射到$k \times d$维空间，由于$k$是固定的超参数，因此将注意力机制的复杂度降低到了线性级别。</p>
<blockquote>
<p>Linformer: <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear Complexity</a></p>
</blockquote>
</li>
<li>
<p>Kernels</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/157490738">Blog: 线性Attention的探索：Attention必须有个Softmax吗？</a></p>
<p>Linear Trans: <a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a></p>
</blockquote>
<p>之前的一些研究中提到制约注意力机制性能的关键因素是其中的softmax操作，Scaled-Dot Attention其实就是对value做加权平均，未来得到attention score，就必须先对query和key进行运算。但是，以核函数变换的形式可以得到一个更加通用的注意力机制的数学表达，通过将相似性度量拆分，可以实现注意力机制线性的复杂度（原来的相似度计算中，指数操作的存在使得query，key，value的矩阵操作无法使用结合律）。由于通过kernel方法计算得到的是注意力矩阵的一种近似形式，因此核方法也可以认为是一种特殊的low-rank方法。</p>
</li>
<li>
<p>Recurrence</p>
<p>Recurrence实际上也是fixed pattern中blockwise的一种延申，本质上仍是对输入序列进行区域划分， 只是它进一步对划分后的block做了一层训练连接，通过这样的层级关系就可以把一个长序列的输出得到更好的表征。Transformer-XL使用segment-level recurrence，将上一个segment的状态缓存下来，然后再计算当前segment的时候重复使用上一个的隐藏状态，虽然加快了推理速度，但是由于需要进行缓存，是一种空间换时间的方案。</p>
<blockquote>
<p>Transformer-XL: <a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p>
</blockquote>
</li>
</ul>
<h2 id="32-efficient-architecture-design">3.2 Efficient Architecture Design</h2>
<p>除了改善Transformer中注意力机制的复杂度，修改Transformer中其他部分可能同样有效。实际上，比如针对Bert-Base而言，从参数量的角度看，模型总的参数量约为104MB，其中多头注意力机制部分的参数量大约为27MB；从计算量和访存量的角度看，即使针对较长的序列而言，多头注意力机制部分也只是占了整个模型计算量/访存量的一半左右。因此，设计更加高效的网络架构，同样可以提高模型运行时的性能。同样，本节根据不同模型架构的设计思路和特点进行分类总结。</p>
<blockquote>
<p><a href="https://blog.csdn.net/qq_28385535/article/details/127213648">Blog: Hydra Attention: Efficient Attention with Many Heads翻译</a></p>
<p><a href="https://blog.csdn.net/huangblog/article/details/119639001">Blog: 一文懂“NLP Bert-base” 模型参数量计算</a></p>
<p>[Paper: Full Stack Optimization of Transformer Inference: a Survey] Table 3</p>
</blockquote>
<ul>
<li>
<p>增加感受野</p>
<p>通过增加感受野，模型可以处理更加高分辨率的图像，但同时需要尽量降低额外带来的计算量。Efficient-ViT使用MobileNetV2中的MBConv作为基本块，使用线性注意力机制替代传统注意力机制，并且在前馈神经网络中使用可变形卷积。EdgeNeXt与之相似，它使用分裂的深度转置注意力模块（Split Depth-wise Transpose Attention， SDTA）来替代传统的多头注意力机制，SDTA将输入通道分成多个通道组，利用深度可分离卷积和跨通道的自注意力来有效增加模型的感受野。</p>
<blockquote>
<p>[Paper: EfficientViT: Lightweight Multi-Scale Attention for On-Device Semantic Segmentation]</p>
<p>[Paper: EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications]</p>
<p>[Paper: MobileNetV2: Inverted Residuals and Linear Bottlenecks]</p>
</blockquote>
</li>
<li>
<p>使用池化层</p>
<p>通常在注意力机制之后使用池化层，来减少推理延迟。NextViT交替使用卷积块和注意力块，其中卷积块由多头卷积注意力和MLP构成，卷积块主要使用了多头自注意力机制，但是注意力机制中key和value都先经过了一个池化层。PoolFormer总结了一种成为MetaFormer的通用架构，通过使用不同的Token-Mixer可以获得不同的具体架构，当Token-Mixer被修改为一个简单的池化层时，PoolFormer以极少的参数同样获得了与其他模型相似的准确度。</p>
<blockquote>
<p>[Paper: Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios]</p>
<p>[Paper: MetaFormer Is Actually What You Need for Vision]</p>
</blockquote>
</li>
<li>
<p>使用局部特征</p>
<p>LeViT再次将充分使用CNN的局部特征，尤其是首先通过卷积来得到低分辨率的特征图，然后通过修改注意力模块进行特征图的下采样。MobileViT网络主要使用MobileViT块和MBConv块堆叠而成，其中MobileViT块负责进行全局信息与局部信息的交互，其中将特征图通过卷积层进行局部建模得到局部信息，然后将局部信息的特征图基于注意力机制进行全局建模，最后进行残差连接。</p>
<blockquote>
<p>[Paper: LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference]</p>
<p>[Paper: MobileViT: Light-Weight, General-Purpose, and Mobile-Friendly Vision Transformer]</p>
</blockquote>
</li>
<li>
<p>保持维度一致性</p>
<p>相对于多头注意力机制角度的计算量对于推理延迟的影响，特征维度一致性对推理延迟同样甚至有更大的影响，比如网络中存在大量低效的reshape操作，反复调整特征的维度，会极大影响推理的速度。EfficientFormer提出了一种维度一致性的设计，将网络分成一个特征图为四维的部分和一个特征图为三维的部分，网络从patch embedding开始，首先进入四维特征图部分，最后进入三维特征图部分。在四维特征图部分，主要通过卷积结构为主；在三维特征图部分，此时网络结构中加入注意力机制和MLP结构。最终四维和三维分区的长度是通过网络架构搜索得到的。</p>
<blockquote>
<p>[Paper: EfficientFormer: Vision Transformers at MobileNet Speed]</p>
</blockquote>
</li>
<li>
<p>并行网络</p>
<p>一些模型可以并行的执行特定的层，从而加快推理速度。比如Mobile-Former的两个并行分支分别提取局部和全局信息，通过双向桥接进行信息的双向融合。MixFormer基于并行分支设计，将局部自注意力和通道分离卷积两个分支进行交互，并且根据不同分支上操作共享参数的维度不同，使用双向交互模块融合不同维度的信息，针对每个分支提供互补的信息来进一步学习到更好的特征。</p>
<blockquote>
<p>[Paper: Mobile-Former: Bridging MobileNet and Transformer]</p>
<p>[Paper: MixFormer: Mixing Features across Windows and Dimensions]</p>
</blockquote>
</li>
</ul>
<h2 id="33-efficient-efforts">3.3 Efficient Efforts</h2>
<p>除了针对注意力机制和Transformer的架构进行改进，通用的模型压缩同样可以提高Transformer的推理性能，同时保持模型精度或将模型精度的下降控制在一个合理范围内。模型压缩主要包括剪枝、蒸馏、量化等。其中，剪枝和蒸馏可以减少模型参数量，量化可以提高模型的访存效率，而且不同的方法可以是正交的，即可以先进行模型的剪枝，再进行模型的量化。许多研究提出了不同的方法来进行Transformer模型的压缩，本节简单进行介绍。由于在自然语言处理领域和计算机视觉领域中，模型压缩的方法可能略有不同，本节更加侧重于视觉方面的模型压缩方法。</p>
<p>此外，由于Transformer的广泛应用，为了提高模型的推理性能，在设计模型架构时有时需要将硬件也纳入考虑，比如考虑到硬件限制的网络架构搜索，软硬件协同设计等，虽然本综述不涉及硬件的描述，但是本节最后介绍一种针对GPU的新型注意力机制FlashAttention，通过优化注意力机制算法的访存过程，来显著提高模型的运行速度、降低所需内存，同时保持对结果不变和对用户的透明。</p>
<h3 id="331-pruning">3.3.1 Pruning</h3>
<p>剪枝方法基于lottery ticket假设，即模型中只有小部分参数起到了核心作用，其他的大部分参数都是无效参数或是不重要的参数，可以去除掉，在减小模型参数量的同时，保持模型原有的精度。剪枝可以分为结构化剪枝与非结构化剪枝。非结构化剪枝允许修建任何参数，定位参数中接近于0的参数，将这些参数归零，使得权重矩阵稀疏化。虽然非结构化剪枝可以极大减少模型参数，但是由于硬件的限制，很多场景中无法完全发挥非结构化剪枝的效果。结构化剪枝是粒度较大的剪枝，修剪模型中结构化的部分，比如权重的整行，多头注意力中不需要的注意力头，多层Transformer中不需要的若干层等。由于存在一定限制，结构化剪枝的模型压缩率较小，但是更加适合于硬件运行。</p>
<p>考虑到Transformer中大部分的计算量是在多头注意力（MSA）和前馈神经网络（FFN）部分，为了简Transformer的结构，Vision Transformer Pruning（VTP）是第一个专门用于Vision Tranormer的剪枝方法。VTP首先使用L1稀疏正则化进行训练，VTP获取每一个Transformer block中Dimension的重要性分数，然后对分数较低的Dimension进行裁剪，这样大量的不重要的Dimension将会被裁剪，最后进行微调。不同于VTP主要关注于通道维度的冗余，PS-ViT方法关注于patch层面的冗余，通过计算patch对于最终分类特征的重要性得分来判断每个patch的有效性，同时保证信息一致性，显著降低了计算量并保持了原始模型的精度。NViT在剪枝时将模型的推理时间纳入考虑，通过重分配使用的参数，进行全局结构性剪枝。后续模型分别针对剪枝范围和粒度、剪枝方法、剪枝过程等做出改进，进一步提高模型的推理性能。</p>
<blockquote>
<p>[Paper: THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS]</p>
<p>[Paper: Vision Transformer Pruning]</p>
<p>[Paper: Vision Transformer with Progressive Sampling]</p>
<p>[Paper: NViT: Vision Transformer Compression and Parameter Redistribution]</p>
</blockquote>
<h3 id="332-distillation">3.3.2 Distillation</h3>
<p>蒸馏是指用教师模型来指导学生模型训练，通过蒸馏的方式让学生模型学习到教师模型的知识。在模型压缩中，教师模型是一个提前训练好的复杂模型，而学生模型则是一个规模较小的模型。由训练好的教师模型，在相同的数据下，通过将教师网络对该样本的预测值作为学生模型的预测目标指导学生模型学习。通过教师模型的指导，让学生模型学习教师模型的泛化能力，以达到或媲美教师模型的准确度。</p>
<p>在计算机视觉领域，DeiT在ViT的基础上，提出了一种专门针对Transformer的蒸馏方法，将distillation token与原始的class token同时加入网络，同时对损失函数进行相应的变化，显著减小了模型训练时间和训练所需的数据量。Mainfold Distiallation方法考虑了视觉Transformer的特点，在模型中间层引入了patch层级的细粒度监督信号，它是一种基于内积计算特征空间的流形结构表示，通过约束学生模型与教师模型的特征空间具有相似的流形结构，可以更好的将教师模型的知识迁移到学生模型中。TaT中进一步考虑到，由于教师模型和学生模型在结构上的异构型，直接对比像素级别的特征图可能导致不对齐的问题，因此使用注意力机制来隐式对齐语义，并提出一种近似的方法来改善方法的复杂度。</p>
<blockquote>
<p>[Training data-efficient image transformers &amp; distillation through attention]</p>
<p>[Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation]</p>
<p>[Knowledge Distillation via the Target-aware Transformer]</p>
</blockquote>
<h3 id="333-quantization">3.3.3 Quantization</h3>
<p>量化的基本思想即使用低精度、低比特的数据类型来代替原本的浮点数据类型，可以量化参数权重，也可以量化激活值，不但显著减小了模型的体积，更为重要的意义是优化了模型在运行时的访存，相较于单个指令的计算，访存耗时要远高于计算，因此可以显著加速模型推理。量化最核心的挑战在于使用更低精度的权重的同时保持模型精度尽可能少的降低。量化主要分为两大类，训练后量化（Post-Training Quantization，PTQ）和量化感知训练（Quantization-Aware Training，QAT）。训练后量化是将训练好的模型中的参数或激活值量化为低精度类型的数值类型，虽然使用简单，但是模型精度精度下降一般要高于量化感知训练。量化感知训练在训练过程中模拟量化过程，进而在更新参数时考虑量化产生的误差，虽然量化感知训练得到的量化模型精度下降较低，但是因为需要重新训练，所以开销较大，在实际使用中需要进行权衡使用。</p>
<p>虽然在卷积神经网络中可以相对简单的使用量化，但是将量化应用于Transformer存在一些挑战。Transformer激活值范围较大，很难使用低精度数据类型表示。传统的卷积神经网络会将异常的离群值截断，但是在Transformer中，这样的离群值有助于深层网络中形成特定的注意力模式，直接截断会改变网络的特性和精度，如果不截断会导致数值分辨率降低，而且注意力机制中存在一些难以量化的算子，进一步导致Transformer模型难以量化。PTQ4ViT提出了使用孪生均匀量化方法来解决激活值范围大的问题，同时为了获得最优的量化参数（而非局部最优），使用Hessian引导度量来评估不同的标定因子，从而以较小的成本提高校准准确率，最终达到了近乎无损的量化效果。针对部分算子难以量化的问题，FQ-ViT中使用Power-of-Two Factor（PTF）来量化LayerNorm，使用Log-Int-Softmax（LIS）来量化softmax，并使用4位量化和BitShift来进行简化，这也是第一个实现Transformer无损全量化的工作。</p>
<blockquote>
<p>[Understanding and Overcoming the Challenges of Efficient Transformer Quantization]</p>
<p>[PTQ4ViT: Post-Training Quantization Framework for Vision Transformers with Twin Uniform Quantization]</p>
<p>[FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer]</p>
</blockquote>
<h3 id="334-flashattention">3.3.4 FlashAttention</h3>
<p>FlashAttention是一种对标准注意力机制进行加速的算法，减少了对HBM（High Bandwidth Memory，通常用于GPU显存）的访问，而且它的训练和推理过程的结果和标准注意力机制完全相同，对用户透明，并且显著减小了标准注意力机制的运行时间和所需内存。</p>
<p>FlashAttention主要从两个方面减少注意力机制的HBM的访问。首先在计算softmax时，FlashAttention可以在不访问整个输入的情况下计算softmax reduction，将输入分割成块，在输入块上多次传递，从而以增量的方式计算softmax reduction。其次，在传统注意力机制中，需要将$QK^T$的计算结果$S$和$softmax(S)$后的计算结果$P$分别存储到显存中，FlashAttention对此做出改进，在反向传播中不存储中间注意力矩阵，避免从显存中读取和写入中间结果矩阵。通过分块写入到HBM中去，存储前向传递的 softmax 归一化因子，在后向传播中快速重新计算片上注意力，这比从HBM中读取中间注意力矩阵的标准方法更快。即使由于重新计算导致 FLOPS 增加，但因为减少了HBM访问，导致运行速度更快并且使用更少的显存（序列长度线性）。</p>
<p>此外，最新的研究SCFA进一步进行拓展，使得FlashAttention可以计算稀疏注意力，特别是针对Hash-based Attention和Query/Key-Dropping Based Attention，都得到了显著的推理加速。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/618533434">Blog: 论文分享：新型注意力算法FlashAttention</a></p>
<p>[Paper: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness]</p>
<p>[Paper: Faster Causal Attention Over Large Sequences Through Sparse Flash Attention]</p>
</blockquote>
<h1 id="4-discussion-on-future-research">4. Discussion on Future Research</h1>
<p>Transformer虽然有很强的建模能力，但是由于其中注意力机制具有序列长度平方的复杂度，限制了Transformer在很多场景中的使用。在未来的工作中，仍然可能会有很多工作对efficient attention、efficient transformer、模型压缩的不同方面进行改进。除此之外，本文观察到另外两个方向未来可能有进一步的发展。</p>
<h2 id="41-early-exit">4.1 Early Exit</h2>
<p>虽然当前很多研究关注于大模型在大量数据上的有效训练，但是经过训练的模型在实际使用中仍然速度较慢，特别是大模型作为基础设施时，越来越多的关注集中于提高模型的推理速度上。从模型来分析，很多大语言模型都是自回归模型，需要根据前面的单词递推的预测下一个单词，这个过程不能并行化，而且考虑到大模型庞大的参数量，整个推理过程需要大量的计算与较高的延迟。</p>
<p>在推理时，有些单词的预测比较轻松，可能在比较浅层的网络中就可以预测出正确的结果，不用计算到最后一层就可以正确预测，即提前退出（early exit），有的单词就需要较多的计算才能预测，但是很多模型在推理时针对这两种情况使用了相同的计算量。有一些工作已经初步在这方面进行了尝试，比如CALM，不是等待所有解码器层完成，而是尝试在某个中间层之后更早地预测下一个单词。 为了决定是进行某个预测还是将预测推迟到后面的层，测量模型对其中间预测的置信度。 只有当模型有足够的信心预测不会改变时，才会跳过其余的计算。</p>
<blockquote>
<p>[Paper: Confident Adaptive Language Modeling]</p>
<p><a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/120340325">Blog: 基于动态提前退出的预训练模型推理加速简介</a></p>
</blockquote>
<h2 id="42-alternatives-to-attention">4.2 Alternatives to Attention</h2>
<p>虽然注意力机制对于Transformer而言至关重要，但是由于其较高的复杂度，一些研究开始寻找注意力机制的替代而非单纯改进注意力机制。在AFT模型中，同样有类似于标准的点积注意力算法，同样由查询向量Q，被查向量K，内容向量V相互作用而成。但不同的是，AFT中的K和V首先与一组学习得到的位置偏差（position bias）结合，然后再进行同位元素对应相乘（element-wise multiplication）。这一新操作的内存复杂度、文本规模、特征维度都是线性的。当前一个较新的尝试是Hyena。Hyena将时域卷积和频域卷积作为一个组合，通过递归进行多次来增大表达能力，其全局卷积网络达到了超越Transformer建模的效果。</p>
<blockquote>
<p>[Paper: An Attention Free Transformer]</p>
<p>[Paper: Hyena Hierarchy: Towards Larger Convolutional Language Models]</p>
</blockquote>
<h1 id="5-conclusion">5. Conclusion</h1>
<p>在本综述中，从推理的角度出发，对efficient transformer进行了粗粒度的调研、分析与总结，并且相对侧重于计算机视觉方面的研究。首先介绍模型不同角度的efficiency和评价efficiency的量化指标。然后从模型算法的角度，从不同层次分析了当前提高模型效率的方法，比如设计复杂度更低的注意力机制，更加高效的网络设计，模型压缩和优化等方法，并针对每种方法进一步做了分类和总结，选取代表性的方法进行具体说明。最后，简单讨论了一些efficient transformer未来可能的发展方向，比如早退机制、注意力机制的替代品等。</p>
<h1 id="6-more-reading">6. More Reading</h1>
<p><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#sparse-attention-patterns">Large Transformer Model Inference Optimization</a></p>
<p><a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#low-rank-attention">The Transformer Family Version 2.0</a></p>
<p><a href="https://medium.com/data-science-at-microsoft/efficient-transformers-survey-of-recent-work-75022cddc86a">Efficient transformers: Survey of recent work</a></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/120292394">Bert/Transformer模型压缩与优化加速</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
