<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>flash_attention简要笔记 | Paul's Blog</title>
<meta name=keywords content="deep learning"><meta name=description content="本文中有较多Latex数学公式，博客上有一些数学公式格式渲染不正确，可以查看flash_attention简要笔记 优化效果 原来，attent"><meta name=author content="Paul"><link rel=canonical href=https://qinganzhang.github.io/posts/flash_attention%E7%AE%80%E8%A6%81%E7%AC%94%E8%AE%B0/><link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style><link rel=icon href=https://qinganzhang.github.io/favicon.ico><link rel=apple-touch-icon href=https://qinganzhang.github.io/apple-touch-icon.png><meta name=twitter:title content="flash_attention简要笔记 | Paul's Blog"><meta name=twitter:description content="本文中有较多Latex数学公式，博客上有一些数学公式格式渲染不正确，可以查看flash_attention简要笔记 优化效果 原来，attent"><meta property="og:title" content="flash_attention简要笔记 | Paul's Blog"><meta property="og:description" content="本文中有较多Latex数学公式，博客上有一些数学公式格式渲染不正确，可以查看flash_attention简要笔记 优化效果 原来，attent"><meta property="og:type" content="article"><meta property="og:url" content="https://qinganzhang.github.io/posts/flash_attention%E7%AE%80%E8%A6%81%E7%AC%94%E8%AE%B0/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-07T14:07:19+08:00"><meta property="article:modified_time" content="2024-09-07T14:07:19+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Post","item":"https://qinganzhang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"flash_attention简要笔记","item":"https://qinganzhang.github.io/posts/flash_attention%E7%AE%80%E8%A6%81%E7%AC%94%E8%AE%B0/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"flash_attention简要笔记 | Paul's Blog","name":"flash_attention简要笔记","description":"本文中有较多Latex数学公式，博客上有一些数学公式格式渲染不正确，可以查看flash_attention简要笔记 优化效果 原来，attent","keywords":["deep learning"],"wordCount":"5267","inLanguage":"en","datePublished":"2024-09-07T14:07:19+08:00","dateModified":"2024-09-07T14:07:19+08:00","author":{"@type":"Person","name":"Paul"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qinganzhang.github.io/posts/flash_attention%E7%AE%80%E8%A6%81%E7%AC%94%E8%AE%B0/"},"publisher":{"@type":"Organization","name":"Paul's Blog","logo":{"@type":"ImageObject","url":"https://qinganzhang.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://qinganzhang.github.io/ accesskey=h title="Paul's Blog (Alt + H)">Paul's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://qinganzhang.github.io/posts/ title=Posts class=active>Posts</a></li><li><a href=https://qinganzhang.github.io/archives/ title=Archive>Archive</a></li><li><a href=https://qinganzhang.github.io/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=https://qinganzhang.github.io/tags/ title=Tags>Tags</a></li><li><a href=https://qinganzhang.github.io/categories/ title=Categories>Categories</a></li><li><a href=https://qinganzhang.github.io/about/ title=About>About</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://qinganzhang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://qinganzhang.github.io/posts/>Post</a></div><h1 class=post-title>flash_attention简要笔记</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>2024-09-07</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=https://qinganzhang.github.io/tags/deep-learning/>deep learning</a></span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>5267 words</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>11 min</span></span></div></header><div class="toc side right"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%bc%98%e5%8c%96%e6%95%88%e6%9e%9c aria-label=优化效果>优化效果</a></li><li><a href=#%e5%85%b7%e4%bd%93%e8%bf%87%e7%a8%8b aria-label=具体过程>具体过程</a></li><li><a href=#%e7%ae%80%e8%a6%81%e5%88%86%e6%9e%90 aria-label=简要分析>简要分析</a></li><li><a href=#%e5%ae%9e%e9%99%85%e4%bd%bf%e7%94%a8 aria-label=实际使用>实际使用</a><ul><li><a href=#%e6%8e%a5%e5%8f%a3%e8%bf%94%e5%9b%9e%e5%80%bc aria-label=接口返回值>接口返回值</a></li><li><a href=#varlen-attention aria-label="varlen attention">varlen attention</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></details></div><div class=post-content><p>本文中有较多Latex数学公式，博客上有一些数学公式格式渲染不正确，可以查看<a href=https://blog.csdn.net/weixin_44343319/article/details/142318098>flash_attention简要笔记</a></p><h1 id=优化效果>优化效果<a hidden class=anchor aria-hidden=true href=#优化效果>¶</a></h1><p>原来，attention部分的计算量和中间激活占用显存的复杂度都是$O(N^2)$</p><blockquote><p>计算量部分原来QK矩阵乘和attn_score@V矩阵乘的计算量，复杂度都是$O(N^2)$；中间激活因为中间有一个attn_score，所以复杂度也是$O(N^2)$</p></blockquote><p>现在，attention部分的中间激活占用显存的复杂度变为$O(N)$，计算量的复杂度没有变但是通过减少访存加快了计算速度，而且fa与原attention完全等价</p><h1 id=具体过程>具体过程<a hidden class=anchor aria-hidden=true href=#具体过程>¶</a></h1><p>flash-attention还是基于kernel融合的思想，将QK矩阵乘法、mask、softmax、dropout合并成一个kernel，这样不仅减少了中间变量对显存的占用，而且也减少了计算过程中的访存</p><p>一些符号表示：</p><ul><li><p>$S_{ij}=Q_i \times K_j^T$，Q分块和K分块的乘积，形状为$[B_r, B_c]$</p></li><li><p>$\widetilde{m}<em>{ij}=rowmax(S</em>{ij})$：对分块$S_{ij}$而言，得到其每行的最大值，形状为$[B_r, 1]$</p></li><li><p>$\widetilde{P}<em>{ij}=e^{S</em>{ij}-\widetilde{m}<em>{ij}}=e^{S</em>{ij}-rowmax(S_{ij})}$：每个分块$S_{ij}$减去其局部rowmax $\widetilde{m}_{ij}$，形状为$[B_r, B_c]$</p></li><li><p>$\widetilde{l}<em>{ij}=rowsum(\widetilde{P}</em>{ij})=rowsum(e^{S_{ij}-rowmax(S_{ij})})$：对$\widetilde{P}_{ij}$而言，按行求和，形状为$[B_r, 1]$</p></li><li><p>$m^{new}<em>i=max(\widetilde{m}</em>{i0}, \widetilde{m}<em>{i1}, &mldr; , \widetilde{m}</em>{ij})=rowmax(concat(S_{i0}, S_{i1}, &mldr; , S_{ij}))$：即$contcat(S_{i0}, S_{i1}, &mldr; , S_{ij})$这j+1个分块的每行的最大值，形状为$[Br, 1]$</p></li><li><p>$m_i$：$m_i^{new}$位于SRAM上，将$m_i^{new}$写回到HBM就是$m_i$，初始化$m=-\infty$</p></li><li><p>$l^{new}<em>i=e^{m_i-m_i^{new}}l_i + e^{\widetilde{m}</em>{ij}-m_i^{new}} \widetilde{l}<em>{ij}=rowsum[e^{S</em>{00}-max(\widetilde{m}<em>{00},&mldr;,\widetilde{m}</em>{0j})}] + &mldr; + rowsum[e^{S_{0j}-max(\widetilde{m}<em>{00},&mldr;,\widetilde{m}</em>{0j})}]$：</p></li><li><p>$l_i$：$l_i^{new}$位于SRAM上，将$l_i^{new}$写回到HBM就是$l_i$，初始化$l=0$</p></li></ul><p>如果不使用flash-attention，具体过程为：</p><ol><li>$S = Q K ^T $</li><li>$P = softmax(S+mask)$</li><li>$O = P V$</li></ol><p>如果使用flash-attention，前向过程为：</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-16-23:13:58.png alt=image-20240916230932000 style=zoom:40%><p>大致过程为：</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-01:47:02.jpg alt=FA style=zoom:100%><ol><li>首先对QKV进行分块，K、V分块方法相同（V的分块图中没画出来），首先可以计算$S_{ij}=Q_i\times K_j^T$。因为对QKV进行了分块，所以每次SRAM上能保留$S_{ij}$和$\widetilde{P}_{ij}$（橙黄色表示存储在SRAM上；橙红色表示虽然也存储在SRAM上，但是这些部分每次outer loop会写回到HBM中）</li><li>如果有mask，此时对$S_{ij}$进行mask</li><li>使用一个局部变量$\widetilde{m}<em>{ij}$和一个全局变量$m$（或者说$m^{new}$，$m^{new}$的值在SRAM上，但是每次outer loop会写回到HBM中）来记录分块$S</em>{ij}$局部rowmax和中间遍历过的分块$S_{i:}$的历史rowmax</li><li>然后基于分块$S_{ij}$计算局部的safe softmax的分子部分，即$e^{S_{ij}-rowmax(S_{ij})}$，safe softmax的分子部分累加就是分母部分，这样，就得到了一个针对分块$S_{ij}$的、局部的safe softmax的分母$\widetilde{l}<em>{ij}$，和 一个 遍历过的历史分块$S</em>{i:}$的 safe softmax分子部分的 累加和$l^{new}$（注意断句，写公式有点晦涩难懂，用语言描述又不太好描述），局部的$\widetilde{l}<em>{ij}$就是用来更新全局的$l$（或者说$l^{new}$，$l^{new}$的值在SRAM上，但是每次outer loop会写回到HBM中），对$\widetilde{l}</em>{ij}$举一个例子：<ul><li>当j=0，i=0时，$l_0^{new}=e^{m_0-m_0^{new}} l_0+e^{\widetilde{m}<em>{00}-m_0^{new}} \widetilde{l}</em>{00}=\widetilde{l}_{00}$</li><li>当j=1，i=0时，$l_0^{new} = rowsum(e^{S_{00}-max⁡(\widetilde{m}<em>{00}, \widetilde{m}</em>{01})})+rowsum(e^{S_{01}-max⁡(\widetilde{m}<em>{00}, \widetilde{m}</em>{01})})$</li></ul></li><li>然后对$\widetilde{P}_{ij}$进行dropout</li><li>然后相当于要进行$O+=\widetilde{P}_{ij} V_i$了，对于算法的第15行，可以使用分配律拆开看，其中有两个操作：<ol><li>后半部分：对于当前的$\widetilde{P}<em>{ij} V_i$相乘，$\widetilde{P}</em>{ij}$中减去的是分块$S_{ij}$局部的rowmax，需要调整到 此时已经见过的、所有分块$S_{i:}$的rowmax，就是第15行后半部分中$e^{\widetilde{m}_{ij}-m_i^{new}}$的意思</li><li>前半部分：调整上一次的$O$，先乘旧的$l_i$恢复到safe softmax的分子部分，然后乘以$e^{m_i-m_i^{new}}$更新一下safe softmax分子部分中减去的全局rowmax，最后再除以当前的safe softmax的分母</li></ol></li></ol><p>（反向过程还是看别的博客吧）</p><h1 id=简要分析>简要分析<a hidden class=anchor aria-hidden=true href=#简要分析>¶</a></h1><p>首先分析一下fa的FLOPs（只分析大块的矩阵乘法，其他小的操作就不计算了）：</p><ul><li>一开始的$Q_i K^T_j$矩阵相乘，其中$Q_i$的形状为$[B_r, d]$，$K_j^t$的形状为$[d, B_c]$，此时FLOPs=$2d \times B_r \times B_c$</li><li>后面计算O的时候有一个$\widetilde{P}<em>{ij} V_i$矩阵相乘，其中$\widetilde{P}</em>{ij}$的形状为$[B_r, B_c]$，$V_i$的形状为$[B_c, d]$，此时FLOPs=$2B_c \times B_r \times d$一共进行了$\frac{N}{B_r} \times \frac{N}{B_c}$次上面的循环，所以FLOPs=$4N^2d$，如果d远小于N，则计算复杂度就变成了$O(N^2)$，计算复杂度相比于standard attention没有变化</li></ul><p>然后再分析一下显存占用（显存占用说的是HBM上的显存占用，假设计算精度为$w$ Bytes）</p><ul><li>HBM上需要维护一个全局的rowmax和expsum，占用显存为$w\times N$</li><li>然后还要存储一个最后的输出$O$，占用显存为$wNd$，但是这个部分是必须的</li><li>因此，显存占用的复杂度为$O(Nd)$（或者$O(N)$，如果不考虑$O$的话）。standard attention需要保存中间的$S, P$，显存占用复杂度为$O(N^2)$</li></ul><p>fa相对于standard attention一个优势，在于减小了计算过程中的访存量，最后来分析一下访存次数：</p><ul><li>standard attention<ul><li>从HBM中读取Q，K（形状都是$[N, d]$），访存量=$wNd$，计算$S=QK^T$，然后向HBM中写回S（形状为$[N, N]$），访存量=$wN^2$</li><li>从HBM中读取S，访存量=$w N^2$，计算$P=softmax(S)$，向HBM中写回P，访存量=$w N^2$</li><li>从HBM中读取P（形状为$[N, N]$）、V（形状为$[N, d]$），访存量=$w N^2 + wNd$，计算$O=PV$，向HBM中写回O（形状为$[N, d]$），访存量=$wNd$</li><li>总的访存量=$w(3Nd+4N^2)$，如果d远小于N，则访存量的复杂度变成了$O(N^2)$</li></ul></li><li>flash attention（分析时将inner loop作为一个整体进行分析，就像上面示意图画的那样）<ul><li>从HBM中读取分块$Q_i, i=0, &mldr;, T_r -1$，读取分块$K_j$，访存量=$w(Nd+B_c d)$；后面$S_{ij}, \widetilde{P}_{ij}$不需要写回HBM；$m, l$只是一个向量，数据量很少，忽略；再后面读取和写入分块$O_i, i = 0, &mldr;,T_r =1$，访存量=$w(2\times Nd)$</li><li>outer loop共有$\frac{N}{B_c}=T_c$次，总的访存量=$w\times \frac{N}{B_c} \times (Nd + B_cd + 2Nd)=w(Nd+\frac{3N^2d}{B_c})=w(T_c+1)Nd$</li><li>比如N=1024，d=64，B=64，standard_attention访存量-flash_attention访存量=$w(3Nd+4N^2-Nd-\frac{3N^2d}{B_c})=w(2Nd+(4-\frac{3d}{B_c})N^2)=w(2Nd+N^2)$，可以看出少了很多访存</li></ul></li></ul><h1 id=实际使用>实际使用<a hidden class=anchor aria-hidden=true href=#实际使用>¶</a></h1><h2 id=接口返回值>接口返回值<a hidden class=anchor aria-hidden=true href=#接口返回值>¶</a></h2><p>flash-attention开源代码中，针对不同qkv、是否是varlen、是否需要kv_cache等不同需求封装了不同的<a href=https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/flash_attn_interface.py>接口</a>，这里说一下返回值。这些接口的返回值都相同，除了返回输出的$O$之外，如果设置了<code>return_attn_probs=True</code>，还会返回softmax_lse和S_dmask：</p><ul><li>softmax_lse（形状$[nheads, seqlen]$）：在计算$S=\frac{QK^T}{scale}$之后，会得到形状为$[bs, seqlen, seqlen]$的方阵S，在计算softmax的过程中，需要按行求和，得到一个列向量，然后再取log，写成表达式即为：$softmax_lse=log[\sum_je^{S_{ij}}]$，注意不是$softmax_lse=log[\sum_je^{S_{ij}-rowmax(S_{ij})}]$，参考issue：<a href=https://github.com/Dao-AILab/flash-attention/issues/404>What&rsquo;s the exactly formula of <code>softmax_lse</code>? #404</a></li><li>S_dmask（形状$[bs, nheads, seqlen, seqlen]$）：就是返回$P=softmax(\frac{QK^T}{scale}+mask)$的这个P矩阵</li></ul><h2 id=varlen-attention>varlen attention<a hidden class=anchor aria-hidden=true href=#varlen-attention>¶</a></h2><p>特别的，这里再说一下<code>flash_attn_varlen_func</code>等一些支持varlen的接口，其函数形参中还有<code>cu_seqlens_q</code>、<code>cu_seqlens_k</code>、<code>max_seqlen_q</code>、<code>max_seqlen_k</code>等特有的参数。这里介绍一些varlen是什么。</p><p>varlen即变长序列，产生的背景是”数据拼接“，即LLM使用的训练数据集中，长度较短的序列占大多数，这些短序列为了能够符合Transformer固定长度的输入，就要进行padding，序列越短，padding越多，而我们不太想要padding，padding只是无奈之举。此时，我们可以使用varlen特性，简单来说就是将多个短序列拼接成一个长序列，但是还是每个短序列自己内部计算注意力，短序列之间是隔离的，这样减少了padding，节省计算量和显存。</p><p>这里举个例子（<a href=https://xtuner.readthedocs.io/zh-cn/latest/acceleration/varlen_flash_attn.html>参考</a>），比如一些短序列长度分别是：70，300，180， &mldr;，260，120，1200，&mldr;等，attention固定输入长度是4096，此时我们将这些短序列拼接起来，使用varlen_attn后，就像右图所示，每个短序列自己内部计算attention，短序列之间不计算attention（否则就像左图这样，白白多了很多浪费的计算）</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-17-11:48:10.png alt=XTuner style=zoom:40%><p>为了实现varlen特性，需要对接口有一些调整。比如不使用varlen的flash_attn接口中，传入的Q、K、V的形状一般为$[bs, seqlen, nheads, head_dim]$（K和V的nheads可以少于Q的nheads，此时就是GQA/MQA）。在使用varlen的flash_attn接口中，主要有两点变化：</p><ul><li>Q、K、V的形状一般为$[total_seq, nheads, head_dim]$，这里将多个batch拼接起来，拼起来的长度为$total_seq$</li><li>多了<code>cu_seqlens_q</code>、<code>cu_seqlens_k</code>、<code>max_seqlen_q</code>、<code>max_seqlen_k</code>等特有的参数<ul><li><code>cu_seqlens_q</code>是对每个短序列的Q的长度的exclusive_scan，作用就是找到原来每个batch的起始点（offset），比如上面的例子，此时<code>cu_seqlens_q=[0, 70, 370, 550, ... ]</code>，如果<code>cu_seqlens_q</code>的形状为$[batch_size+1]$，则需要在最后拼接上序列Q的总长度</li><li><code>max_seqlen_q</code>好理解，就是短序列的Q的最长长度</li></ul></li></ul><p>在具体实现中，对每个序列的每个head分别launch kernel，来实现并行计算，这个过程中要通过<code>cu_seqlens_q</code>来确定对应Q的start_idx和end_idx。</p><p>参考：</p><p><a href=https://66ring.github.io/2024/05/31/universe/ml/flash_attn_varlen_batcing_api_usage/>Flash attention变长batching API使用</a></p><p><a href=https://github.com/Dao-AILab/flash-attention/issues/850>How did flash-attn compute attention for cu_seqlens #850</a></p><h1 id=参考>参考<a hidden class=anchor aria-hidden=true href=#参考>¶</a></h1><p><a href=https://zhuanlan.zhihu.com/p/669926191>图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑</a></p><p>优质好文：</p><p><a href=https://zhuanlan.zhihu.com/p/668888063>[Attention优化][2w字]🔥原理&图解: 从Online-Softmax到FlashAttention V1/V2/V3</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>LLM时代的transformer参数量、计算量、激活值的分析</span>
</a><a class=next href=https://qinganzhang.github.io/posts/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E7%AE%80%E6%9E%90/><span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>反向传播和自动微分简析</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qinganzhang.github.io/>Paul's Blog</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>