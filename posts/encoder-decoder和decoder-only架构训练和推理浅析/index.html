<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Encoder Decoder和decoder Only架构训练和推理浅析 | Paul's Blog</title>
<meta name=keywords content="deep learning,transformer"><meta name=description content="之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐to"><meta name=author content="Paul"><link rel=canonical href=https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/><link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style><link rel=icon href=https://qinganzhang.github.io/favicon.ico><link rel=apple-touch-icon href=https://qinganzhang.github.io/apple-touch-icon.png><meta name=twitter:title content="Encoder Decoder和decoder Only架构训练和推理浅析 | Paul's Blog"><meta name=twitter:description content="之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐to"><meta property="og:title" content="Encoder Decoder和decoder Only架构训练和推理浅析 | Paul's Blog"><meta property="og:description" content="之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐to"><meta property="og:type" content="article"><meta property="og:url" content="https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-07T14:45:54+08:00"><meta property="article:modified_time" content="2024-09-07T14:45:54+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Post","item":"https://qinganzhang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Encoder Decoder和decoder Only架构训练和推理浅析","item":"https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Encoder Decoder和decoder Only架构训练和推理浅析 | Paul's Blog","name":"Encoder Decoder和decoder Only架构训练和推理浅析","description":"之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐to","keywords":["deep learning","transformer"],"wordCount":"3530","inLanguage":"en","datePublished":"2024-09-07T14:45:54+08:00","dateModified":"2024-09-07T14:45:54+08:00","author":{"@type":"Person","name":"Paul"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/"},"publisher":{"@type":"Organization","name":"Paul's Blog","logo":{"@type":"ImageObject","url":"https://qinganzhang.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://qinganzhang.github.io/ accesskey=h title="Paul's Blog (Alt + H)">Paul's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://qinganzhang.github.io/posts/ title=Posts class=active>Posts</a></li><li><a href=https://qinganzhang.github.io/archives/ title=Archive>Archive</a></li><li><a href=https://qinganzhang.github.io/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=https://qinganzhang.github.io/tags/ title=Tags>Tags</a></li><li><a href=https://qinganzhang.github.io/categories/ title=Categories>Categories</a></li><li><a href=https://qinganzhang.github.io/about/ title=About>About</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://qinganzhang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://qinganzhang.github.io/posts/>Post</a></div><h1 class=post-title>Encoder Decoder和decoder Only架构训练和推理浅析</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>2024-09-07</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=https://qinganzhang.github.io/tags/deep-learning/>deep learning</a><a href=https://qinganzhang.github.io/tags/transformer/>transformer</a></span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>3530 words</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>8 min</span></span></div></header><div class="toc side right"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%80%e5%b1%b1%e4%b9%8b%e4%bd%9cattention-is-all-you-need aria-label="开山之作：Attention is all you need">开山之作：Attention is all you need</a><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e5%92%8c%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b aria-label=模型架构和训练过程>模型架构和训练过程</a></li><li><a href=#%e6%8e%a8%e7%90%86%e8%bf%87%e7%a8%8b aria-label=推理过程>推理过程</a></li></ul></li><li><a href=#encoder-only%e7%9a%84%e4%bb%a3%e8%a1%a8bert aria-label=encoder-only的代表：Bert>encoder-only的代表：Bert</a></li><li><a href=#llm%e6%97%b6%e4%bb%a3 aria-label=LLM时代>LLM时代</a><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e5%92%8c%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b-1 aria-label=模型架构和训练过程>模型架构和训练过程</a></li><li><a href=#%e6%8e%a8%e7%90%86%e8%bf%87%e7%a8%8b-1 aria-label=推理过程>推理过程</a></li></ul></li></ul></div></details></div><div class=post-content><p>之前一直对LLM推理过程中的prefill阶段和decode阶段有些困惑，prefill阶段处理prompt，decode阶段自回归地逐token生成。欸，那之前传统机器翻译不也是这样吗，输出一个中文句子，然后再自回归地逐token生成英文句子，有点混乱了。下面我们来重新梳理一下，LLM的发展过程、典型模型架构和训练过程。</p><h1 id=开山之作attention-is-all-you-need>开山之作：Attention is all you need<a hidden class=anchor aria-hidden=true href=#开山之作attention-is-all-you-need>¶</a></h1><h2 id=模型架构和训练过程>模型架构和训练过程<a hidden class=anchor aria-hidden=true href=#模型架构和训练过程>¶</a></h2><p>模型架构自不必多说，就像下面最左边和最右边经典的模型架构图所示，是典型的encoder-decoder架构。Transformer一开始，解决的是一个seq2seq的任务（下面以中译英机器翻译任务为例），模型结构中包含了encoder和decoder（将此时的decoder称为naive-decoder），下面图中是encoder-decoder的Transformer的训练过程，左边是encoder部分，右边是decoder部分。在训练阶段，将中文句子经过encoder生成编码矩阵C，将shifted right的ground truth（开头加了BOS的英文句子）作为decoder的输入，整体模型的监督信号是对应的英文句子。因此，训练是并行的，一个iteration就可以将一个中英pair训练完（假设batch size=1），训练任务就是next token predicate。</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:01:57.png alt=encoder-decoder架构的训练过程（以机器翻译为例） style=zoom:120%><h2 id=推理过程>推理过程<a hidden class=anchor aria-hidden=true href=#推理过程>¶</a></h2><p>训练完成后，实际使用中开始进行推理。首先encoder部分要输入一个中文句子，还是经过encoder生成编码矩阵C。然后是decoder部分，一开始输入一个BOS，通过自回归的方式逐token生成，渐变的颜色就表示token生成的不同次序。</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:02:08.png alt=encoder-decoder架构的推理过程（以机器翻译为例） style=zoom:80%><p>这里有一个问题需要明确一下，每次decoder部分的输入，是一个token呢，还是前面的所有token呢？比如要预测“a”这个token，那么此时decoder的输入是“have”这个token呢，还是BOS+“I”+“have”这三个token都输入呢？其实都可以，</p><ul><li><p>比如每次decoder部分的输入是前面所有的token（比如BOS+“I”+“have”这三个token都输入），那么Masked MHA、MHA部分的Q、K、V都有多行（比如现在是三行），最终decoder出来的这个矩阵，我们只需要最后一个向量（下面红框的这个向量），进行linear+softmax+采样，得到next token</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-05-15:39:09.png alt=image-20240905153902066 style=zoom:50%></li><li><p>也可以每次decoder部分的输入是一个token（比如预测“a”这个token，只输入“have”这个token），那么此时就需要将之前token的K、V向量缓存下来（即KV-Cache），一个Q向量和concat(当前的KV向量，之前缓存下来的KV向量)算注意力，Masked MHA、MHA部分的attn-score只有一行，然后最终decoder出来的矩阵也只是一个向量，直接进行linear+softmax+采样，得到next token</p></li></ul><p>采用kv-cache的方式，可以在推理过程中减少很多不必要的运算，而且从显存占用来看，kv-cache也是划算的。如果不使用kv-cache，那么每次都要输入之前生成的所有token，Q、K、V都是完整的矩阵；如果使用kv-cache，K、V显存占用与之前相同，Q只是一个行向量而原来只是一个多行的矩阵，使用kv-cache的显存占用相对于不使用kv-cache的峰值显存占用是更少的。虽然说“相对峰值显存更少”，但是需要留意的是，kv-cache还是很占显存的，尤其是大batch_size和长序列的情况下，后面产生了MQA、GQA等优化，这是后话了。</p><p>这样看来，似乎在很早以前就有kv-cache的概念，但是似乎在LLM中才真正被普遍应用起来，我想有这么几个原因（not for sure, hope your discussion）：</p><ol><li>之前可能更关注模型架构的改进，之前encoder-decoder架构、encoder-only架构、decoder-only架构没有一个特别突出的模型，直到GPT-3（decoder-only）的出现，掀起了LLM的时代，现在的LLM基本上都是decoder-only的架构</li><li>在推理场景中，之前的模型上下文长度（或者叫最长序列长度）比较小，计算强度没那么大（但是比如语音流式识别等场景中也会用到kv-cache，不是很确定），LLM时代的transformer上下文长度普遍较大，而且往往是chat场景，对延迟有要求，因此要想法设法减少计算</li></ol><h1 id=encoder-only的代表bert>encoder-only的代表：Bert<a hidden class=anchor aria-hidden=true href=#encoder-only的代表bert>¶</a></h1><p>Bert是典型的encoder-only架构，其训练和推理过程很相似，训练怎么训，往往推理就是直接一个前向的过程。现在LLM基本都是生成式任务，encoder-only的架构总是感觉很别扭，decoder-only架构就很自然。可以关注知乎问题：<a href=https://www.zhihu.com/question/588325646>为什么现在的LLM都是Decoder only的架构？</a></p><h1 id=llm时代>LLM时代<a hidden class=anchor aria-hidden=true href=#llm时代>¶</a></h1><h2 id=模型架构和训练过程-1>模型架构和训练过程<a hidden class=anchor aria-hidden=true href=#模型架构和训练过程-1>¶</a></h2><p>在模型架构方面，GPT-3和Llama只是Decoder-only架构的两个典型代表，基本保持了Vanilla-Transformer-decoder的结构，但是中间很多地方做了改动，比如去掉了与encoder的cross-attn，在masked-MHA的输入QK前面在加上旋转位置编码RoPE，将LayerNomr调整为post-norm结构，MLP部分可能会进行一些调整等，如果仅仅是走一遍训练和推理的流程，那么不会有影响。</p><p>从LLM训练过程来看，预训练阶段与之前的语言模型基本一致（如下图，这里不讨论微调、RLHF等过程，更侧重工程和流程方面），都是next token predicate任务，没有了encoder部分，模型结构看起来更加简洁。</p><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-06-23:26:59.png alt=decoder-only架构的训练过程（以语言模型为例）></p><h2 id=推理过程-1>推理过程<a hidden class=anchor aria-hidden=true href=#推理过程-1>¶</a></h2><p>Decoder-only架构的推理过程和训练过程基本保持了相同的形式，推理过程就是先输入prompt，然后自回归的逐token生成。这里提一下prompt，LLM可以认为是一种特殊的语言模型，语言模型通俗来说就是续写，但是LLM包含了in-context learning的能力，prompt可以认为是一个指令，一个问题，使得“续写”的内容能够反映prompt的意图。</p><p>这里我们可以尝试分析一下文章开头提出的疑惑：</p><ul><li>在encoder-decoder架构的机器翻译任务中，<ul><li>训练过程：中文句子输入到encoder，decoder部分的训练就是语言模型</li><li>推理过程：中文句子输入到encoder，BOS输入到decoder，然后自回归的逐token进行生成</li></ul></li><li>在decoder-only架构的LLM中，<ul><li>预训练过程：没有encoder，就是训练语言模型</li><li>推理过程：prompt输入到decoder（prefill阶段），然后自回归的逐token进行生成（decode阶段）</li></ul></li></ul><p>LLM推理过程分为prefill阶段和decode阶段，不仅仅是从推理过程上看起来可以分成两个阶段，更重要的是，这两个阶段的特点不同，性质不同，为了尽可能推理加速，才有必要分成两个阶段。</p><ul><li>prefill阶段：输入prompt，生成第一个token。由于prompt往往较长，或者实际使用中将多个prompt打成一个batch（小batch），以提高模型吞吐，所以这个阶段计算量较大。衡量该阶段的一个指标是首字延迟（TTFT，Time To First Token）。还有一个需要注意的是，为了减小decode阶段的计算量，prefill阶段在计算prompt的注意力机制的时候，会将K、V矩阵缓存下来，空间换时间，即kv-cache。</li><li>decode阶段：后续自回归的逐token进行生成的过程，直到生成一个终止符（或者达到长度限制）。该阶段中，每次自回归过程中，输入一个token，然后生成q、k、v向量，k、v向量更新到kv-cache中，然后q向量和矩阵的计算（gemv），计算量较小，访存逐渐称为bottleneck。为了提高计算强度，往往会将多个请求decoder阶段的计算组成一个大batch。衡量该阶段的一个指标是TPOT（Time Per Output Token，生成每个token的耗时）。</li></ul><p>上面TTFT和TPOT指标是针对streaming generate场景，如果是non-streaming generate场景，则还是使用经典的延迟（Latency，生成一个完整输出的耗时）、吞吐（Throughput，每秒可以生成几个完整的输出）作为指标。</p><p>reference and more reading：</p><p><a href=https://zhuanlan.zhihu.com/p/685706549>一些已成为LLM 推理引擎中事实标准的方法</a></p><p><a href=https://zhuanlan.zhihu.com/p/696850285>大模型高效推理 I 推理技术框架总结</a></p><p><a href=https://zhuanlan.zhihu.com/p/683359705>LLM推理到底需要什么样的芯片？（1）</a> <a href=https://zhuanlan.zhihu.com/p/683908169>LLM推理到底需要什么样的芯片？（2)</a></p><p><a href=https://zhuanlan.zhihu.com/p/704408423>大模型推理原理&流程详解</a></p></div><footer class=post-footer><nav class=paginav><a class=next href=https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/><span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>LLM时代的transformer量化分析 参数量、计算量、激活值</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qinganzhang.github.io/>Paul's Blog</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>