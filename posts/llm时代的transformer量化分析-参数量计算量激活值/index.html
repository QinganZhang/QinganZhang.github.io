<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM时代的transformer量化分析 参数量、计算量、激活值 | Paul's Blog</title>
<meta name=keywords content="deep learning,transformer"><meta name=description content="定性分析 GPU上都存了哪些东西 首先我们来从全局整体的角度看一看，在训练阶段GPU显存上都有哪些内容： Model States：模型训练过程中必须存储的"><meta name=author content="Paul"><link rel=canonical href=https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/><link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style><link rel=icon href=https://qinganzhang.github.io/favicon.ico><link rel=apple-touch-icon href=https://qinganzhang.github.io/apple-touch-icon.png><meta name=twitter:title content="LLM时代的transformer量化分析 参数量、计算量、激活值 | Paul's Blog"><meta name=twitter:description content="定性分析 GPU上都存了哪些东西 首先我们来从全局整体的角度看一看，在训练阶段GPU显存上都有哪些内容： Model States：模型训练过程中必须存储的"><meta property="og:title" content="LLM时代的transformer量化分析 参数量、计算量、激活值 | Paul's Blog"><meta property="og:description" content="定性分析 GPU上都存了哪些东西 首先我们来从全局整体的角度看一看，在训练阶段GPU显存上都有哪些内容： Model States：模型训练过程中必须存储的"><meta property="og:type" content="article"><meta property="og:url" content="https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-07T14:43:19+08:00"><meta property="article:modified_time" content="2024-09-07T14:43:19+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Post","item":"https://qinganzhang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"LLM时代的transformer量化分析 参数量、计算量、激活值","item":"https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLM时代的transformer量化分析 参数量、计算量、激活值 | Paul's Blog","name":"LLM时代的transformer量化分析 参数量、计算量、激活值","description":"定性分析 GPU上都存了哪些东西 首先我们来从全局整体的角度看一看，在训练阶段GPU显存上都有哪些内容： Model States：模型训练过程中必须存储的","keywords":["deep learning","transformer"],"wordCount":"3840","inLanguage":"en","datePublished":"2024-09-07T14:43:19+08:00","dateModified":"2024-09-07T14:43:19+08:00","author":{"@type":"Person","name":"Paul"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/"},"publisher":{"@type":"Organization","name":"Paul's Blog","logo":{"@type":"ImageObject","url":"https://qinganzhang.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://qinganzhang.github.io/ accesskey=h title="Paul's Blog (Alt + H)">Paul's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://qinganzhang.github.io/posts/ title=Posts class=active>Posts</a></li><li><a href=https://qinganzhang.github.io/archives/ title=Archive>Archive</a></li><li><a href=https://qinganzhang.github.io/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=https://qinganzhang.github.io/tags/ title=Tags>Tags</a></li><li><a href=https://qinganzhang.github.io/categories/ title=Categories>Categories</a></li><li><a href=https://qinganzhang.github.io/about/ title=About>About</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://qinganzhang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://qinganzhang.github.io/posts/>Post</a></div><h1 class=post-title>LLM时代的transformer量化分析 参数量、计算量、激活值</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>2024-09-07</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=https://qinganzhang.github.io/tags/deep-learning/>deep learning</a><a href=https://qinganzhang.github.io/tags/transformer/>transformer</a></span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>3840 words</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>8 min</span></span></div></header><div class="toc side right"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%ae%9a%e6%80%a7%e5%88%86%e6%9e%90 aria-label=定性分析>定性分析</a><ul><li><a href=#gpu%e4%b8%8a%e9%83%bd%e5%ad%98%e4%ba%86%e5%93%aa%e4%ba%9b%e4%b8%9c%e8%a5%bf aria-label=GPU上都存了哪些东西>GPU上都存了哪些东西</a></li><li><a href=#%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83 aria-label=混合精度训练>混合精度训练</a></li></ul></li><li><a href=#%e9%87%8f%e5%8c%96%e5%88%86%e6%9e%90 aria-label=量化分析>量化分析</a><ul><li><a href=#transformer%e7%bb%93%e6%9e%84%e8%af%a6%e8%a7%a3 aria-label=transformer结构详解>transformer结构详解</a><ul><ul><ul><li><a href=#%e7%ae%80%e5%8d%95%e5%88%86%e6%9e%90 aria-label=简单分析>简单分析</a></li><li><a href=#%e5%85%b6%e4%bb%96%e8%af%b4%e6%98%8e aria-label=其他说明>其他说明</a><ul><li><a href=#1-layernorm%e7%9a%84%e8%ae%a1%e7%ae%97 aria-label="1. LayerNorm的计算">1. LayerNorm的计算</a></li><li><a href=#2-layernorm%e7%9a%84%e4%bd%8d%e7%bd%ae aria-label="2. LayerNorm的位置">2. LayerNorm的位置</a></li><li><a href=#3-%e5%85%b3%e4%ba%8edropout%e7%9a%84%e4%bd%8d%e7%bd%ae aria-label="3. 关于dropout的位置">3. 关于dropout的位置</a></li></ul></li></ul></ul></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=定性分析>定性分析<a hidden class=anchor aria-hidden=true href=#定性分析>¶</a></h1><h2 id=gpu上都存了哪些东西>GPU上都存了哪些东西<a hidden class=anchor aria-hidden=true href=#gpu上都存了哪些东西>¶</a></h2><p>首先我们来从全局整体的角度看一看，在训练阶段GPU显存上都有哪些内容：</p><ul><li>Model States：模型训练过程中必须存储的states<ul><li>params（下面有时也叫做weights）：模型参数，记参数量为$\Phi$</li><li>grads：模型梯度，梯度数量同参数量$\Phi$</li><li>optimizer states：Adam优化器中的momentum和variance，数量分别是$\Phi$，共$2\Phi$</li></ul></li><li>Residual States：模型训练过程中，中间临时的、动态产生的states<ul><li>activation：中间激活值，这个部分可能在训练过程中占据很大一部分显存，下面会详细分析。但是激活值不是必须存储的，可以使用重计算（recompute，也叫做activation checkpoint），在反向算梯度的时候，再重新算一遍，当然计算增加了，时间换空间，实际使用中可以部分选择性的进行重计算。</li><li>temporary buffers：临时存储，比如cuda、nccl等临时申请的显存。</li><li>unusable fragment memory：内存碎片导致的内存浪费</li></ul></li></ul><p>推理阶段就相对简单一些，最主要的是Model States中的params和Residual States中的activation。</p><p>参考：<a href=https://zhuanlan.zhihu.com/p/618865052>图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></p><h2 id=混合精度训练>混合精度训练<a hidden class=anchor aria-hidden=true href=#混合精度训练>¶</a></h2><p>上面只是列出了训练过程中，显存中存放的内容和保存的数值数量，但是实际训练过程中，为了节省显存，以及考虑到训练过程中间某些过程对精度不是特别敏感，所以中间有些部分会使用fp32，有些部分会使用fp16/bf16。下面以Megatron为例，简单分析混合精度训练的一个大致流程。</p><p>首先我们来看一下不使用混合精度训练的场景，数值精度全使用fp32，作为一个分析的baseline。具体过程是：</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:18:21.png alt=fp32精度训练 style=zoom:40%><p>占用显存为：$4\Phi$（fp32 weights）+$4\Phi$（fp32 momentum）+$4\Phi$（fp32 variance）+$4\Phi$（fp32 grad）+fp32 activation（可能很大）=$16\Phi$ Bytes + fp32 activation（4代表fp32的4Bytes，2代表fp16/bf16的2Bytes）</p><p>如果使用fp16的混合精度训练（bf16应该也可以，但是实际Megatron有点不同，下面会提到），具体过程是：</p><img src=C:\Users\zhang\Desktop\fp16混合精度训练.png alt=fp16混合精度训练 style=zoom:50%><p>占用显存为：$4\Phi$（fp32 weights）+$4\Phi$（fp32 momentum）+$4\Phi$（fp32 variance）+$2\Phi$（fp16 grad）+$2\Phi$（fp16 scaled grad）+$4\Phi$（fp32 unscaled and cliped grad）+fp16 activation（可能很大）=$20\Phi$ Bytes + fp16 activation</p><p>需要说明的有两点：</p><ol><li>当fp16 scaled grad转为为fp32 unscaled and cliped grad后，fp16 scaled grad就没用了，但是此时Megatron中仍然保留着一份fp16 scaled grad，所以显存占用中这两部分都会计算在内，这也符合Megatron offical readme中的描述：</li></ol><img src=C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20240907213340085.png alt=image-20240907213340085 style=zoom:80%><ol start=2><li><p>注意到上面流程中多了一个scale/unscale的操作，这叫做“loss scaling”</p><p>​ 在使用混合精度训练时，如果直接使用fp16的grad来更新fp16的梯度，一是会产生舍入误差（比如梯度很小，权重更新后，由于精度不够，累加上的lr * grad被舍入，权重没变，一句话来说就是<strong>大数吃小数</strong>），二是会产生梯度下溢（比如梯度过小，fp16范围不够，导致很小的梯度下溢成为0，而这样的小梯度占比很大，一句话来说就是<strong>下溢成0</strong>）。对于舍入误差，可以在更新权重时，将fp16的梯度转换为fp32，再更新fp32的权重，从而避免精度问题。对于梯度下溢，需要使用loss scale。</p><p>​ loss scale就是FWD计算出loss后，对loss放大若干倍，由于求导的链式法则，放大的若干倍同样会传导到fp16梯度，这样fp16梯度就不会产生梯度下溢。在更新权重时，将fp16的梯度转换为fp32，同时进行unscale。</p></li></ol><p>刚才说到bf16有一点点特殊，我们看相应的代码：（Megatron中的arguments.py）</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:49:42.png alt=image-20240907214939077 style=zoom:80%><p>注意到如果使用bf16，那么会强行设置accumulate_allreduce_grads_in_fp32=True，这与上面Megatron offical readme截图（Distributed Optimizer）表格中的第二行【bf16 param, fp32 grads】相对应。具体过程应该是（not for sure, hope for discuss）：</p><blockquote><p>accumulate_allreduce_grads_in_fp32：If true, do the gradient accumulation and communication in fp32. <a href=https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/distributed.html>from here</a></p><p>gradient accumulation：在若干次iteration中，每次都会反向得到一份梯度，将这若干次iteration得到的梯度进行累加、求平均，在最后一次iteration才更新权重。gradient accumulation与data parallel是等价的，gradient accumulation在时间维度上训练多个mini-batch，而data parallel在相同时间内将不同mini-batch放在不同的机器上训练，结果都是一样的。</p><p>参考：</p><ul><li><p><a href=https://zhuanlan.zhihu.com/p/595716023>聊聊梯度累加(Gradient Accumulation)</a></p></li><li><p><a href=https://zhuanlan.zhihu.com/p/650710443>梯度累积算法</a></p></li><li><p><a href=https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation>Hugging Face:Performing gradient accumulation with 🤗 Accelerate</a></p></li></ul></blockquote><img src=C:\Users\zhang\Desktop\bf16混合精度训练.png alt=bf16混合精度训练 style=zoom:50%><p>这里找到一个为什么要将bf16与accumulate_allreduce_grads_in_fp32绑定的<a href=https://github.com/NVIDIA/Megatron-LM/issues/372>issue</a>，里面提到“We found this to lead to more stable training before, but you could also try to perform the all-reduce in <code>bf16</code> (it might hurt convergence but will be faster).”</p><p>参考：</p><ul><li><a href=https://zhuanlan.zhihu.com/p/618865052>图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></li><li><a href=https://zhuanlan.zhihu.com/p/662700424>图解大模型训练系列之：Megatron源码解读3，分布式混合精度训练</a></li><li><a href=https://docs.nvidia.com/deeplearning/performance/mixed-precision-training>NVIDIA Docs Hub：Train With Mixed Precision</a></li><li><a href=https://zhuanlan.zhihu.com/p/441591808>全网最全-混合精度训练原理</a></li></ul><h1 id=量化分析>量化分析<a hidden class=anchor aria-hidden=true href=#量化分析>¶</a></h1><h2 id=transformer结构详解>transformer结构详解<a hidden class=anchor aria-hidden=true href=#transformer结构详解>¶</a></h2><p>LLM中的transformer一般是decoder-only结构，所以下面的transformer block主要是decoder，但是与经典的Transformer中的decoder不同的是，这里没有了cross-attn，因此结构看起来反而有点像encoder（但不是，因为有casual mask）。</p><p>下面图中的Transformer，没有上kv-cache、GQA等优化，这部分后面会分析。其中，参数量$\Phi$表示有多少个参数；中间激活值$A$是，单位是Bytes，</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-04-22:12:10.jpg alt=20240904221135 style=zoom:150%><p>KV cache对显存占用的影响</p><p>MQA和GQA对显存占用的影响</p><h5 id=简单分析>简单分析<a hidden class=anchor aria-hidden=true href=#简单分析>¶</a></h5><p>忽略层数l，拿单层来进行分析</p><ol><li><p>参数量与隐藏层维度h的平方成正比</p></li><li><p>激活值中间包含两项，$34bsh$和$8bas^2$，提取出相同的部分，剩下即比较$34h$和$8as$的大小,一般来说8as会大一点（如果没用MQA或GQA），是34h的2~10倍，因此增大b、s、h都会显著增加激活值占用的显存，举个例子说明：</p><table><thead><tr><th>config（假设使用bf16）</th><th>$34bsh$</th><th>$8bas^2$</th><th>total</th></tr></thead><tbody><tr><td>b=1,s=2048,a=96,h=12288</td><td>816MB</td><td>3072MB</td><td>3.8G</td></tr></tbody></table></li><li><p>计算量与参数量成正比，与输入token数量成正比</p></li></ol><h5 id=其他说明>其他说明<a hidden class=anchor aria-hidden=true href=#其他说明>¶</a></h5><h6 id=1-layernorm的计算>1. LayerNorm的计算<a hidden class=anchor aria-hidden=true href=#1-layernorm的计算>¶</a></h6><p>LayerNorm的计算过程见<a href=https://blog.csdn.net/weixin_39228381/article/details/107939602>pytorch LayerNorm参数详解，计算过程</a>，总结一下就是：</p><ol><li>比如输入是<code>[b,s,h]</code>，LN的<code>normalized_shape=[h]</code>，此时就是对每一个大小为<code>h</code>的向量分别进行归一化（一共<code>b*s</code>个）</li><li>然后如果LN的<code>elementwise_affine=True</code>，就需要对每个大小为<code>h</code>的向量elementwise的乘上$\gamma: [h]$，再elementwise的加上$\beta:[h]$，$\gamma$和$\beta$就是该LN层的两个可学习的参数。如果LN的<code>elementwise_affine=False</code>，则只会进行第一步的归一化，不会进行第二步的affine</li></ol><p>一个有趣的问题是，<a href=https://zhuanlan.zhihu.com/p/707778968>Transformer中的LayerNorm可以并行吗？</a></p><p>关键词： Welford online Algorithm，当一个集合新增加一个元素$x_N$的时候，可以通过前N-1个样本的corrected sum of squares($\sum_{i=1}^{N-1}(x_i-\bar{x})^2$)，计算出前N个样本的corrected sum of squares，从而只需要one pass就可以完成LN的计算（之前navie的方法是two pass）</p><h6 id=2-layernorm的位置>2. LayerNorm的位置<a hidden class=anchor aria-hidden=true href=#2-layernorm的位置>¶</a></h6><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-04-22:22:22.png alt=image-20240904222219836></p><p>原始Transformer中使用post-norm居多，后来使用pre-norm居多，而且往往在FFN之前也加一个norm。尤其在大模型中，可能在通过LN之后MHA之前，Q和K还要加上旋转位置编码。</p><p>参考：<a href=https://zhuanlan.zhihu.com/p/474988236>【重新了解Transformer模型系列_1】PostNorm/PreNorm的差别</a></p><h6 id=3-关于dropout的位置>3. 关于dropout的位置<a hidden class=anchor aria-hidden=true href=#3-关于dropout的位置>¶</a></h6><p>一共（可能）在有四个地方有dropout：</p><ol><li>在PositionalEmbedding中有一个dropout：<code>dropout(x + PositionEmbedding(x))</code>，不过好像LLM现在使用旋转位置编码RoPE多一些，在计算attention之前在Q和K上加上RoPE，一开始输入的embedding不加PositionalEmbedding了</li><li>在softmax计算得到的attention score之后有一个droput：$dropout( softmax(\frac{QK^T}{scale}+casual_mask) )$</li><li>在sublayer（Attention和MLP）计算完之后，各有一个dropout：<code>x+dropout(sublayer(norm(x)))</code></li></ol><p>参考：</p><p><a href=https://zhuanlan.zhihu.com/p/624740065>分析transformer模型的参数量、计算量、中间激活、KV cache</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>Encoder Decoder和decoder Only架构训练和推理浅析</span>
</a><a class=next href=https://qinganzhang.github.io/posts/a_survey_of_efficient_transformer_on_inference/><span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>A survey of Efficient Transformer on Inference</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qinganzhang.github.io/>Paul's Blog</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>