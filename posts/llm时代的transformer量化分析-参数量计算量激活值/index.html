<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLMæ—¶ä»£çš„transformerå‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„åˆ†æ | Paul's Blog</title>
<meta name=keywords content="deep learning,transformer"><meta name=description content="å¯¼è¯»ï¼šæœ¬æ–‡å¯ä»¥çœ‹ä½œæ˜¯å¯¹åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cacheçš„è¯¦ç»†è¯´æ˜ å®šæ€§åˆ†æ GPUä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿ é¦–å…ˆæˆ‘"><meta name=author content="Paul"><link rel=canonical href=https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/><link crossorigin=anonymous href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style><link rel=icon href=https://qinganzhang.github.io/favicon.ico><link rel=apple-touch-icon href=https://qinganzhang.github.io/apple-touch-icon.png><meta name=twitter:title content="LLMæ—¶ä»£çš„transformerå‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„åˆ†æ | Paul's Blog"><meta name=twitter:description content="å¯¼è¯»ï¼šæœ¬æ–‡å¯ä»¥çœ‹ä½œæ˜¯å¯¹åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cacheçš„è¯¦ç»†è¯´æ˜ å®šæ€§åˆ†æ GPUä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿ é¦–å…ˆæˆ‘"><meta property="og:title" content="LLMæ—¶ä»£çš„transformerå‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„åˆ†æ | Paul's Blog"><meta property="og:description" content="å¯¼è¯»ï¼šæœ¬æ–‡å¯ä»¥çœ‹ä½œæ˜¯å¯¹åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cacheçš„è¯¦ç»†è¯´æ˜ å®šæ€§åˆ†æ GPUä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿ é¦–å…ˆæˆ‘"><meta property="og:type" content="article"><meta property="og:url" content="https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-07T14:43:19+08:00"><meta property="article:modified_time" content="2024-09-07T14:43:19+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Post","item":"https://qinganzhang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"LLMæ—¶ä»£çš„transformerå‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„åˆ†æ","item":"https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLMæ—¶ä»£çš„transformerå‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„åˆ†æ | Paul's Blog","name":"LLMæ—¶ä»£çš„transformerå‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„åˆ†æ","description":"å¯¼è¯»ï¼šæœ¬æ–‡å¯ä»¥çœ‹ä½œæ˜¯å¯¹åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cacheçš„è¯¦ç»†è¯´æ˜ å®šæ€§åˆ†æ GPUä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿ é¦–å…ˆæˆ‘","keywords":["deep learning","transformer"],"wordCount":"10266","inLanguage":"en","datePublished":"2024-09-07T14:43:19+08:00","dateModified":"2024-09-07T14:43:19+08:00","author":{"@type":"Person","name":"Paul"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qinganzhang.github.io/posts/llm%E6%97%B6%E4%BB%A3%E7%9A%84transformer%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90-%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E9%87%8F%E6%BF%80%E6%B4%BB%E5%80%BC/"},"publisher":{"@type":"Organization","name":"Paul's Blog","logo":{"@type":"ImageObject","url":"https://qinganzhang.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://qinganzhang.github.io/ accesskey=h title="Paul's Blog (Alt + H)">Paul's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://qinganzhang.github.io/posts/ title=Posts class=active>Posts</a></li><li><a href=https://qinganzhang.github.io/archives/ title=Archive>Archive</a></li><li><a href=https://qinganzhang.github.io/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=https://qinganzhang.github.io/tags/ title=Tags>Tags</a></li><li><a href=https://qinganzhang.github.io/categories/ title=Categories>Categories</a></li><li><a href=https://qinganzhang.github.io/about/ title=About>About</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://qinganzhang.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://qinganzhang.github.io/posts/>Post</a></div><h1 class=post-title>LLMæ—¶ä»£çš„transformerå‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„åˆ†æ</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>2024-09-07</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=https://qinganzhang.github.io/tags/deep-learning/>deep learning</a><a href=https://qinganzhang.github.io/tags/transformer/>transformer</a></span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>10266 words</span></span><span class=meta-item>
<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>21 min</span></span></div></header><div class="toc side right"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%ae%9a%e6%80%a7%e5%88%86%e6%9e%90 aria-label=å®šæ€§åˆ†æ>å®šæ€§åˆ†æ</a><ul><li><a href=#gpu%e4%b8%8a%e9%83%bd%e5%ad%98%e4%ba%86%e5%93%aa%e4%ba%9b%e4%b8%9c%e8%a5%bf aria-label=GPUä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿>GPUä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿</a></li><li><a href=#%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83 aria-label=æ··åˆç²¾åº¦è®­ç»ƒ>æ··åˆç²¾åº¦è®­ç»ƒ</a></li></ul></li><li><a href=#%e9%87%8f%e5%8c%96%e5%88%86%e6%9e%90 aria-label=é‡åŒ–åˆ†æ>é‡åŒ–åˆ†æ</a><ul><li><a href=#transformer%e7%bb%93%e6%9e%84%e8%af%a6%e8%a7%a3 aria-label=transformerç»“æ„è¯¦è§£>transformerç»“æ„è¯¦è§£</a></li><li><a href=#kv-cache%e5%af%b9%e5%8f%82%e6%95%b0%e9%87%8f%e8%ae%a1%e7%ae%97%e9%87%8f%e6%bf%80%e6%b4%bb%e5%80%bc%e7%9a%84%e5%bd%b1%e5%93%8d aria-label="KV cacheå¯¹å‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„å½±å“">KV cacheå¯¹å‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„å½±å“</a></li><li><a href=#mqa%e5%92%8cgqa%e5%af%b9%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8%e7%9a%84%e5%bd%b1%e5%93%8d aria-label=MQAå’ŒGQAå¯¹æ˜¾å­˜å ç”¨çš„å½±å“>MQAå’ŒGQAå¯¹æ˜¾å­˜å ç”¨çš„å½±å“</a></li><li><a href=#case-study aria-label="case study">case study</a><ul><li><a href=#%e5%85%b3%e4%ba%8e%e5%8f%82%e6%95%b0%e9%87%8f%e7%9a%84%e5%88%86%e6%9e%90 aria-label=å…³äºå‚æ•°é‡çš„åˆ†æ>å…³äºå‚æ•°é‡çš„åˆ†æ</a><ul><li><a href=#gpt-3 aria-label=GPT-3>GPT-3</a></li><li><a href=#llama-1-llama--open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971 aria-label="Llama 1: LLaMa: Open and Efficient Foundation Language Models">Llama 1: <a href=https://arxiv.org/pdf/2302.13971>LLaMa: Open and Efficient Foundation Language Models</a></a></li><li><a href=#llama-2-llama-2-open-foundation-and-fine-tuned-chat-modelshttpsarxivorgpdf230709288 aria-label="Llama 2: Llama 2: Open Foundation and Fine-Tuned Chat Models">Llama 2: <a href=https://arxiv.org/pdf/2307.09288>Llama 2: Open Foundation and Fine-Tuned Chat Models</a></a></li><li><a href=#llama-3-the-llama-3-herd-of-modelshttpsarxivorgpdf240721783 aria-label="Llama 3: The Llama 3 Herd of Models">Llama 3: <a href=https://arxiv.org/pdf/2407.21783>The Llama 3 Herd of Models</a></a></li></ul></li><li><a href=#%e5%85%b3%e4%ba%8e%e6%bf%80%e6%b4%bb%e7%9a%84%e5%88%86%e6%9e%90 aria-label=å…³äºæ¿€æ´»çš„åˆ†æ>å…³äºæ¿€æ´»çš„åˆ†æ</a></li><li><a href=#%e5%85%b3%e4%ba%8e%e8%ae%a1%e7%ae%97%e9%87%8f%e7%9a%84%e5%88%86%e6%9e%90 aria-label=å…³äºè®¡ç®—é‡çš„åˆ†æ>å…³äºè®¡ç®—é‡çš„åˆ†æ</a></li></ul></li><li><a href=#%e5%85%b6%e4%bb%96%e8%af%b4%e6%98%8e aria-label=å…¶ä»–è¯´æ˜>å…¶ä»–è¯´æ˜</a><ul><ul><ul><ul><li><a href=#1-layernorm%e7%9a%84%e8%ae%a1%e7%ae%97 aria-label="1. LayerNormçš„è®¡ç®—">1. LayerNormçš„è®¡ç®—</a></li><li><a href=#2-%e5%85%b3%e4%ba%8edropout%e7%9a%84%e4%bd%8d%e7%bd%ae aria-label="2. å…³äºdropoutçš„ä½ç½®">2. å…³äºdropoutçš„ä½ç½®</a></li></ul></ul></ul></ul></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=æ€»ç»“>æ€»ç»“</a></li></ul></div></details></div><div class=post-content><p>å¯¼è¯»ï¼šæœ¬æ–‡å¯ä»¥çœ‹ä½œæ˜¯å¯¹<a href=https://zhuanlan.zhihu.com/p/624740065>åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cache</a>çš„è¯¦ç»†è¯´æ˜</p><h1 id=å®šæ€§åˆ†æ>å®šæ€§åˆ†æ<a hidden class=anchor aria-hidden=true href=#å®šæ€§åˆ†æ>Â¶</a></h1><h2 id=gpuä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿>GPUä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿<a hidden class=anchor aria-hidden=true href=#gpuä¸Šéƒ½å­˜äº†å“ªäº›ä¸œè¥¿>Â¶</a></h2><p>é¦–å…ˆæˆ‘ä»¬æ¥ä»å…¨å±€æ•´ä½“çš„è§’åº¦çœ‹ä¸€çœ‹ï¼Œåœ¨è®­ç»ƒé˜¶æ®µGPUæ˜¾å­˜ä¸Šéƒ½æœ‰å“ªäº›å†…å®¹ï¼š</p><ul><li>Model Statesï¼šæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¿…é¡»å­˜å‚¨çš„states<ul><li>paramsï¼ˆä¸‹é¢æœ‰æ—¶ä¹Ÿå«åšweightsï¼‰ï¼šæ¨¡å‹å‚æ•°ï¼Œè®°å‚æ•°é‡ä¸º$\Phi$</li><li>gradsï¼šæ¨¡å‹æ¢¯åº¦ï¼Œæ¢¯åº¦æ•°é‡åŒå‚æ•°é‡$\Phi$</li><li>optimizer statesï¼šAdamä¼˜åŒ–å™¨ä¸­çš„momentumå’Œvarianceï¼Œæ•°é‡åˆ†åˆ«æ˜¯$\Phi$ï¼Œå…±$2\Phi$</li></ul></li><li>Residual Statesï¼šæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸­é—´ä¸´æ—¶çš„ã€åŠ¨æ€äº§ç”Ÿçš„states<ul><li>activationï¼šä¸­é—´æ¿€æ´»å€¼ï¼Œè¿™ä¸ªéƒ¨åˆ†å¯èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å æ®å¾ˆå¤§ä¸€éƒ¨åˆ†æ˜¾å­˜ï¼Œä¸‹é¢ä¼šè¯¦ç»†åˆ†æã€‚ä½†æ˜¯æ¿€æ´»å€¼ä¸æ˜¯å¿…é¡»å­˜å‚¨çš„ï¼Œå¯ä»¥ä½¿ç”¨é‡è®¡ç®—ï¼ˆrecomputeï¼Œä¹Ÿå«åšactivation checkpointï¼‰ï¼Œåœ¨åå‘ç®—æ¢¯åº¦çš„æ—¶å€™ï¼Œå†é‡æ–°ç®—ä¸€éï¼Œå½“ç„¶è®¡ç®—å¢åŠ äº†ï¼Œæ—¶é—´æ¢ç©ºé—´ï¼Œå®é™…ä½¿ç”¨ä¸­å¯ä»¥éƒ¨åˆ†é€‰æ‹©æ€§çš„è¿›è¡Œé‡è®¡ç®—ã€‚</li><li>temporary buffersï¼šä¸´æ—¶å­˜å‚¨ï¼Œæ¯”å¦‚cudaã€ncclç­‰ä¸´æ—¶ç”³è¯·çš„æ˜¾å­˜ã€‚</li><li>unusable fragment memoryï¼šå†…å­˜ç¢ç‰‡å¯¼è‡´çš„å†…å­˜æµªè´¹</li></ul></li></ul><p>æ¨ç†é˜¶æ®µå°±ç›¸å¯¹ç®€å•ä¸€äº›ï¼Œæœ€ä¸»è¦çš„æ˜¯Model Statesä¸­çš„paramså’ŒResidual Statesä¸­çš„activationã€‚</p><p>å‚è€ƒï¼š<a href=https://zhuanlan.zhihu.com/p/618865052>å›¾è§£å¤§æ¨¡å‹è®­ç»ƒä¹‹ï¼šæ•°æ®å¹¶è¡Œä¸‹ç¯‡( DeepSpeed ZeROï¼Œé›¶å†—ä½™ä¼˜åŒ–)</a></p><h2 id=æ··åˆç²¾åº¦è®­ç»ƒ>æ··åˆç²¾åº¦è®­ç»ƒ<a hidden class=anchor aria-hidden=true href=#æ··åˆç²¾åº¦è®­ç»ƒ>Â¶</a></h2><p>ä¸Šé¢åªæ˜¯åˆ—å‡ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ˜¾å­˜ä¸­å­˜æ”¾çš„å†…å®¹å’Œä¿å­˜çš„æ•°å€¼æ•°é‡ï¼Œä½†æ˜¯å®é™…è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸ºäº†èŠ‚çœæ˜¾å­˜ï¼Œä»¥åŠè€ƒè™‘åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­é—´æŸäº›è¿‡ç¨‹å¯¹ç²¾åº¦ä¸æ˜¯ç‰¹åˆ«æ•æ„Ÿï¼Œæ‰€ä»¥ä¸­é—´æœ‰äº›éƒ¨åˆ†ä¼šä½¿ç”¨fp32ï¼Œæœ‰äº›éƒ¨åˆ†ä¼šä½¿ç”¨fp16/bf16ã€‚ä¸‹é¢ä»¥Megatronä¸ºä¾‹ï¼Œç®€å•åˆ†ææ··åˆç²¾åº¦è®­ç»ƒçš„ä¸€ä¸ªå¤§è‡´æµç¨‹ã€‚</p><p>é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒçš„åœºæ™¯ï¼Œæ•°å€¼ç²¾åº¦å…¨ä½¿ç”¨fp32ï¼Œä½œä¸ºä¸€ä¸ªåˆ†æçš„baselineã€‚å…·ä½“è¿‡ç¨‹æ˜¯ï¼š</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:18:21.png alt=fp32ç²¾åº¦è®­ç»ƒ style=zoom:40%><p>å ç”¨æ˜¾å­˜ä¸ºï¼š$4\Phi$ï¼ˆfp32 weightsï¼‰+$4\Phi$ï¼ˆfp32 momentumï¼‰+$4\Phi$ï¼ˆfp32 varianceï¼‰+$4\Phi$ï¼ˆfp32 gradï¼‰+fp32 activationï¼ˆå¯èƒ½å¾ˆå¤§ï¼‰=$16\Phi$ Bytes + fp32 activationï¼ˆ4ä»£è¡¨fp32çš„4Bytesï¼Œ2ä»£è¡¨fp16/bf16çš„2Bytesï¼‰</p><p>å¦‚æœä½¿ç”¨fp16çš„æ··åˆç²¾åº¦è®­ç»ƒï¼ˆbf16åº”è¯¥ä¹Ÿå¯ä»¥ï¼Œä½†æ˜¯å®é™…Megatronæœ‰ç‚¹ä¸åŒï¼Œä¸‹é¢ä¼šæåˆ°ï¼‰ï¼Œå…·ä½“è¿‡ç¨‹æ˜¯ï¼š</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:15.png alt=fp16æ··åˆç²¾åº¦è®­ç»ƒ style=zoom:50%><p>å ç”¨æ˜¾å­˜ä¸ºï¼š$4\Phi$ï¼ˆfp32 weightsï¼‰+$4\Phi$ï¼ˆfp32 momentumï¼‰+$4\Phi$ï¼ˆfp32 varianceï¼‰+$2\Phi$ï¼ˆfp16 gradï¼‰+$2\Phi$ï¼ˆfp16 scaled gradï¼‰+$4\Phi$ï¼ˆfp32 unscaled and cliped gradï¼‰+fp16 activationï¼ˆå¯èƒ½å¾ˆå¤§ï¼‰=$20\Phi$ Bytes + fp16 activation</p><p>éœ€è¦è¯´æ˜çš„æœ‰ä¸¤ç‚¹ï¼š</p><ol><li>å½“fp16 scaled gradè½¬ä¸ºä¸ºfp32 unscaled and cliped gradåï¼Œfp16 scaled gradå°±æ²¡ç”¨äº†ï¼Œä½†æ˜¯æ­¤æ—¶Megatronä¸­ä»ç„¶ä¿ç•™ç€ä¸€ä»½fp16 scaled gradï¼Œæ‰€ä»¥æ˜¾å­˜å ç”¨ä¸­è¿™ä¸¤éƒ¨åˆ†éƒ½ä¼šè®¡ç®—åœ¨å†…ï¼Œè¿™ä¹Ÿç¬¦åˆMegatron offical readmeä¸­çš„æè¿°ï¼š</li></ol><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:22.png alt=image-20240907213340085 style=zoom:80%><ol start=2><li><p>æ³¨æ„åˆ°ä¸Šé¢æµç¨‹ä¸­å¤šäº†ä¸€ä¸ªscale/unscaleçš„æ“ä½œï¼Œè¿™å«åšâ€œloss scalingâ€</p><p>â€‹ åœ¨ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶ï¼Œå¦‚æœç›´æ¥ä½¿ç”¨fp16çš„gradæ¥æ›´æ–°fp16çš„æ¢¯åº¦ï¼Œä¸€æ˜¯ä¼šäº§ç”Ÿèˆå…¥è¯¯å·®ï¼ˆæ¯”å¦‚æ¢¯åº¦å¾ˆå°ï¼Œæƒé‡æ›´æ–°åï¼Œç”±äºç²¾åº¦ä¸å¤Ÿï¼Œç´¯åŠ ä¸Šçš„lr * gradè¢«èˆå…¥ï¼Œæƒé‡æ²¡å˜ï¼Œä¸€å¥è¯æ¥è¯´å°±æ˜¯<strong>å¤§æ•°åƒå°æ•°</strong>ï¼‰ï¼ŒäºŒæ˜¯ä¼šäº§ç”Ÿæ¢¯åº¦ä¸‹æº¢ï¼ˆæ¯”å¦‚æ¢¯åº¦è¿‡å°ï¼Œfp16èŒƒå›´ä¸å¤Ÿï¼Œå¯¼è‡´å¾ˆå°çš„æ¢¯åº¦ä¸‹æº¢æˆä¸º0ï¼Œè€Œè¿™æ ·çš„å°æ¢¯åº¦å æ¯”å¾ˆå¤§ï¼Œä¸€å¥è¯æ¥è¯´å°±æ˜¯<strong>ä¸‹æº¢æˆ0</strong>ï¼‰ã€‚å¯¹äºèˆå…¥è¯¯å·®ï¼Œå¯ä»¥åœ¨æ›´æ–°æƒé‡æ—¶ï¼Œå°†fp16çš„æ¢¯åº¦è½¬æ¢ä¸ºfp32ï¼Œå†æ›´æ–°fp32çš„æƒé‡ï¼Œä»è€Œé¿å…ç²¾åº¦é—®é¢˜ã€‚å¯¹äºæ¢¯åº¦ä¸‹æº¢ï¼Œéœ€è¦ä½¿ç”¨loss scaleã€‚</p><p>â€‹ loss scaleå°±æ˜¯FWDè®¡ç®—å‡ºlossåï¼Œå¯¹lossæ”¾å¤§è‹¥å¹²å€ï¼Œç”±äºæ±‚å¯¼çš„é“¾å¼æ³•åˆ™ï¼Œæ”¾å¤§çš„è‹¥å¹²å€åŒæ ·ä¼šä¼ å¯¼åˆ°fp16æ¢¯åº¦ï¼Œè¿™æ ·fp16æ¢¯åº¦å°±ä¸ä¼šäº§ç”Ÿæ¢¯åº¦ä¸‹æº¢ã€‚åœ¨æ›´æ–°æƒé‡æ—¶ï¼Œå°†fp16çš„æ¢¯åº¦è½¬æ¢ä¸ºfp32ï¼ŒåŒæ—¶è¿›è¡Œunscaleã€‚</p></li></ol><p>åˆšæ‰è¯´åˆ°bf16æœ‰ä¸€ç‚¹ç‚¹ç‰¹æ®Šï¼Œæˆ‘ä»¬çœ‹ç›¸åº”çš„ä»£ç ï¼šï¼ˆMegatronä¸­çš„arguments.pyï¼‰</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-21:49:42.png alt=image-20240907214939077 style=zoom:80%><p>æ³¨æ„åˆ°å¦‚æœä½¿ç”¨bf16ï¼Œé‚£ä¹ˆä¼šå¼ºè¡Œè®¾ç½®accumulate_allreduce_grads_in_fp32=Trueï¼Œè¿™ä¸ä¸Šé¢Megatron offical readmeæˆªå›¾ï¼ˆDistributed Optimizerï¼‰è¡¨æ ¼ä¸­çš„ç¬¬äºŒè¡Œã€bf16 param, fp32 gradsã€‘ç›¸å¯¹åº”ã€‚å…·ä½“è¿‡ç¨‹åº”è¯¥æ˜¯ï¼ˆnot for sure, hope for discussï¼‰ï¼š</p><blockquote><p>accumulate_allreduce_grads_in_fp32ï¼šIf true, do the gradient accumulation and communication in fp32. <a href=https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/distributed.html>from here</a></p><p>gradient accumulationï¼šåœ¨è‹¥å¹²æ¬¡iterationä¸­ï¼Œæ¯æ¬¡éƒ½ä¼šåå‘å¾—åˆ°ä¸€ä»½æ¢¯åº¦ï¼Œå°†è¿™è‹¥å¹²æ¬¡iterationå¾—åˆ°çš„æ¢¯åº¦è¿›è¡Œç´¯åŠ ã€æ±‚å¹³å‡ï¼Œåœ¨æœ€åä¸€æ¬¡iterationæ‰æ›´æ–°æƒé‡ã€‚gradient accumulationä¸data parallelæ˜¯ç­‰ä»·çš„ï¼Œgradient accumulationåœ¨æ—¶é—´ç»´åº¦ä¸Šè®­ç»ƒå¤šä¸ªmini-batchï¼Œè€Œdata parallelåœ¨ç›¸åŒæ—¶é—´å†…å°†ä¸åŒmini-batchæ”¾åœ¨ä¸åŒçš„æœºå™¨ä¸Šè®­ç»ƒï¼Œç»“æœéƒ½æ˜¯ä¸€æ ·çš„ã€‚</p><p>å‚è€ƒï¼š</p><ul><li><p><a href=https://zhuanlan.zhihu.com/p/595716023>èŠèŠæ¢¯åº¦ç´¯åŠ (Gradient Accumulation)</a></p></li><li><p><a href=https://zhuanlan.zhihu.com/p/650710443>æ¢¯åº¦ç´¯ç§¯ç®—æ³•</a></p></li><li><p><a href=https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation>Hugging Face:Performing gradient accumulation with ğŸ¤— Accelerate</a></p></li></ul></blockquote><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-07-23:19:37.png alt=bf16æ··åˆç²¾åº¦è®­ç»ƒ style=zoom:50%><p>è¿™é‡Œæ‰¾åˆ°ä¸€ä¸ªä¸ºä»€ä¹ˆè¦å°†bf16ä¸accumulate_allreduce_grads_in_fp32ç»‘å®šçš„<a href=https://github.com/NVIDIA/Megatron-LM/issues/372>issue</a>ï¼Œé‡Œé¢æåˆ°â€œWe found this to lead to more stable training before, but you could also try to perform the all-reduce in <code>bf16</code> (it might hurt convergence but will be faster).â€</p><p>å‚è€ƒï¼š</p><ul><li><a href=https://zhuanlan.zhihu.com/p/618865052>å›¾è§£å¤§æ¨¡å‹è®­ç»ƒä¹‹ï¼šæ•°æ®å¹¶è¡Œä¸‹ç¯‡( DeepSpeed ZeROï¼Œé›¶å†—ä½™ä¼˜åŒ–)</a></li><li><a href=https://zhuanlan.zhihu.com/p/662700424>å›¾è§£å¤§æ¨¡å‹è®­ç»ƒç³»åˆ—ä¹‹ï¼šMegatronæºç è§£è¯»3ï¼Œåˆ†å¸ƒå¼æ··åˆç²¾åº¦è®­ç»ƒ</a></li><li><a href=https://docs.nvidia.com/deeplearning/performance/mixed-precision-training>NVIDIA Docs Hubï¼šTrain With Mixed Precision</a></li><li><a href=https://zhuanlan.zhihu.com/p/441591808>å…¨ç½‘æœ€å…¨-æ··åˆç²¾åº¦è®­ç»ƒåŸç†</a></li></ul><h1 id=é‡åŒ–åˆ†æ>é‡åŒ–åˆ†æ<a hidden class=anchor aria-hidden=true href=#é‡åŒ–åˆ†æ>Â¶</a></h1><h2 id=transformerç»“æ„è¯¦è§£>transformerç»“æ„è¯¦è§£<a hidden class=anchor aria-hidden=true href=#transformerç»“æ„è¯¦è§£>Â¶</a></h2><p>LLMä¸­çš„transformerä¸€èˆ¬æ˜¯decoder-onlyç»“æ„ï¼Œæ‰€ä»¥ä¸‹é¢çš„transformer blockä¸»è¦æ˜¯decoderï¼Œä½†æ˜¯ä¸Vanilla Transformerä¸­çš„decoderä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œæ²¡æœ‰äº†cross-attnï¼Œå› æ­¤ç»“æ„çœ‹èµ·æ¥åè€Œæœ‰ç‚¹åƒencoderï¼ˆä½†ä¸æ˜¯ï¼Œå› ä¸ºæœ‰casual maskï¼‰ã€‚</p><p>ä¸‹é¢å›¾ä¸­çš„Transformerï¼Œæ²¡æœ‰ä¸Škv-cacheã€GQAç­‰ä¼˜åŒ–ï¼Œè¿™éƒ¨åˆ†åé¢ä¼šåˆ†æã€‚å…¶ä¸­ï¼Œå‚æ•°é‡$\Phi$è¡¨ç¤ºæœ‰å¤šå°‘ä¸ªå‚æ•°ï¼›ä¸­é—´æ¿€æ´»å€¼$A$çš„å•ä½æ˜¯Bytesï¼Œä¸»è¦å‚è€ƒçš„æ˜¯<a href=https://zhuanlan.zhihu.com/p/624740065>åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cache</a></p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-15:58:31.png alt=transformerè¯¦ç»†åˆ†æ style=zoom:150%><p>åœ¨<a href=https://arxiv.org/pdf/2205.05198>Reducing Activation Recomputation in Large Transformer Models</a> 4.1èŠ‚ä¸­ä¹Ÿå¯¹transformeræ¿€æ´»å€¼è¿›è¡Œäº†ä¸€ä¸ªåˆ†æï¼Œä½†æ˜¯è¯¥è®ºæ–‡ä¸­ï¼Œself-attention blockéƒ¨åˆ†softmaxä¹‹å‰æ²¡æœ‰åŠ maskï¼Œä¸Šå›¾ä¸­æ·»åŠ äº†maskï¼Œå…·ä½“åœ¨Attentionéƒ¨åˆ†stage SA_3ï¼Œå…¶ä¸­maskç”±äºæ˜¯æ•´ä¸ªtransformerå…±äº«çš„ï¼Œæ‰€ä»¥å°±çœç•¥äº†ï¼Œ$QK^T$çš„ä¹˜ç§¯è¢«maskåŸåœ°ä¿®æ”¹ï¼Œæ‰€ä»¥$wbas^2$ä¹Ÿçœç•¥äº†ï¼Œè¿™æ ·æ¿€æ´»å€¼ä¸åŸè®ºæ–‡ä¸­ä»ç„¶æ˜¯ä¸€æ ·çš„ã€‚</p><h2 id=kv-cacheå¯¹å‚æ•°é‡è®¡ç®—é‡æ¿€æ´»å€¼çš„å½±å“>KV cacheå¯¹å‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„å½±å“<a hidden class=anchor aria-hidden=true href=#kv-cacheå¯¹å‚æ•°é‡è®¡ç®—é‡æ¿€æ´»å€¼çš„å½±å“>Â¶</a></h2><p>å…³äºKV Cacheçš„æ¥é¾™å»è„‰ï¼Œ<a href=https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/>Encoder Decoderå’Œdecoder Onlyæ¶æ„è®­ç»ƒå’Œæ¨ç†æµ…æ</a>ä¸­ç®€å•æ‹äº†ä¸€ä¸‹ã€‚ç®€å•æ¥è¯´ï¼Œkv cacheåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ï¼Œè€Œä¸”æ¨¡å‹åªèƒ½æ˜¯decoder-onlyæ¶æ„ã€‚ç”±äºè‡ªå›å½’çš„æ–¹å¼é€tokenç”Ÿæˆï¼Œself-attentionéƒ¨åˆ†å¿…é¡»ä½¿ç”¨casual maskï¼Œå› æ­¤QçŸ©é˜µéƒ¨åˆ†åªéœ€è¦è®¡ç®—æœ€æ–°tokençš„qå‘é‡å³å¯ï¼ŒKã€VçŸ©é˜µéƒ¨åˆ†åªéœ€è¦æ‹¼æ¥æ–°tokençš„kã€vå‘é‡å³å¯ï¼š</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-14:04:56.png alt=kv_cache style=zoom:50%><p>ä¸Šé¢åˆé‡æ–°å›é¡¾äº†ä¸€ä¸‹kv cacheã€‚é¦–å…ˆkv cacheä¸ä¼šå¯¹å‚æ•°é‡æœ‰å½±å“ï¼Œkv cacheä¸»è¦æ˜¯ç”¨æ¥å‡å°‘ä¸å¿…è¦çš„è®¡ç®—çš„ï¼Œæ˜¾å­˜å› æ­¤ä¹Ÿå¯èƒ½æœ‰ç›¸åº”çš„å‡å°‘ï¼Œä¸Šé¢åªæ˜¯ä¸€ä¸ªç¤ºæ„å›¾ï¼Œä¸­é—´çœç•¥äº†ä¸€äº›éƒ¨åˆ†ï¼Œè¯¦ç»†çš„é‡åŒ–åˆ†æè§ä¸‹å›¾ï¼Œéœ€è¦è¯´æ˜çš„æœ‰ä¸¤ç‚¹ï¼š</p><ol><li>kv cacheä½¿ç”¨åœºæ™¯æ˜¯æ¨ç†åœºæ™¯ï¼ŒLLMæ¨ç†åˆ†ä¸ºprefillé˜¶æ®µå’Œdecodeé˜¶æ®µï¼Œprefillé˜¶æ®µåˆ›å»ºkv-cacheï¼Œdecodeé˜¶æ®µæ›´æ–°kv-cacheã€‚åœ¨è¾“å…¥promptçš„è¿™ä¸ªprefillé˜¶æ®µä¸­ï¼Œwith kv-cacheå’Œwithout kv-cacheçš„è®¡ç®—é‡æ˜¯ç›¸åŒçš„ï¼ˆæ˜¾å­˜å ç”¨ç”±äºåˆ†é…kv-cacheï¼Œå¯èƒ½with kv-cacheä¼šæ›´å¤šä¸€ç‚¹ï¼‰ã€‚è®¡ç®—é‡çš„å‡å°‘ä¸»è¦ä½“ç°åœ¨decodeé˜¶æ®µï¼Œå› æ­¤ä¸‹é¢çš„åˆ†æä¸»è¦æ˜¯é’ˆå¯¹å•æ¬¡decodeé˜¶æ®µçš„ï¼Œå› æ­¤å›ºå®š$s==1$</li><li>ä¸‹å›¾ä¸­è¯´çš„â€œç›¸å¯¹äºåŸæ¥â€œæŒ‡çš„æ˜¯without kv-cacheæ—¶ï¼Œæ¯æ¬¡éƒ½è¾“å…¥ä¹‹å‰æ‰€æœ‰çš„tokenï¼Œè®¡ç®—å®Œæ•´çš„attention-scoreæ–¹é˜µï¼Œå› è€Œæ­¤æ—¶çš„åºåˆ—é•¿åº¦$s=s_n \le s_m$ã€‚åœ¨æœ€ç»ˆåˆ†ææ—¶ï¼Œå–æœ€å¤§å€¼$s=s_m$è¿›è¡Œæ¯”è¾ƒï¼Œå¯¹åº”decodeé˜¶æ®µçš„æœ€åä¸€ä¸ªtokençš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæœ‰çš„åšå®¢å¯èƒ½ä¼šå°†è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆprompté•¿åº¦ï¼‰å’Œè¾“å‡ºåºåˆ—é•¿åº¦åˆ†å¼€ï¼Œè¿™é‡Œåˆèµ·æ¥äº†ï¼Œæ³¨æ„åŒºåˆ«ã€‚</li></ol><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-16:10:01.png alt="transformerè¯¦ç»†åˆ†æï¼ˆkv cacheï¼‰" style=zoom:150%><table><thead><tr><th></th><th>åŸæ¥ï¼ˆwithout kv-cacheï¼‰</th><th>ç°åœ¨ï¼ˆwith kv-cacheï¼‰</th><th>å˜åŒ–</th></tr></thead><tbody><tr><td>å‚æ•°é‡</td><td>$2Vh+(12h^2+13h)l$</td><td>$2Vh+(12h^2+13h)l$</td><td>ä¸å˜</td></tr><tr><td>ä¸­é—´æ¿€æ´»</td><td>$2bsh+(34bs_mh+5bas_m^2)l$</td><td>$2bsh+(30bh+4bs_mh+5bas_m)l$</td><td>å‡å°‘äº†$(30bh(s_m-1)+5bas_m(s_m-1))l$ï¼ŒåŸæ¥ä¸­é—´æ¿€æ´»æ˜¯æœ€é•¿åºåˆ—é•¿åº¦$s_m$çš„äºŒæ¬¡æ–¹ï¼Œç°åœ¨éšç€$s_m$çº¿æ€§å¢é•¿</td></tr><tr><td>è®¡ç®—é‡</td><td>$(24h+4s_m)bs_mhl+2bs_mhV$</td><td>$(24h+4s_m)bhl+2bhV$</td><td>å‡å°‘äº†$(24h+4s_m)bhl(s_m-1)+2bhV(s_m-1)$ï¼ŒåŸæ¥è®¡ç®—é‡æ˜¯æœ€é•¿åºåˆ—é•¿åº¦$s_m$çš„äºŒæ¬¡æ–¹ï¼Œç°åœ¨éšç€$s_m$çº¿æ€§å¢é•¿</td></tr></tbody></table><p>code: from <a href=https://zhuanlan.zhihu.com/p/667763542>ã€æ‰‹æ’•LLM-KVCacheã€‘æ˜¾å­˜åˆºå®¢çš„å‰ä¸–ä»Šç”Ÿ&ndash;æ–‡æœ«å«ä»£ç </a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># author: xiaodongguaAIGC</span>
</span></span><span class=line><span class=cl><span class=c1># KV-Cache + Generation + decoder </span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>LlamaModel</span><span class=p>,</span> <span class=n>LlamaConfig</span><span class=p>,</span> <span class=n>LlamaForCausalLM</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>D</span> <span class=o>=</span> <span class=mi>128</span> <span class=c1># single-head-dim</span>
</span></span><span class=line><span class=cl><span class=n>V</span> <span class=o>=</span> <span class=mi>64</span>  <span class=c1># vocab_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>xiaodonggua_kv_cache</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>D</span><span class=p>,</span> <span class=n>V</span><span class=p>):</span>  
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>D</span> <span class=o>=</span> <span class=n>D</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>V</span> <span class=o>=</span> <span class=n>V</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>Embedding</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>V</span><span class=p>,</span><span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>Wq</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>D</span><span class=p>,</span><span class=n>D</span><span class=p>)</span>     
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>Wk</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>D</span><span class=p>,</span><span class=n>D</span><span class=p>)</span>     
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>Wv</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>D</span><span class=p>,</span><span class=n>D</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>D</span><span class=p>,</span><span class=n>V</span><span class=p>)</span> <span class=c1># LM_head</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_V</span> <span class=o>=</span> <span class=kc>None</span>  <span class=c1># initial</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span><span class=p>,</span><span class=n>K</span><span class=p>,</span><span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Wq</span><span class=p>(</span><span class=n>X</span><span class=p>),</span><span class=bp>self</span><span class=o>.</span><span class=n>Wk</span><span class=p>(</span><span class=n>X</span><span class=p>),</span><span class=bp>self</span><span class=o>.</span><span class=n>Wv</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;input_Q:&#34;</span><span class=p>,</span> <span class=n>Q</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;input_K:&#34;</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;input_V:&#34;</span><span class=p>,</span> <span class=n>V</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Easy KV_Cache</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span> <span class=o>==</span> <span class=kc>None</span><span class=p>:</span> <span class=c1># first time</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span> <span class=o>=</span> <span class=n>K</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cache_V</span> <span class=o>=</span> <span class=n>V</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span><span class=p>,</span> <span class=n>K</span><span class=p>),</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cache_V</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=bp>self</span><span class=o>.</span><span class=n>cache_V</span><span class=p>,</span> <span class=n>V</span><span class=p>),</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span>
</span></span><span class=line><span class=cl>            <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_V</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;cache_K:&#34;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;cache_V:&#34;</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_K</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># ignore proj/MLP/scaled/mask/multi-head when calculate Attention</span>
</span></span><span class=line><span class=cl>        <span class=n>attn</span> <span class=o>=</span><span class=n>Q</span><span class=nd>@K.transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span><span class=nd>@V</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># output</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span><span class=p>(</span><span class=n>attn</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>xiaodonggua_kv_cache</span><span class=p>(</span><span class=n>D</span><span class=p>,</span><span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl><span class=c1># åˆ›å»ºæ•°æ®ã€ä¸ä½¿ç”¨tokenizer</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Generation </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2> step input_shape: </span><span class=si>{</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>ï¼š&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>),</span><span class=o>-</span><span class=mi>1</span><span class=p>)[:,</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>next_token</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>next_token</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>reference and more readingï¼š</p><p><a href=https://blog.csdn.net/weixin_65514978/article/details/141399339>ã€å¤§æ¨¡å‹ç†è®ºç¯‡ã€‘Transformer KV CacheåŸç†æ·±å…¥æµ…å‡º</a></p><p><a href=https://juejin.cn/post/7362789570217885759#heading-3>å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯-KV Cache</a></p><p><a href=https://zhuanlan.zhihu.com/p/686183300>ä¸€æ–‡è¯»æ‡‚KVCache</a></p><h2 id=mqaå’Œgqaå¯¹æ˜¾å­˜å ç”¨çš„å½±å“>MQAå’ŒGQAå¯¹æ˜¾å­˜å ç”¨çš„å½±å“<a hidden class=anchor aria-hidden=true href=#mqaå’Œgqaå¯¹æ˜¾å­˜å ç”¨çš„å½±å“>Â¶</a></h2><p>åœ¨å®é™…æ¨ç†åœºæ™¯ä¸­ï¼Œkv-cacheå·²ç»æ˜¯é»˜è®¤çš„é€‰é¡¹ã€‚ä½†æ˜¯kv-cacheæ˜¯å¾ˆå æ˜¾å­˜çš„ï¼Œå ç”¨æ˜¾å­˜ä¸º$2 w_{kv} b s_m (a h_a) l$ï¼ˆå…¶ä¸­$h=a * h_a$ï¼‰ï¼Œåé¢ä¼šæœ‰case studyåˆ†æã€‚é’ˆå¯¹kv cacheçš„å„ç§ä¼˜åŒ–å±‚å‡ºä¸ç©·ï¼Œä¸‹é¢çš„å‚è€ƒä¸­æœ‰å‡ ç¯‡åšå®¢æ€»ç»“äº†ä¸€ä¸‹å¯¹kv cacheçš„å„ç§ä¼˜åŒ–ï¼Œç®€å•æ¥è¯´ï¼Œä»ä¸Šé¢çš„æ˜¾å­˜åˆ†æå…¥æ‰‹ï¼Œæœ‰ä»¥ä¸‹å‡ ç§ä¼˜åŒ–æ–¹æ³•ï¼š</p><ul><li>é’ˆå¯¹attention çª—å£ï¼ˆæˆ–è€…å«åšcontextï¼Œä¸Šä¸‹æ–‡ï¼Œæˆ–è€…å½“ä½œæœ€é•¿åºåˆ—é•¿åº¦$s_m$ï¼‰$s_m$çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚window attentionï¼Œsparse attentionï¼ŒStreamingLLM</li><li>é’ˆå¯¹æ³¨æ„åŠ›å¤´$a$çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚MQAï¼ŒGQAå…±äº«kv-cacheï¼ˆsharingï¼‰</li><li>é’ˆå¯¹å±‚æ•°$l$çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚YOCOå±‚é—´å…±äº«kv-cacheï¼ˆsharingï¼‰</li><li>é’ˆå¯¹ç²¾åº¦$w_{kv}$çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚kv-cacheé‡‡ç”¨int8é‡åŒ–</li><li>é’ˆå¯¹å†…å­˜åˆ†é…çš„ä¼˜åŒ–ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡ç­‰ï¼Œæ¯”å¦‚PagedAttention</li><li>å…¶ä»–ä¼˜åŒ–ã€‚ã€‚ã€‚</li></ul><p>å…¶ä¸­MQA/GQAåœ¨LLMä¸­å¹¿æ³›ä½¿ç”¨ï¼Œæ¯”å¦‚Llama2ä¸­å°±ä½¿ç”¨åˆ°äº†GQAã€‚ä¸‹é¢ç®€å•åˆ†æä¸€ä¸‹ã€‚</p><p>GQAæ–¹æ³•å¾ˆç®€å•ï¼ŒåŸæ¥MHAä¸­æ¯ä¸ªqå‘é‡å¯¹åº”ä¸€ä¸ªkå‘é‡å’Œvå‘é‡ï¼Œè¿›è¡Œattentionè®¡ç®—ï¼›ç°åœ¨å¥½å‡ ä¸ªqå‘é‡å¯¹åº”ï¼ˆæˆ–è€…è¯´å…±äº«ï¼‰ä¸€ä¸ªkå‘é‡å’Œvå‘é‡ï¼Œè¿™â€œå¥½å‡ ä¸ªqå‘é‡â€æ„æˆä¸€ç»„ï¼Œä¸€å…±æœ‰gç»„ï¼Œæ¯ç»„å°±æœ‰$\frac{a}{g}$ä¸ªqå‘é‡ã€‚å¦‚æœg=1ï¼Œé‚£ä¹ˆå°±æ˜¯MQAï¼Œaä¸ªqå‘é‡æ„æˆä¸€ç»„ï¼Œå…±äº«ä¸€ä¸ªkã€vå‘é‡ï¼›å¦‚æœg=aï¼Œé‚£ä¹ˆå°±æ˜¯MHAï¼Œæ¯ä¸ªqå‘é‡æ„æˆä¸€ç»„ï¼Œå¯¹åº”ä¸€ä¸ªkã€vå‘é‡ã€‚å®é™…åœºæ™¯ä¸­ï¼Œå¾€å¾€g=8ï¼Œæ¯”å¦‚æ¨ç†åœºæ™¯ä¸­å•å¡æ”¾ä¸ä¸‹ï¼Œæ­£å¥½å•æœºå…«å¡ï¼Œæ¯å¼ å¡å¯¹åº”ä¸€ç»„qå‘é‡ã€‚</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-16:40:24.png alt=image-20240908164016647 style=zoom:67%><p>è™½ç„¶MQA/GQAæ˜¯é’ˆå¯¹æ¨ç†è¿‡ç¨‹ä¸­kv-cacheçš„ä¼˜åŒ–ï¼Œä½†æ˜¯åœ¨è®­ç»ƒä¸­ä¹Ÿèƒ½ç”¨ï¼Œä¹Ÿèƒ½çœæ˜¾å­˜ã€‚ä¸‹é¢å¯¹GQAåœ¨æ¨ç†åœºæ™¯ä¸­çš„ä½¿ç”¨ï¼ˆwith kv_cacheï¼‰è¿›è¡Œä¸€ä¸ªé‡åŒ–åˆ†æã€‚</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-17:24:56.png alt=image-20240908172449500 style=zoom:150%><p>å› ä¸ºGQAåªå½±å“self-attentionè®¡ç®—éƒ¨åˆ†ï¼Œå› æ­¤å…¶ä»–éƒ¨åˆ†çœç•¥ï¼Œä¸‹é¢çš„è¡¨æ ¼ä¹Ÿæ˜¯åªåˆ†æè¿™ä¸ªå˜åŒ–çš„éƒ¨åˆ†ã€‚å¯ä»¥çœ‹å‡ºï¼Œç”±äºkv-cacheåœ¨é•¿åºåˆ—çš„æƒ…å†µä¸‹ä¼šå ç”¨å¾ˆå¤šæ˜¾å­˜ï¼ŒGQAé’ˆå¯¹ä¸­é—´æ¿€æ´»çš„ä¼˜åŒ–ä¸åºåˆ—é•¿åº¦ç›¸å…³ï¼Œå®é™…ä¸ŠGQAå¯¹ä¸­é—´æ¿€æ´»çš„ä¼˜åŒ–å°±æ˜¯å°†kv-cacheå˜ä¸ºåŸæ¥çš„$\frac{g}{a}$å€ã€‚</p><table><thead><tr><th></th><th>åŸæ¥ï¼ˆMHAï¼‰-ç°åœ¨ï¼ˆGQAï¼‰</th><th>è¯´æ˜</th></tr></thead><tbody><tr><td>å‚æ•°é‡</td><td>$\left [3(h^2+h) \right ]l - \left [ (\frac{2g}{a}+1)(h^2+h) \right ]l=2(1-\frac{g}{a})(h^2+h)l$</td><td></td></tr><tr><td>ä¸­é—´æ¿€æ´»</td><td>$\left [ wbsh+2w_{kv}bs_mh \right]l - \left [ wbsh + 2w_{kv}bs_mh \times\frac{g}{a} \right ]l = 2w_{kv}bs_mhl(1-\frac{g}{a})$</td><td>å°¤å…¶å½“é•¿åºåˆ—ï¼ˆ$bs_m$è¾ƒå¤§ï¼‰ï¼Œå¤§æ¨¡å‹ï¼ˆ$hl$è¾ƒå¤§ï¼‰æ—¶ï¼Œå‰é¢ç³»æ•°è¾ƒå¤§ï¼Œæ•´ä½“æ¿€æ´»å‡å°‘æ¯”è¾ƒå¯è§‚</td></tr><tr><td>è®¡ç®—é‡</td><td>$\left [ 6bsh^2 \right ]l - \left [ 2bsh^2 (\frac{2g}{a}+1) \right ] l = 4bsh^2l(1-\frac{g}{a}) \overset{s=1}{=} 4bh^2l(1-\frac{g}{a}) $</td><td></td></tr></tbody></table><p>åœ¨è®­ç»ƒåœºæ™¯ä¸­ï¼ŒåŒæ ·ç»™å‡ºé‡åŒ–åˆ†æã€‚éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œä¸Šè¿°åˆ†ææ˜¯åœ¨æ¨ç†åœºæ™¯+kv_cache+GQAçš„æƒ…å†µä¸‹è¿›è¡Œçš„åˆ†æï¼Œä¸‹é¢å…¬å¼æ˜¯é’ˆå¯¹çš„æ˜¯è®­ç»ƒåœºæ™¯+GQAã€‚</p><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-17:47:01.png alt=transformerè®­ç»ƒåœºæ™¯åˆ†æï¼ˆGQAï¼‰></p><p>codeï¼š from <a href=https://zhuanlan.zhihu.com/p/717838262>MHAï¼ŒMQAï¼ŒGQAæ³¨æ„åŠ›</a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GroupedQueryAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_groups</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_groups</span> <span class=o>=</span> <span class=n>num_groups</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>embed_dim</span> <span class=o>//</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=c1># attention weights</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wq</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wk</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>num_groups</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>num_groups</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wo</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>num_groups</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># n == num_heads or num_groups</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>  <span class=c1># (batch_size, seq_len, n, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=n>head_dim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>num_groups</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>//</span> <span class=n>num_groups</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># (batch_size, num_heads, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>merge_heads</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        :param x: (batch_size, num_heads, seq_len, head_dim)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>  <span class=c1># (batch_size, seq_len, num_heads, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># ( batch_size, seq_len, embed_dim)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>causal_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>wq</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>wk</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>wv</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># åˆ†å‰²æ³¨æ„åŠ›å¤´</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>num_groups</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>num_groups</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=n>v</span><span class=p>,</span> <span class=n>num_groups</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>num_groups</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># æ³¨æ„åŠ›è®¡ç®—</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>))</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>k</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>q</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># causal mask</span>
</span></span><span class=line><span class=cl>        <span class=n>mask_value</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>finfo</span><span class=p>(</span><span class=n>attn_weights</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span><span class=o>.</span><span class=n>min</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>causal_mask</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>seq_len</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>causal_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bool</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>causal_mask</span><span class=p>,</span> <span class=n>attn_weights</span><span class=p>,</span> <span class=n>mask_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># å½’ä¸€åŒ–</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>v</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># åˆå¹¶æ³¨æ„åŠ›å¤´</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>merge_heads</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>wo</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>attn_output</span>
</span></span></code></pre></td></tr></table></div></div><p>å‚è€ƒï¼š</p><p><a href=https://zhuanlan.zhihu.com/p/685853516>å¤§æ¨¡å‹ç™¾å€æ¨ç†åŠ é€Ÿä¹‹KV cacheç¯‡</a></p><p><a href=https://zhuanlan.zhihu.com/p/659770503>LLMï¼ˆäºŒåï¼‰ï¼šæ¼«è°ˆ KV Cache ä¼˜åŒ–æ–¹æ³•ï¼Œæ·±åº¦ç†è§£ StreamingLLM</a></p><p><a href=https://zhuanlan.zhihu.com/p/697311739>[KV Cacheä¼˜åŒ–]ğŸ”¥MQA/GQA/YOCO/CLA/MLKVç¬”è®°: å±‚å†…å’Œå±‚é—´KV Cacheå…±äº«</a></p><p><a href=https://zhuanlan.zhihu.com/p/708120479>å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿï¼šKV Cache å’Œ GQA</a></p><h2 id=case-study>case study<a hidden class=anchor aria-hidden=true href=#case-study>Â¶</a></h2><p>æˆ‘ä»¬ä»¥GPTå’ŒLlamaä¸ºä¾‹ï¼Œè¿›è¡Œcase studyã€‚</p><h3 id=å…³äºå‚æ•°é‡çš„åˆ†æ>å…³äºå‚æ•°é‡çš„åˆ†æ<a hidden class=anchor aria-hidden=true href=#å…³äºå‚æ•°é‡çš„åˆ†æ>Â¶</a></h3><h4 id=gpt-3>GPT-3<a hidden class=anchor aria-hidden=true href=#gpt-3>Â¶</a></h4><p>GPT-3æ¨¡å‹ç»“æ„å°±å¤§è‡´ä¸Šé¢ã€transformerç»“æ„è¯¦è§£ã€‘ä¸­çš„ç»“æ„ï¼Œä½†æ˜¯å¤šäº†ä¸€ä¸ªå¯å­¦ä¹ çš„position embeddingï¼ŒåŒ…å«$n_{ctx} * h$ä¸ªå‚æ•°ï¼Œå…¶ä¸­$n_{ctx}=2048$ï¼Œrectifiedè¿™ä¸€åˆ—æ˜¯åŠ ä¸Šè¿™äº›å‚æ•°åçš„å‚æ•°é‡ã€‚</p><table><thead><tr><th>params</th><th>h</th><th>l</th><th>a</th><th>b</th><th>V <a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>from GPT-2</a></th><th>calculated params=$Vh+(12h^2+13h)l$</th><th>rectified</th></tr></thead><tbody><tr><td>GPT-3 Small: 125M</td><td>768</td><td>12</td><td>64</td><td>0.5M</td><td>50257</td><td>123651840 $\approx$ 123.7M</td><td>125224704 $\approx$ 125.2M</td></tr><tr><td>GPT-3 Medium: 350M</td><td>1024</td><td>24</td><td>64</td><td>0.5M</td><td>50257</td><td>353772544 $\approx$353.8M</td><td>355869696 $\approx$ 355.9M</td></tr><tr><td>GPT-3 Large: 760M</td><td>1536</td><td>24</td><td>96</td><td>0.5M</td><td>50257</td><td>757151232 $\approx$ 757.1M</td><td>760296960 $\approx$ 760.3M</td></tr><tr><td>GPT-3 2.7B</td><td>2560</td><td>32</td><td>80</td><td>1M</td><td>50257</td><td>2646305280 $\approx$ 2.64B</td><td>2651548160 $\approx$ 2.65B</td></tr><tr><td>GPT-3 6.7B</td><td>4096</td><td>32</td><td>128</td><td>2M</td><td>50257</td><td>6650007552 $\approx$ 6.65B</td><td>6658396160 $\approx$ 6.67B</td></tr><tr><td>GPT-3 13B</td><td>5140</td><td>40</td><td>128</td><td>2M</td><td>50257</td><td>12942401780 $\approx$ 12.94B</td><td>12952928500 $\approx$ 12.95B</td></tr><tr><td>GPT-3 175B</td><td>12288</td><td>96</td><td>128</td><td>3.2M</td><td>50257</td><td>174579068928 $\approx$ 174.58B</td><td>174604234752 $\approx$ 174.60B</td></tr></tbody></table><blockquote><p>è¯´æ˜ï¼š</p><ol><li>GPT-3è¯è¡¨å¤§å°Våœ¨è®ºæ–‡ä¸­æ²¡æ‰¾åˆ°ï¼Œæ‰€ä»¥ç”¨çš„GPT-2çš„è¯è¡¨å¤§å°ï¼Œè¿™é‡Œè®ºæ–‡ä¸­æ˜¯æåˆ°çš„</li></ol><p>more relative readingï¼š</p><ul><li><a href=https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters>How does GPT-3 spend its 175B parameters?</a></li></ul></blockquote><h4 id=llama-1-llama--open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971>Llama 1: <a href=https://arxiv.org/pdf/2302.13971>LLaMa: Open and Efficient Foundation Language Models</a><a hidden class=anchor aria-hidden=true href=#llama-1-llama--open-and-efficient-foundation-language-modelshttpsarxivorgpdf230213971>Â¶</a></h4><p>æ¨¡å‹ç»“æ„ï¼š<a href=https://huggingface.co/docs/transformers/model_doc/llama>from hugging face transformers LLaMA</a></p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-22:35:04.png alt=llama1 style=zoom:40%><p>è®ºæ–‡ä¸­è¯´ï¼Œè¯¥æ¨¡å‹ä¸Vanilla Transformeræœ‰ä¸‰å¤„åŒºåˆ«ï¼š</p><ol><li><p>Pre-normalization and RMSNorm</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-04-22:22:22.png alt=image-20240904222219836 style=zoom:50%><p>â€‹ åŸå§‹Transformerä¸­ä½¿ç”¨post-normå±…å¤šï¼Œåæ¥ä½¿ç”¨pre-normå±…å¤šï¼Œè€Œä¸”å¾€å¾€åœ¨FFNä¹‹å‰ä¹ŸåŠ ä¸€ä¸ªnormã€‚å°¤å…¶åœ¨å¤§æ¨¡å‹ä¸­ï¼Œå¯èƒ½åœ¨é€šè¿‡LNä¹‹åMHAä¹‹å‰ï¼ŒQå’ŒKè¿˜è¦åŠ ä¸Šæ—‹è½¬ä½ç½®ç¼–ç ã€‚</p><blockquote><p>å‚è€ƒï¼š<a href=https://zhuanlan.zhihu.com/p/474988236>ã€é‡æ–°äº†è§£Transformeræ¨¡å‹ç³»åˆ—_1ã€‘PostNorm/PreNormçš„å·®åˆ«</a></p></blockquote></li><li><p>SwiGLU activation function</p><p>SwiGLUæ¿€æ´»å‡½æ•°ä¸å¤ªåƒä¼ ç»Ÿçš„ReLUç­‰æ¿€æ´»å‡½æ•°é‚£æ ·ç®€å•ï¼Œæ¯”å¦‚ReLUéƒ½ä¸å¸¦å‚æ•°ï¼Œè€ŒSwiGLUä¹ä¸€çœ‹ä¸Šå»ä¸æ˜è§‰å‰ï¼Œå®é™…ä¸Šå°†SwiGLUç†è§£æˆå¯¹ä¼ ç»ŸFFMçš„æ›¿æ¢ï¼Œæ„Ÿè§‰æ›´åˆé€‚ä¸€äº›ã€‚ç›´æ¥çœ‹å…¬å¼æœ‰ç‚¹æ‡µï¼Œçœ‹å›¾æ›´å®¹æ˜“ç†è§£ï¼Œä¸‹é¢æ˜¯FFMå’ŒSwiGLUçš„å¯¹æ¯”</p><img src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-08-23:03:55.png alt=SwiGLU style=zoom:50%><p>SwiGLUå†™æˆå…¬å¼å°±æ˜¯$SwiGLU(x) = \left [ SiGU \left( gate_proj(x) \right) \odot up_proj(x) \right] \times down_proj(x)$ï¼Œå…¶ä¸­å¯èƒ½æœ‰ç‚¹å›°æƒ‘çš„æ˜¯è¿™ä¸ª$\frac{8h}{3}$æ˜¯æ€ä¹ˆæ¥çš„ï¼Œå®é™…ä¸Šå°±æ˜¯ä¸ºäº†å·¦å³è¿™ä¸¤ä¸ªç»“æ„çš„å‚æ•°é‡ç›¸ç­‰ï¼š$2 \times h \times 4h \equiv 2 \times h \times \frac{8h}{3} + \frac{8h}{3} \times h$</p></li><li><p>Rotary Embedding</p></li></ol><p>ä¸‹é¢æ˜¯æ¨¡å‹é…ç½®ï¼ŒéªŒè¯ä¸€ä¸‹å‰é¢æ¨å‡ºæ¥çš„å‚æ•°é‡ç›¸å…³çš„å…¬å¼èƒ½å¦å¯¹ä¸Šï¼š</p><table><thead><tr><th>params</th><th>h</th><th>l</th><th>a</th><th>b</th><th><a href=https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaConfig>V</a></th><th>intermediate_size</th><th>calculated params=$2Vh+(12h^2+13h)l$</th></tr></thead><tbody><tr><td>6.7B</td><td>4096</td><td>32</td><td>32</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_7b.yaml>11008</a></td><td>6706298880 $\approx$ 6.71B</td></tr><tr><td>13.0B</td><td>5120</td><td>40</td><td>40</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_13b.yaml>13824</a></td><td>12913254400 $\approx$ 12.91B</td></tr><tr><td>32.5B</td><td>6656</td><td>60</td><td>52</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_30b.yaml>17920</a></td><td>32328857600 $\approx$ 32.33B</td></tr><tr><td>65.2B</td><td>8192</td><td>80</td><td>64</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_65b.yaml>22016</a></td><td>64957317120 $\approx$ 64.96B</td></tr></tbody></table><p>æ¯æ¬¡æ€»æ˜¯å·®ä¸€ç‚¹ï¼Œä½†æ˜¯å·®çš„ä¸å¤šï¼Œå·®åœ¨äº†å“ªé‡Œå‘¢ï¼ŸMLPéƒ¨åˆ†ï¼Œç†è®ºä¸Šintermediate_size=$\frac{8h}{3}$ï¼Œä½†æ˜¯å®é™…ä¸Šå¯èƒ½ä¼šæ¯”è¿™ä¸ªå€¼å¤§ä¸€äº›ï¼Œå¾€å¾€å‘ä¸Šå–åˆ°256ã€512ã€1024ç­‰çš„å€æ•°ï¼Œå¯¹çŸ©é˜µä¹˜æ³•æ€§èƒ½æ›´å¥½ï¼Œå› æ­¤æ¥ä¿®æ­£ä¸€ä¸‹å‚æ•°é‡ã€è®¡ç®—é‡ã€æ¿€æ´»å€¼çš„é‡åŒ–åˆ†æï¼š</p><p><img loading=lazy src=https://cdn.jsdelivr.net/gh/QinganZhang/ImageHosting/img/2024-09-09-16:15:19.png alt=transformerè¯¦ç»†åˆ†æ(llama)></p><p>é‡æ–°è®¡ç®—ä¸€ä¸‹ï¼Œè¿™æ¬¡å‚æ•°é‡å°±å¾ˆæ¥è¿‘äº†</p><table><thead><tr><th>params</th><th>h</th><th>l</th><th>a</th><th>b</th><th><a href=https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaConfig>V</a></th><th>intermediate_size</th><th>calculated params=$2Vh+(4h+4+3I)hl$</th></tr></thead><tbody><tr><td>6.7B</td><td>4096</td><td>32</td><td>32</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_7b.yaml>11008</a></td><td>6738673664 $\approx$ 6.74B</td></tr><tr><td>13.0B</td><td>5120</td><td>40</td><td>40</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_13b.yaml>13824</a></td><td>13016268800 $\approx$ 13.02B</td></tr><tr><td>32.5B</td><td>6656</td><td>60</td><td>52</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_30b.yaml>17920</a></td><td>32529735680 $\approx$ 32.53B</td></tr><tr><td>65.2B</td><td>8192</td><td>80</td><td>64</td><td>4M</td><td>32K</td><td><a href=https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama1_65b.yaml>22016</a></td><td>65286963200 $\approx$ 65.29B</td></tr></tbody></table><h4 id=llama-2-llama-2-open-foundation-and-fine-tuned-chat-modelshttpsarxivorgpdf230709288>Llama 2: <a href=https://arxiv.org/pdf/2307.09288>Llama 2: Open Foundation and Fine-Tuned Chat Models</a><a hidden class=anchor aria-hidden=true href=#llama-2-llama-2-open-foundation-and-fine-tuned-chat-modelshttpsarxivorgpdf230709288>Â¶</a></h4><p>Llama2åœ¨æ¨¡å‹ç»“æ„æ–¹é¢ä¸Llama1ç›¸å·®ä¸å¤§ï¼Œåªæ˜¯å°†MHAæ›¿æ¢ä¸ºGQAï¼Œå°†attentionçš„context lengthä»2kæå‡åˆ°4kã€‚ä¸‹é¢æ˜¯Llama2çš„æ¨¡å‹é…ç½®</p><table><thead><tr><th>config</th><th>h</th><th>l</th><th>a</th><th>b</th><th><a href=https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig>V</a></th><th>intermediate_size</th><th>MHA or GQA</th><th>calculated params=$2Vh+(12h^2+13h)l$</th><th>calculated params=$2Vh+(4h+4+3I)hl$</th></tr></thead><tbody><tr><td>7B, <a href=https://gitee.com/hf-models/Llama-2-7b-hf/blob/main/config.json>config</a></td><td>4096</td><td>32</td><td>32</td><td>4M</td><td>32K</td><td>11008</td><td>MHA</td><td>6706298880 $\approx$ 6.71B</td><td>6738673664 $\approx$ 6.74B</td></tr><tr><td>13B, <a href=https://gitee.com/hf-models/Llama-2-13b-hf/blob/main/config.json>config</a></td><td>5120</td><td>40</td><td>40</td><td>4M</td><td>32K</td><td>13824</td><td>MHA</td><td>12913254400 $\approx$ 12.91B</td><td>13016268800 $\approx$ 13.02B</td></tr></tbody></table><p>è‡³äº70Bçš„<a href=https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json>config</a>ï¼ˆh=8192, l=80, a=64, b=4M, V=32K, intermediate_size=28672, g=8ï¼‰ä½¿ç”¨äº†group=8çš„GQAï¼Œåªæœ‰attentionéƒ¨åˆ†çš„å‚æ•°é‡ä¼šå‘ç”Ÿä¸€äº›å˜åŒ–ï¼Œè°ƒæ•´å…¬å¼åï¼Œåˆ†åˆ«è®¡ç®—ä¸€ä¸‹ï¼š</p><ul><li>calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$ = 5556092928 $\approx$ 55.56Bï¼Œç›¸å·®è¾ƒå¤§</li><li>llama calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$ = 68977950720 $\approx$ 68.98Bï¼Œæ¯”è¾ƒæ¥è¿‘äº†</li></ul><p>å› æ­¤ï¼Œå¯¹äºtransformerè€Œè¨€ï¼Œ</p><ul><li>å¦‚æœMLPæ˜¯ä¼ ç»ŸFFNé‚£æ ·çš„ç»“æ„ï¼Œcalculated params=$2Vh+(12h^2+13h)l$<ul><li>å¦‚æœattentionéƒ¨åˆ†ä½¿ç”¨äº†GQAï¼Œåˆ™calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$</li></ul></li><li>å¦‚æœMLPæ˜¯SwiGLUé‚£æ ·çš„ç»“æ„ï¼Œcalculated params=$2Vh+(4h+4+3I)hl$<ul><li>å¦‚æœattentionéƒ¨åˆ†ä½¿ç”¨äº†GQAï¼Œåˆ™calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</li></ul></li></ul><p>ä½†æ˜¯æ€»çš„æ¥è¯´ï¼Œtransformerçš„å¤æ‚åº¦è¿˜æ˜¯$O(h^2l)$çº§åˆ«çš„</p><blockquote><p>more relative readingï¼š</p><p><a href=https://medium.com/@saratbhargava/mastering-llama-math-part-1-a-step-by-step-guide-to-counting-parameters-in-llama-2-b3d73bc3ae31>â€œMastering Llama Math (Part-1): A Step-by-Step Guide to Counting Parameters in Llama-2â€</a></p><p><a href=https://bitddd.blog.csdn.net/article/details/132161203>LLM - Transformer && LLaMA2 ç»“æ„åˆ†æä¸ LoRA è¯¦è§£</a></p></blockquote><h4 id=llama-3-the-llama-3-herd-of-modelshttpsarxivorgpdf240721783>Llama 3: <a href=https://arxiv.org/pdf/2407.21783>The Llama 3 Herd of Models</a><a hidden class=anchor aria-hidden=true href=#llama-3-the-llama-3-herd-of-modelshttpsarxivorgpdf240721783>Â¶</a></h4><p>Llama3çš„æ”¹è¿›ç›¸å¯¹äºLlama2å’ŒLlama1ï¼Œä¸»è¦ä½“ç°åœ¨ä½¿ç”¨äº†æ›´é«˜è´¨é‡çš„æ•°æ®å’Œæ›´å¤§è§„æ¨¡çš„è®­ç»ƒï¼Œæ¨¡å‹ç»“æ„åŸºæœ¬æ²¡å˜ã€‚ä¸‹é¢æ˜¯æ¨¡å‹é…ç½®ï¼Œ</p><table><thead><tr><th>config</th><th>h</th><th>l</th><th>a</th><th>b</th><th>V</th><th>intermediate_size</th><th>GQA group</th><th>calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</th></tr></thead><tbody><tr><td>8B, <a href=https://gitee.com/hf-models/llava-llama-3-8b-hf/blob/main/config.json>config</a></td><td>32</td><td>4096</td><td>32</td><td>4M->8M->16M</td><td>128K</td><td>14336</td><td>8</td><td>8028422144 $\approx$ 8.03B</td></tr><tr><td>70B, <a href=https://gitee.com/hf-models/Meta-Llama-3-70B/blob/main/config.json>config</a></td><td>80</td><td>8192</td><td>64</td><td>4M->8M->16M</td><td>128K</td><td>28672</td><td>8</td><td>70550814720 $\approx$ 70.55B</td></tr><tr><td>405B</td><td>126</td><td>16384</td><td>128</td><td>4M->8M->16M</td><td>128K</td><td>53248</td><td>8</td><td>405849112576 $\approx$ 405.85B</td></tr></tbody></table><p>å‚è€ƒï¼š</p><p><a href=https://blog.csdn.net/weixin_54338498/article/details/135269411>LLaMa-1/2/3 åŸç†+æºç â€”â€”æ‹†è§£ (KV-Cache, RoPE, RMSNorm, GQA, SwiGLU)</a></p><h3 id=å…³äºæ¿€æ´»çš„åˆ†æ>å…³äºæ¿€æ´»çš„åˆ†æ<a hidden class=anchor aria-hidden=true href=#å…³äºæ¿€æ´»çš„åˆ†æ>Â¶</a></h3><p>å‰é¢æ€»è¯´ä¸­é—´æ¿€æ´»å¯èƒ½å¾ˆå æ˜¾å­˜ï¼Œæˆ‘ä»¬æ¥åˆ†æå‡ ä¸ªcaseã€‚</p><p>GPT-3</p><table><thead><tr><th>config</th><th>h</th><th>l</th><th>a</th><th>b</th><th>s</th><th>V <a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>from GPT-2</a></th><th>activation $\approx (34bsh+5bas^2)l$</th><th>activation ï¼ˆwith GQAï¼‰$\approx \left [ (28+\frac{4g}{a})bsh+5bas^2\right]l$</th></tr></thead><tbody><tr><td>GPT-3 Small: 125M</td><td>768</td><td>12</td><td>64</td><td>1</td><td>2048</td><td>50257</td><td>15972.0MB $\approx 67.0 \times 2\Phi$</td><td>15873.0MB $\approx 66.58 \times 2\Phi$</td></tr><tr><td>GPT-3 Medium: 350M</td><td>1024</td><td>24</td><td>64</td><td>1</td><td>2048</td><td>50257</td><td>32352.0MB $\approx 48.5 \times 2\Phi$</td><td>32088.0 $\approx 48.1 \times 2\Phi$</td></tr><tr><td>GPT-3 Large: 760M</td><td>1536</td><td>24</td><td>96</td><td>1</td><td>2048</td><td>50257</td><td>48528.0 MB $\approx 33.5 \times 2\Phi$</td><td>48120.0MB $\approx 33.2 \times 2\Phi$</td></tr><tr><td>GPT-3 2.7B</td><td>2560</td><td>32</td><td>80</td><td>1</td><td>2048</td><td>50257</td><td>55.3GB $\approx 11.0 \times 2\Phi$ wrong</td><td>54.4GB $\approx 10.82 \times 2\Phi$</td></tr><tr><td>GPT-3 6.7B</td><td>4096</td><td>32</td><td>128</td><td>1</td><td>2048</td><td>50257</td><td>88.5GB $\approx 7.10 \times 2\Phi$</td><td>87.1GB $\approx 6.98 \times 2\Phi$</td></tr><tr><td>GPT-3 13B</td><td>5140</td><td>40</td><td>128</td><td>1</td><td>2048</td><td>50257</td><td>113.3GB $\approx 4.68 \times 2\Phi$</td><td>111.1GB $\approx 4.59 \times 2\Phi$</td></tr><tr><td>GPT-3 175B</td><td>12288</td><td>96</td><td>128</td><td>1</td><td>2048</td><td>50257</td><td>316.5GB $\approx 0.97 \times 2\Phi$</td><td>303.6GB $\approx 0.93 \times 2\Phi$</td></tr><tr><td>GPT-3 175B</td><td>12288</td><td>96</td><td>128</td><td>8</td><td>2048</td><td>50257</td><td>2532.0GB $\approx 7.77 \times 2\Phi$</td><td>2428.5GB $\approx 7.45 \times 2\Phi$</td></tr><tr><td>GPT-3 175B</td><td>12288</td><td>96</td><td>128</td><td>64</td><td>2048</td><td>50257</td><td>19.78TB $\approx 62.14 \times 2\Phi$</td><td>18.97TB $\approx 59.60 \times 2 \Phi$</td></tr></tbody></table><p>Llama-2ï¼š</p><table><thead><tr><th>config</th><th>h</th><th>l</th><th>a</th><th>b</th><th>s</th><th><a href=https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig>V</a></th><th>intermediate_size</th><th>GQA: group</th><th>activation ï¼ˆwith GQAï¼‰$\approx \left [ (13+\frac{4g}{a})bsh+5bas^2 + 6bsI\right]l$</th></tr></thead><tbody><tr><td>7B, <a href=https://gitee.com/hf-models/Llama-2-7b-hf/blob/main/config.json>config</a></td><td>4096</td><td>32</td><td>32</td><td>1</td><td>4096</td><td>32K</td><td>11008</td><td>32(MHA)</td><td>96.6GB $\approx 7.4 \times 2\Phi$</td></tr><tr><td>13B, <a href=https://gitee.com/hf-models/Llama-2-13b-hf/blob/main/config.json>config</a></td><td>5120</td><td>40</td><td>40</td><td>1</td><td>4096</td><td>32K</td><td>13824</td><td>40(MHA)</td><td>150.9GB $\approx 6.2 \times 2\Phi$</td></tr><tr><td>70B, <a href=https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json>config</a></td><td>8192</td><td>80</td><td>64</td><td>1</td><td>4096</td><td>32K</td><td>28672</td><td>8</td><td>486.25GB $\approx 3.7 \times 2\Phi$</td></tr><tr><td>70B, <a href=https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json>config</a></td><td>8192</td><td>80</td><td>64</td><td>8</td><td>4096</td><td>32K</td><td>28672</td><td>8</td><td>3890.0GB $\approx 29.8 \times 2\Phi$</td></tr><tr><td>70B, <a href=https://gitee.com/hf-models/Llama-2-70b-hf/blob/main/config.json>config</a></td><td>8192</td><td>80</td><td>64</td><td>64</td><td>4096</td><td>32K</td><td>28672</td><td>8</td><td>30.39TB $\approx 238.7 \times 2\Phi$</td></tr></tbody></table><blockquote><p>ç”±äºå‰é¢åˆ†æè¿‡ï¼Œintermediate_sizeå¾€å¾€ä¼šç•¥å¾®å¤§äº$\frac{8h}{3}$ï¼Œå› æ­¤æ ¹æ®å‰é¢åˆ†æçš„llamaç»“æ„ï¼Œé‡æ–°æ¨å¯¼ä¸€ä¸‹æ¿€æ´»çš„è®¡ç®—å…¬å¼ï¼Œè¿™é‡Œçœç•¥äº†ã€‚</p></blockquote><p>å¯ä»¥çœ‹å‡ºï¼Œå½“å¤§batchã€é•¿åºåˆ—çš„æƒ…å†µä¸‹ï¼Œä¸­é—´æ¿€æ´»å¯ä»¥æ˜¯æ¨¡å‹å‚æ•°æ‰€å æ˜¾å­˜çš„å¾ˆå¤šå€ï¼Œå³ä½¿ä½¿ç”¨äº†GQAã€‚</p><p>ä¸Šé¢éƒ½æ˜¯åœ¨è®­ç»ƒåœºæ™¯ä¸‹çš„æ¿€æ´»å€¼åˆ†æï¼Œåœ¨æ¨ç†é˜¶æ®µä¸­ï¼Œå¯ä»¥ä½¿ç”¨kv-cacheå‡å°‘æ¨¡å‹è®¡ç®—é‡ï¼ŒåŒæ—¶ä¸­é—´æ¿€æ´»ä¹Ÿå¤§å¹…åº¦å‡å°‘ï¼Œkv-cacheçš„å¤§å°ä¸º$2w_{kv}bs_mh$ï¼ˆå•å±‚ï¼‰ï¼Œæˆ‘ä»¬ä¹Ÿæ¥é‡åŒ–åˆ†æä¸€ä¸‹ï¼ˆå‡è®¾$w_{kv}$=2ï¼Œä¸”s=1ï¼Œæ¨ç†contexté•¿åº¦æœ€åä¸€ä¸ªtokençš„æƒ…å†µï¼Œå³æœ€åæƒ…å†µï¼‰</p><table><thead><tr><th>config</th><th>b</th><th>$s_m$</th><th>h</th><th>a</th><th>l</th><th>kv_cache size=$2w_{kv}bs_mhl$</th><th>without kv-cache activation$\approx (34bs_mh+5bas_m^2)l$</th><th>with kv-cache activation $\approx (30bh+4bs_mh+5bas_m)l$</th></tr></thead><tbody><tr><td>GPT-3 Small: 125M</td><td>1</td><td>2048</td><td>768</td><td>64</td><td>12</td><td>72MB $\approx 0.30 \times 2\Phi$</td><td>15972.0MB $\approx 67.0 \times 2\Phi$</td><td>79.8MB $\approx 0.33 \times 2\Phi$</td></tr><tr><td>GPT-3 Medium: 350M</td><td>1</td><td>2048</td><td>1024</td><td>64</td><td>24</td><td>192MB $\approx 0.29 \times 2\Phi$</td><td>32352.0MB $\approx 48.5 \times 2\Phi$</td><td>207.7MB $\approx 0.31 \times 2\Phi$</td></tr><tr><td>GPT-3 Large: 760M</td><td>1</td><td>2048</td><td>1536</td><td>96</td><td>24</td><td>288MB $\approx 0.20 \times 2\Phi$</td><td>48528.0MB $\approx 33.5 \times 2\Phi$</td><td>311.6MB $\approx 0.21 \times 2\Phi$</td></tr><tr><td>GPT-3 2.7B</td><td>1</td><td>2048</td><td>2560</td><td>80</td><td>32</td><td>640MB $\approx 0.12 \times 2\Phi$</td><td>55.3GB $\approx 11.0 \times 2\Phi$</td><td>667.3MB $\approx 0.13 \times 2\Phi$</td></tr><tr><td>GPT-3 6.7B</td><td>1</td><td>2048</td><td>4096</td><td>128</td><td>40</td><td>1280MB $\approx 0.1 \times 2\Phi$</td><td>110.6GB $\approx 8.9 \times 2 \Phi$</td><td>1334.7MB $\approx 0.1 \times 2 \Phi$</td></tr><tr><td>GPT-3 13B</td><td>1</td><td>2048</td><td>5140</td><td>128</td><td>96</td><td>3.76GB $\approx 0.15 \times 2\Phi$</td><td>272.0GB $\approx 11.2 \times 2\Phi$</td><td>3.89GB $\approx 0.16 \times 2\Phi$</td></tr><tr><td>GPT-3 175B</td><td>1</td><td>2048</td><td>12288</td><td>128</td><td>96</td><td>9.0GB $\approx 0.02 \times 2\Phi$</td><td>316.5GB $\approx 0.97\times 2\Phi $</td><td>9.15GB $\approx 0.03 \times 2\Phi$</td></tr><tr><td>GPT-3 175B</td><td>8</td><td>2048</td><td>12288</td><td>128</td><td>96</td><td>72.0GB $\approx 0.22 \times 2\Phi$</td><td>2532.0GB $\approx 7.77 \times 2\Phi$</td><td>73.2GB $\approx 0.22 \times 2\Phi$</td></tr><tr><td>GPT-3 175B</td><td>64</td><td>2048</td><td>12288</td><td>128</td><td>96</td><td>576.0GB $\approx 1.77 \times 2\Phi$</td><td>19.78TB $\approx 62.1 \times 2\Phi$</td><td>585.6GB $\approx 1.80 \times 2\Phi$</td></tr></tbody></table><p>å¯ä»¥çœ‹å‡ºåœ¨æ¨ç†æ—¶ï¼Œkv-cacheå¤§å¹…åº¦å‡å°‘äº†ä¸­é—´æ¿€æ´»ã€‚è€Œä¸”ä½¿ç”¨äº†kv-cacheä»¥åï¼Œkv-cacheåœ¨æ¿€æ´»ä¸­å æ®äº†ç»å¤§éƒ¨åˆ†çš„æ¯”ä¾‹ï¼Œkv-cacheç”šè‡³å¯ä»¥è¶…è¿‡æ¨¡å‹æ‰€å å†…å­˜ã€‚</p><h3 id=å…³äºè®¡ç®—é‡çš„åˆ†æ>å…³äºè®¡ç®—é‡çš„åˆ†æ<a hidden class=anchor aria-hidden=true href=#å…³äºè®¡ç®—é‡çš„åˆ†æ>Â¶</a></h3><p>é‡åŒ–åˆ†ææ¨¡å‹çš„è®¡ç®—é‡ï¼Œä¸»è¦æ˜¯ä¸ºäº†é¢„ä¼°æ¨¡å‹è®­ç»ƒæ—¶é—´ã€‚æ ¹æ®å‰é¢çš„åˆ†æï¼Œä¸€ä¸ªFWD+BWDçš„iterationè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¡ç®—é‡FLOPs=$6 \times \Phi \times è¾“å…¥tokensæ•°é‡$ï¼Œå› æ­¤å¯ä»¥å¤§è‡´ä¼°è®¡è®­ç»ƒæ—¶é—´=$\frac{6 \times \Phi \times è¾“å…¥tokensæ•°é‡}{GPUæ•°é‡\times GPUç®—åŠ›(flops) \times MFU}$ã€‚</p><h2 id=å…¶ä»–è¯´æ˜>å…¶ä»–è¯´æ˜<a hidden class=anchor aria-hidden=true href=#å…¶ä»–è¯´æ˜>Â¶</a></h2><h6 id=1-layernormçš„è®¡ç®—>1. LayerNormçš„è®¡ç®—<a hidden class=anchor aria-hidden=true href=#1-layernormçš„è®¡ç®—>Â¶</a></h6><p>LayerNormçš„è®¡ç®—è¿‡ç¨‹è§<a href=https://blog.csdn.net/weixin_39228381/article/details/107939602>pytorch LayerNormå‚æ•°è¯¦è§£ï¼Œè®¡ç®—è¿‡ç¨‹</a>ï¼Œæ€»ç»“ä¸€ä¸‹å°±æ˜¯ï¼š</p><ol><li>æ¯”å¦‚è¾“å…¥æ˜¯<code>[b,s,h]</code>ï¼ŒLNçš„<code>normalized_shape=[h]</code>ï¼Œæ­¤æ—¶å°±æ˜¯å¯¹æ¯ä¸€ä¸ªå¤§å°ä¸º<code>h</code>çš„å‘é‡åˆ†åˆ«è¿›è¡Œå½’ä¸€åŒ–ï¼ˆä¸€å…±<code>b*s</code>ä¸ªï¼‰</li><li>ç„¶åå¦‚æœLNçš„<code>elementwise_affine=True</code>ï¼Œå°±éœ€è¦å¯¹æ¯ä¸ªå¤§å°ä¸º<code>h</code>çš„å‘é‡elementwiseçš„ä¹˜ä¸Š$\gamma: [h]$ï¼Œå†elementwiseçš„åŠ ä¸Š$\beta:[h]$ï¼Œ$\gamma$å’Œ$\beta$å°±æ˜¯è¯¥LNå±‚çš„ä¸¤ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚å¦‚æœLNçš„<code>elementwise_affine=False</code>ï¼Œåˆ™åªä¼šè¿›è¡Œç¬¬ä¸€æ­¥çš„å½’ä¸€åŒ–ï¼Œä¸ä¼šè¿›è¡Œç¬¬äºŒæ­¥çš„affine</li></ol><p>ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜æ˜¯ï¼Œ<a href=https://zhuanlan.zhihu.com/p/707778968>Transformerä¸­çš„LayerNormå¯ä»¥å¹¶è¡Œå—ï¼Ÿ</a></p><p>å…³é”®è¯ï¼š Welford online Algorithmï¼Œå½“ä¸€ä¸ªé›†åˆæ–°å¢åŠ ä¸€ä¸ªå…ƒç´ $x_N$çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡å‰N-1ä¸ªæ ·æœ¬çš„corrected sum of squares($\sum_{i=1}^{N-1}(x_i-\bar{x})^2$)ï¼Œè®¡ç®—å‡ºå‰Nä¸ªæ ·æœ¬çš„corrected sum of squaresï¼Œä»è€Œåªéœ€è¦one passå°±å¯ä»¥å®ŒæˆLNçš„è®¡ç®—ï¼ˆä¹‹å‰navieçš„æ–¹æ³•æ˜¯two passï¼‰</p><h6 id=2-å…³äºdropoutçš„ä½ç½®>2. å…³äºdropoutçš„ä½ç½®<a hidden class=anchor aria-hidden=true href=#2-å…³äºdropoutçš„ä½ç½®>Â¶</a></h6><p>ä¸€å…±ï¼ˆå¯èƒ½ï¼‰åœ¨æœ‰å››ä¸ªåœ°æ–¹æœ‰dropoutï¼š</p><ol><li>åœ¨PositionalEmbeddingä¸­æœ‰ä¸€ä¸ªdropoutï¼š<code>dropout(x + PositionEmbedding(x))</code>ï¼Œä¸è¿‡å¥½åƒLLMç°åœ¨ä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç RoPEå¤šä¸€äº›ï¼Œåœ¨è®¡ç®—attentionä¹‹å‰åœ¨Qå’ŒKä¸ŠåŠ ä¸ŠRoPEï¼Œä¸€å¼€å§‹è¾“å…¥çš„embeddingä¸åŠ PositionalEmbeddingäº†</li><li>åœ¨softmaxè®¡ç®—å¾—åˆ°çš„attention scoreä¹‹åæœ‰ä¸€ä¸ªdroputï¼š$dropout( softmax(\frac{QK^T}{scale}+casual_mask) )$</li><li>åœ¨sublayerï¼ˆAttentionå’ŒMLPï¼‰è®¡ç®—å®Œä¹‹åï¼Œå„æœ‰ä¸€ä¸ªdropoutï¼š<code>x+dropout(sublayer(norm(x)))</code></li></ol><h1 id=æ€»ç»“>æ€»ç»“<a hidden class=anchor aria-hidden=true href=#æ€»ç»“>Â¶</a></h1><p>transformerçš„å‚æ•°é‡çš„å¤æ‚åº¦æ˜¯$O(h^2l)$çº§åˆ«çš„ï¼Œç²—ç•¥ä¼°è®¡å¯ä»¥è®¤ä¸ºæ˜¯$12h^2l$æˆ–è€…$(4h+3I)hl$ï¼Œå¦‚æœè¦è¯¦ç»†åˆ†æï¼Œå°±è¦çœ‹ä¸€çœ‹æ¯ä¸ªéƒ¨åˆ†çš„ç»“æ„ï¼Œæ˜¯å¦ä½¿ç”¨äº†biasï¼Œä½¿ç”¨çš„ä¸åŒä¼˜åŒ–ï¼Œæ¯”å¦‚ï¼š</p><ul><li>å¦‚æœMLPæ˜¯ä¼ ç»ŸFFNé‚£æ ·çš„ç»“æ„ï¼Œcalculated params=$2Vh+(12h^2+13h)l$<ul><li>å¦‚æœattentionéƒ¨åˆ†ä½¿ç”¨äº†GQAï¼Œåˆ™calculated params=$2Vh+\left[ 10h^2 + 11h + \frac{2g}{a}(h^2+h)\right] l$</li></ul></li><li>å¦‚æœMLPæ˜¯SwiGLUé‚£æ ·çš„ç»“æ„ï¼Œcalculated params=$2Vh+(4h+4+3I)hl$<ul><li>å¦‚æœattentionéƒ¨åˆ†ä½¿ç”¨äº†GQAï¼Œåˆ™calculated params=$2Vh + \left [ (2+\frac{2g}{a}) h ^ 2 + 4h + 3hI \right ] l$</li></ul></li></ul><p>å¯¹transformerä¸­é—´æ¿€æ´»çš„åˆ†æè¦åˆ†è®­ç»ƒåœºæ™¯å’Œæ¨ç†åœºæ™¯</p><ul><li>åœ¨è®­ç»ƒåœºæ™¯ä¸­ï¼Œä¸­é—´æ¿€æ´»å¯ä»¥æ˜¯æ¨¡å‹å‚æ•°æ‰€å æ˜¾å­˜çš„å¾ˆå¤šå€ï¼Œå°¤å…¶åœ¨å¤§batchã€é•¿åºåˆ—çš„æƒ…å†µä¸‹ã€‚<ul><li>ä¸­é—´æ¿€æ´»å€¼æ‰€å æ˜¾å­˜ç²—ç•¥ä¼°è®¡å¯ä»¥è®¤ä¸ºæ˜¯$(34bsh+5bas^2)l$æˆ–è€…$(17bsh+5bas^2+6bsI)l$ï¼Œå¯ä»¥çœ‹å‡ºä¸è¾“å…¥tokenæ•°é‡ï¼ˆbatchå’Œseq_lenï¼‰ã€éšè—å±‚ç»´åº¦ã€å¤´æ•°ã€intermediate_sizeã€å±‚æ•°ç›¸å…³ï¼Œå› æ­¤ç›¸å¯¹å‚æ•°é‡çš„åˆ†æç¨å¾®å¤æ‚ä¸€ç‚¹ã€‚</li></ul></li><li>åœ¨æ¨ç†åœºæ™¯ä¸­ï¼Œprefillé˜¶æ®µåŸºæœ¬åŒè®­ç»ƒåœºæ™¯ï¼Œdecodeé˜¶æ®µæ¯æ¬¡è¾“å…¥çš„åºåˆ—é•¿åº¦ä¸º1ï¼Œè€Œä¸”é»˜è®¤ä½¿ç”¨kv-cacheã€‚ç”±äºä½¿ç”¨kv-cacheï¼Œä¸­é—´æ¿€æ´»ç›¸å¯¹äºè®­ç»ƒæ—¶çš„ä¸­é—´æ¿€æ´»å¤§å¹…åº¦å‡å°ï¼Œä½†æ˜¯åœ¨å¤§batchã€é•¿åºåˆ—çš„æƒ…å†µä¸‹ï¼Œkv-cacheçš„æ˜¾å­˜å ç”¨ä»ç„¶å¯èƒ½è¶…è¿‡æ¨¡å‹å‚æ•°çš„æ˜¾å­˜å ç”¨ã€‚è¿˜æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œæ¨ç†åœºæ™¯ä¸­kv-cacheåœ¨ä¸­é—´æ¿€æ´»ä¸­å æ®äº†ç»å¤§éƒ¨åˆ†ã€‚<ul><li>ä¸­é—´æ¿€æ´»å€¼æ‰€å æ˜¾å­˜ç²—ç•¥ä¼°è®¡å¯ä»¥è®¤ä¸ºæ˜¯$(30bh+4bs_mh+5bas_m)l$æˆ–è€…$(13bh+4bs_mh+5bs_ma+6bI)l$</li></ul></li></ul><p>å¯¹transformerçš„è®¡ç®—é‡çš„åˆ†ææ¯”è¾ƒç®€å•ï¼Œtransformerä¸­è®¡ç®—è¾ƒä¸ºè§„æ•´ï¼Œè®¡ç®—é‡ä½“ç°åœ¨è‹¥å¹²ä¸ªå¤§å—çŸ©é˜µçš„ä¹˜æ³•ã€‚ä¸€èˆ¬é‡åŒ–åˆ†æè®¡ç®—é‡ä¸»è¦æ˜¯ä¸ºäº†é¢„ä¼°æ¨¡å‹è®­ç»ƒæ—¶é—´ï¼Œæ‰€ä»¥ä¸€èˆ¬åˆ†æçš„ä¸å¤šï¼ˆä¸€èˆ¬ä¹Ÿæ²¡æœ‰æœºä¼šè®­ç»ƒå¤§æ¨¡å‹ï¼Œå¦‚æœè®­ç»ƒæ™®é€šè§„æ¨¡çš„ç½‘ç»œï¼Œå°è¯•è·‘å‡ ä¸ªiterationå°±èƒ½ä¼°è®¡ï¼‰ã€‚</p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://qinganzhang.github.io/posts/encoder-decoder%E5%92%8Cdecoder-only%E6%9E%B6%E6%9E%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E6%B5%85%E6%9E%90/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>Encoder Decoderå’Œdecoder Onlyæ¶æ„è®­ç»ƒå’Œæ¨ç†æµ…æ</span>
</a><a class=next href=https://qinganzhang.github.io/posts/a_survey_of_efficient_transformer_on_inference/><span class=title>Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>A survey of Efficient Transformer on Inference</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://qinganzhang.github.io/>Paul's Blog</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script></body></html>